{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcvBXbd8M1md9WqYm8VdEP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niikkkhiil/niikkkhiil/blob/main/Statistics_Advance_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Explain the properties of the F-distribution.**\n",
        "\n",
        "The F-distribution is a type of probability distribution often used in statistics, especially when comparing the variability between two groups. Here are its main features:\n",
        "\n",
        "*Shape:* The F-distribution is skewed to the right (positively skewed), meaning it has a long tail on the right side. As sample sizes grow, it starts to look more like a normal (bell-shaped) distribution.\n",
        "\n",
        "*Range:* It only has values from 0 to infinity because it’s based on a ratio of variances, which are always positive.\n",
        "\n",
        "*Degrees of Freedom:* The shape of the F-distribution depends on two numbers, called degrees of freedom, which come from the sample sizes of the groups being compared.\n",
        "\n",
        "*Hypothesis Testing:* The F-distribution is used in hypothesis tests, like ANOVA, to check if there are significant differences between the variances (spread) of two or more groups.\n",
        "\n",
        "Simply put, the F-distribution helps us see if the differences we observe between groups are likely due to random chance or if they are statistically meaningful."
      ],
      "metadata": {
        "id": "8jYF4_UNTo8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?**\n",
        "\n",
        "The F-distribution is commonly used in several statistical tests, primarily because it assesses differences in variance, making it suitable for comparing multiple groups. Here are some main tests that use the F-distribution:\n",
        "\n",
        "**Analysis of Variance (ANOVA):**\n",
        "\n",
        "*Purpose:* ANOVA tests whether there are significant differences between the means of three or more groups.\n",
        "\n",
        "*Why it’s appropriate:* It uses the F-distribution to compare the variance within groups to the variance between groups. If the between-group variance is significantly larger, it suggests at least one group mean is different.\n",
        "Regression Analysis:\n",
        "\n",
        "*Purpose:* In multiple regression, the F-test checks the overall significance of the model, meaning it tests if the model explains a significant portion of the variance in the dependent variable.\n",
        "\n",
        "*Why it’s appropriate:* The F-distribution is used to compare the variance explained by the model to the variance left unexplained, helping determine if predictors as a group are significantly related to the outcome.\n",
        "\n",
        "**Testing Equality of Two Variances:**\n",
        "\n",
        "*Purpose:* This test examines if the variances of two independent populations are equal.\n",
        "\n",
        "*Why it’s appropriate:* The F-distribution directly measures the ratio of two variances, making it ideal for comparing the spread of two groups.\n",
        "\n",
        "The F-distribution is appropriate for these tests because it’s based on the ratio of variances, which is exactly what these tests assess. It provides a clear statistical measure for understanding whether observed differences are likely due to chance."
      ],
      "metadata": {
        "id": "64r4vxQtUjBu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. What are the key assumptions required for conducting an F-test to compare the variances of two\n",
        "populations?**\n",
        "\n",
        "To conduct an F-test for comparing the variances of two populations, several key assumptions must be met:\n",
        "\n",
        "**Normality:** The populations from which the samples are drawn should follow a normal distribution. The F-test is sensitive to non-normal data, and violating this assumption can lead to inaccurate results.\n",
        "\n",
        "**Independence:** The two samples should be independent of each other. This means that there is no relationship or pairing between the individual observations in each sample.\n",
        "\n",
        "**Random Sampling:** The samples should be randomly selected from the populations. This helps ensure that the sample variances accurately reflect the population variances.\n",
        "\n",
        "**Ratio of Variances:** The F-test compares the ratio of two variances, so it’s important that the sample sizes are not too small. Small sample sizes can lead to an unstable estimate of variance, affecting the accuracy of the F-test.\n",
        "\n",
        "If these assumptions are met, the F-test can provide reliable insights into whether the variances of two populations are significantly different."
      ],
      "metadata": {
        "id": "V_jZs76UVBQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. What is the purpose of ANOVA, and how does it differ from a t-test?**\n",
        "\n",
        "\n",
        "The purpose of ANOVA (Analysis of Variance) is to determine whether there are statistically significant differences between the means of three or more groups. It does this by comparing the variation between group means with the variation within each group, using the F-distribution.\n",
        "\n",
        "**Here’s how ANOVA differs from a t-test:**\n",
        "\n",
        "**Number of Groups:**\n",
        "\n",
        "*   ANOVA: Used when comparing the means of three or more groups.\n",
        "*   t-test: Used for comparing the means of only two groups (either independent or paired).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Type of Test:**\n",
        "\n",
        "\n",
        "*   ANOVA: Aims to check for any significant differences among multiple group means. It doesn’t specify which means differ—only that at least one group mean is different from the others.\n",
        "*   t-test: Tests specifically whether there is a significant difference between the means of two groups.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Error Rates:**\n",
        "\n",
        "ANOVA: Controls for the overall Type I error rate when comparing multiple groups. Running multiple t-tests for each pair of groups can increase the risk of a Type I error (false positive), while ANOVA maintains this error rate for all comparisons.\n",
        "\n",
        "**Post-hoc Analysis:**\n",
        "\n",
        "If ANOVA finds a significant difference, post-hoc tests (like Tukey’s HSD) are often conducted to identify which specific group means differ. In a two-sample t-test, there’s no need for further testing because only two groups are being compared.\n",
        "\n",
        "In summary, ANOVA is a more efficient and reliable method than multiple t-tests for analyzing differences across three or more groups, reducing the risk of error and simplifying the analysis."
      ],
      "metadata": {
        "id": "Y2BSf5VAYIPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more\n",
        "than two groups.**\n",
        "\n",
        "You would use a one-way ANOVA instead of multiple t-tests when comparing the means of more than two groups. Here’s when and why it’s preferable:\n",
        "\n",
        "**When to Use a One-Way ANOVA**\n",
        "\n",
        "Three or More Groups: When you need to compare the means of three or more independent groups based on a single factor (independent variable), one-way ANOVA is the appropriate choice.\n",
        "\n",
        "**Why Use a One-Way ANOVA Instead of Multiple t-Tests**\n",
        "\n",
        "**Controls Type I Error Rate:** Every time a t-test is performed, there’s a chance of a Type I error (incorrectly finding a difference when there isn’t one). If multiple t-tests are run, this error risk accumulates, increasing the likelihood of false positives. One-way ANOVA controls this error by testing all groups simultaneously, maintaining the overall significance level.\n",
        "\n",
        "**Efficiency:** Running one ANOVA test is more efficient than performing multiple pairwise t-tests, especially as the number of groups increases. ANOVA provides a single test to assess if any group differs significantly from the others.\n",
        "\n",
        "**Comprehensive Analysis:** ANOVA tells you if there is a significant difference among the group means as a whole, but it doesn’t specify which groups differ. This broad view is useful in initial analyses, and, if ANOVA shows significant differences, post-hoc tests can then determine the specific groups that differ.\n",
        "\n",
        "**Example**\n",
        "Suppose you are comparing the average test scores of students in three different teaching methods (Group A, Group B, and Group C). A one-way ANOVA would reveal if there is a statistically significant difference among the methods overall, and if so, post-hoc tests could then identify which methods specifically differ from each other."
      ],
      "metadata": {
        "id": "qRRYSNpJYyw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n",
        "How does this partitioning contribute to the calculation of the F-statistic?**\n",
        "\n",
        "In ANOVA, total variance is divided into between-group variance and within-group variance:\n",
        "\n",
        "**Between-Group Variance:**\n",
        "\n",
        "\n",
        "*   Measures the variability due to differences among group means.\n",
        "*  Calculated as the average of squared differences between each group mean and the overall mean.\n",
        "*  Indicates how much group means differ.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Within-Group Variance:**\n",
        "\n",
        "* Measures variability within each group around its mean.\n",
        "\n",
        "* Calculated as the average of squared differences of observations from their group means.\n",
        "\n",
        "* Reflects random error or natural variation within groups.\n",
        "\n",
        "**F-Statistic Calculation**\n",
        "The F-statistic is calculated as:\n",
        "\n",
        "𝐹\n",
        "=\n",
        "(Between-Group Variance)/\n",
        "(Within-Group Variance)\n",
        "\n",
        "​\n",
        "\n",
        "* A larger F-statistic suggests significant differences between group means, indicating that the variation between groups is greater than what would be expected by chance.\n",
        "\n",
        "This partitioning of variance is crucial for determining if observed differences among group means are statistically significant."
      ],
      "metadata": {
        "id": "F04SW26HZja1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\n",
        "differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?**\n",
        "\n",
        "The classical (frequentist) approach to ANOVA and the Bayesian approach differ significantly in their philosophies and methodologies. Here are the key differences in how they handle uncertainty, parameter estimation, and hypothesis testing:\n",
        "\n",
        "**1. Handling Uncertainty**\n",
        "\n",
        "Frequentist Approach:\n",
        "\n",
        "* Uncertainty is addressed through the concept of sampling distributions and p-values. Results are interpreted based on the probability of observing the data given a null hypothesis.\n",
        "\n",
        "* Confidence intervals provide a range for parameter estimates, but they do not provide a direct probability of the parameter itself.\n",
        "\n",
        "Bayesian Approach:\n",
        "\n",
        "* Uncertainty is expressed in terms of probability distributions for parameters. Bayesian methods allow for direct probability statements about the parameters being estimated.\n",
        "\n",
        "* The posterior distribution represents updated beliefs about the parameters after observing data.\n",
        "\n",
        "**2. Parameter Estimation**\n",
        "\n",
        "Frequentist Approach:\n",
        "\n",
        "* Parameters are treated as fixed but unknown quantities. Point estimates (e.g., means) are derived from the data without incorporating prior information.\n",
        "\n",
        "* Estimation typically involves calculating estimates (like means and variances) based solely on the sample data.\n",
        "\n",
        "Bayesian Approach:\n",
        "\n",
        "* Parameters are treated as random variables with their own probability distributions. Prior distributions reflect previous knowledge or beliefs about parameters before observing data.\n",
        "\n",
        "* Posterior estimates are derived from combining prior beliefs with observed data, resulting in updated estimates that reflect both prior knowledge and new evidence.\n",
        "\n",
        "**3. Hypothesis Testing**\n",
        "\n",
        "Frequentist Approach:\n",
        "\n",
        "* Hypothesis testing is done using p-values to determine the likelihood of observing the data if the null hypothesis is true. A common threshold (e.g., 0.05) is used to decide whether to reject the null hypothesis.\n",
        "\n",
        "* Focus is on rejecting or failing to reject the null hypothesis, without providing the probability that the null hypothesis is true.\n",
        "\n",
        "Bayesian Approach:\n",
        "\n",
        "* Hypothesis testing involves comparing the posterior probabilities of different hypotheses. It allows for assessing the probability of one hypothesis being true given the data.\n",
        "\n",
        "* Bayesian methods can provide credible intervals and Bayesian factor comparisons to evaluate the strength of evidence for or against specific hypotheses.\n",
        "\n",
        "**Summary**\n",
        "\n",
        "* The frequentist approach focuses on long-run frequency properties and hypothesis testing based on p-values, treating parameters as fixed.\n",
        "\n",
        "* The Bayesian approach incorporates prior beliefs, treats parameters as random, and allows for direct probability statements about hypotheses.\n",
        "\n",
        "These differences lead to distinct interpretations and methodologies in conducting ANOVA and analyzing results, with Bayesian methods offering a more flexible framework for incorporating uncertainty and prior information."
      ],
      "metadata": {
        "id": "LCCHnuJvaHgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Question: You have two sets of data representing the incomes of two different professions1\n",
        "V Profession A: [48, 52, 55, 60, 62'\n",
        "V Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
        "incomes are equal. What are your conclusions based on the F-test?**\n",
        "\n",
        "**Task: Use Python to calculate the F-statistic and p-value for the given data.**\n",
        "\n",
        "**Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison.**\n",
        "\n",
        "The results of the F-test for the two sets of data are as follows:\n",
        "\n",
        "F-Statistic:\n",
        "𝐹\n",
        "≈\n",
        "2.09\n",
        "\n",
        "p-value:\n",
        "𝑝\n",
        "≈\n",
        "0.247\n",
        "\n",
        "**Conclusion**\n",
        "To determine if the variances of the two professions' incomes are equal, we compare the p-value to a significance level (commonly set at\n",
        "𝛼\n",
        "=\n",
        "0.05\n",
        ").\n",
        "\n",
        "* Since the p-value (0.247) is greater than the significance level (0.05), we fail to reject the null hypothesis.\n",
        "\n",
        "This means that there is not enough evidence to conclude that the variances of incomes between Profession A and Profession B are significantly different. Therefore, we can say that the variances are considered equal based on this F-test. ​\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o61wkeTLbPwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Data for the two professions\n",
        "profession_a = np.array([48, 52, 55, 60, 62])\n",
        "profession_b = np.array([45, 50, 55, 52, 47])\n",
        "\n",
        "# Calculate variances\n",
        "var_a = np.var(profession_a, ddof=1)  # Sample variance\n",
        "var_b = np.var(profession_b, ddof=1)  # Sample variance\n",
        "\n",
        "# Calculate F-statistic\n",
        "f_statistic = var_a / var_b\n",
        "\n",
        "# Degrees of freedom\n",
        "dof_a = len(profession_a) - 1\n",
        "dof_b = len(profession_b) - 1\n",
        "\n",
        "# Calculate p-value\n",
        "p_value = 1 - stats.f.cdf(f_statistic, dof_a, dof_b)\n",
        "\n",
        "f_statistic, p_value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWB1qxuVb6_R",
        "outputId": "c9a97a7e-1128-4294-968f-4b56e4c51a4d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.089171974522293, 0.24652429950266952)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in\n",
        "average heights between three different regions with the following data1\n",
        "V Region A: [160, 162, 165, 158, 164'\n",
        "V Region B: [172, 175, 170, 168, 174'\n",
        "V Region C: [180, 182, 179, 185, 183'\n",
        "V Task: Write Python code to perform the one-way ANOVA and interpret the results\n",
        "V Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value.**\n",
        "\n",
        "\n",
        "The results of the one-way ANOVA for the average heights between the three regions are as follows:\n",
        "\n",
        "F-Statistic:\n",
        "𝐹\n",
        "≈\n",
        "67.87\n",
        "\n",
        "p-value:\n",
        "𝑝\n",
        "≈\n",
        "2.87\n",
        "×\n",
        "1\n",
        "0\n",
        "−\n",
        "7\n",
        "\n",
        "\n",
        "**Interpretation of Results**\n",
        "* Significance Level: If we use a common significance level of\n",
        "𝛼\n",
        "=\n",
        "0.05\n",
        ", we compare the p-value to this threshold.\n",
        "\n",
        "* Conclusion: Since the p-value (approximately\n",
        "2.87\n",
        "×\n",
        "1\n",
        "0\n",
        "−\n",
        "7\n",
        " )\n",
        "\n",
        "is much less than 0.05, we reject the null hypothesis.\n",
        "\n",
        "This indicates that there are statistically significant differences in average heights among the three regions (Region A, Region B, and Region C). In other words, at least one region has a mean height that is significantly different from the others.\n",
        "\n",
        "**Next Steps**\n",
        "To determine which specific regions differ from each other, you could conduct post-hoc tests (like Tukey's HSD) if needed.\n",
        "\n"
      ],
      "metadata": {
        "id": "6fUlF3-QcZtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Data for the three regions\n",
        "region_a = np.array([160, 162, 165, 158, 164])\n",
        "region_b = np.array([172, 175, 170, 168, 174])\n",
        "region_c = np.array([180, 182, 179, 185, 183])\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(region_a, region_b, region_c)\n",
        "\n",
        "f_statistic, p_value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2zn_RRncWlM",
        "outputId": "85a5f7fd-1fc0-4401-9554-3871c5c6afef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(67.87330316742101, 2.870664187937026e-07)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IbYR665Mcku6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}