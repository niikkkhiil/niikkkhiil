{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMG/wVKj8WcuyHGgR2C4XZM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niikkkhiil/niikkkhiil/blob/main/Feature_Engineering_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is a parameter?**\n",
        "\n",
        "Parameters refer to settings or values used to customize or control various transformations, methods, or models applied to data. Here’s a breakdown of parameters based on their use cases:\n",
        "\n",
        "  **Handling Missing Data**\n",
        "\n",
        "  Imputation Parameters:\n",
        "\n",
        "Strategy: \"mean\", \"median\", \"most_frequent\", or a custom value.\n",
        "\n",
        "Neighbor Count (KNN Imputation): Number of nearest neighbors to consider when imputing missing values.\n",
        "\n",
        "**2. Encoding Categorical Variables**\n",
        "\n",
        "**Label Encoding:**\n",
        "\n",
        "categories: Defines the order of categories (e.g., [low, medium, high]).\n",
        "\n",
        "**One-Hot Encoding:**\n",
        "\n",
        "drop: Specify whether to drop one column to avoid multicollinearity (e.g., \"first\" or \"none\").\n",
        "\n",
        "**Target Encoding:**\n",
        "\n",
        "smoothing: Degree of regularization applied to reduce overfitting.\n",
        "min_samples: Minimum number of samples required for a category.\n",
        "\n",
        "**3. Scaling and Normalization**\n",
        "\n",
        "**Standardization:**\n",
        "\n",
        "mean and std: Pre-computed mean and standard deviation values.\n",
        "\n",
        "**Min-Max Scaling:**\n",
        "\n",
        "feature_range: Tuple defining the range (default is [0, 1]).\n",
        "\n",
        "**Robust Scaler:**\n",
        "\n",
        "quantile_range: Determines the range for scaling (e.g., [25, 75]).\n"
      ],
      "metadata": {
        "id": "IvOGg050cPN_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. What is correlation? What does negative correlation mean?**\n",
        "\n",
        "**Correlation**\n",
        "\n",
        "Correlation is a statistical measure that expresses the strength and direction of a relationship between two variables. It quantifies how changes in one variable are associated with changes in another. Correlation values range from -1 to +1, and are commonly represented by the correlation coefficient.\n",
        "\n",
        "**Positive Correlation:** When one variable increases, the other variable also increases (e.g., height and weight).\n",
        "\n",
        "**Negative Correlation:** When one variable increases, the other variable decreases (e.g., exercise time and body fat percentage).\n",
        "\n",
        "**No Correlation:** No discernible relationship between the variables (e.g., shoe size and intelligence).\n",
        "\n",
        "**Negative Correlation**\n",
        "\n",
        "Negative correlation occurs when two variables move in opposite directions. It indicates that as the value of one variable increases, the value of the other decreases, and vice versa.\n",
        "\n",
        "**Key Characteristics:**\n",
        "\n",
        "Correlation coefficient: Between -1 and 0.\n",
        "\n",
        "-1 indicates a perfect negative correlation, where one variable decreases proportionally as the other increases.\n",
        "\n",
        "Closer to 0 indicates a weaker negative relationship.\n",
        "\n",
        "Example:\n",
        "Temperature and Coat Sales: As the temperature decreases (gets colder), coat sales tend to increase.\n",
        "\n",
        "Time Spent Studying and Errors on a Test: More study time typically leads to fewer mistakes on a test.\n"
      ],
      "metadata": {
        "id": "BnKZjkaLi83C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Define Machine Learning. What are the main components in Machine Learning?**\n",
        "\n",
        "Machine Learning (ML) is a branch of artificial intelligence (AI) that enables computers to learn and improve from experience without being explicitly programmed. It involves creating systems that automatically learn patterns and make predictions or decisions based on data.\n",
        "\n",
        "**Main Components of Machine Learning**\n",
        "\n",
        "**Data**\n",
        "\n",
        "The foundation of ML, comprising structured or unstructured information.\n",
        "\n",
        "Key processes: data collection, preprocessing, and feature engineering.\n",
        "\n",
        "**Model**\n",
        "\n",
        "A mathematical representation of the problem being solved.\n",
        "Examples: Linear Regression, Neural Networks, Decision Trees.\n",
        "Algorithms\n",
        "\n",
        "The methods used to train models by finding patterns in data.\n",
        "\n",
        "Types:\n",
        "\n",
        "**Supervised Learning:** Uses labeled data (e.g., Classification, Regression).\n",
        "\n",
        "**Unsupervised Learning:** Identifies patterns in unlabeled data (e.g., Clustering, Dimensionality Reduction).\n",
        "\n",
        "**Reinforcement Learning:** Learns through trial-and-error interactions with the environment.\n",
        "\n",
        "**Features**\n",
        "\n",
        "Measurable properties of the data that influence the model’s predictions.\n",
        "\n",
        "Processes: feature selection and feature extraction.\n",
        "Training Process\n",
        "\n",
        "Involves feeding data into the model to learn patterns.\n",
        "\n",
        "Key techniques: data splitting (training, validation, testing) and optimization algorithms like Gradient Descent.\n"
      ],
      "metadata": {
        "id": "Tu5lTklWl3YH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. How does loss value help in determining whether the model is good or not?**\n",
        "\n",
        "\n",
        "The loss value is a critical metric in machine learning that helps determine how well a model is performing during training and testing. It measures the error between the model’s predictions and the actual target values. Here's how the loss value is used to assess a model's quality:\n",
        "\n",
        "**1. What is the Loss Value?**\n",
        "\n",
        "Definition: The loss value quantifies the discrepancy between the predicted output of the model and the true output.\n",
        "\n",
        "Calculation: It is computed using a loss function. Common examples include:\n",
        "\n",
        "Mean Squared Error (MSE): For regression tasks.\n",
        "\n",
        "Cross-Entropy Loss: For classification tasks.\n",
        "\n",
        "Hinge Loss: For support vector machines.\n",
        "\n",
        "**2. How Loss Value Determines Model Performance**\n",
        "\n",
        "**Lower Loss = Better Fit:**\n",
        "\n",
        "A low loss value indicates that the model's predictions are close to the actual values, meaning it is performing well.\n",
        "\n",
        "A high loss value signals poor performance, indicating that the model is not capturing patterns effectively.\n",
        "\n",
        "**Helps Detect Overfitting or Underfitting:**\n",
        "\n",
        "Overfitting: The model performs well on training data (low training loss) but poorly on validation/test data (high validation/test loss).\n",
        "\n",
        "Underfitting: High loss on both training and validation/test data indicates the model cannot learn patterns effectively.\n",
        "\n",
        "**Monitors Model Optimization:**\n",
        "\n",
        "During training, the loss value should decrease as the model learns from the data.\n",
        "\n",
        "If the loss stops decreasing or fluctuates significantly, it may indicate a learning problem, such as a poor learning rate or insufficient data."
      ],
      "metadata": {
        "id": "9E5pCN0CnRaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. What are continuous and categorical variables**?\n",
        "\n",
        "In statistics and machine learning, variables are typically classified as continuous or categorical based on the type of data they represent. Here's a clear distinction:\n",
        "\n",
        "**1. Continuous Variables**\n",
        "\n",
        "Definition: Variables that can take any value within a range. They are measured on a continuous scale and have an infinite number of possible values.\n",
        "\n",
        "Examples:\n",
        "Height (e.g., 5.67 feet)\n",
        "\n",
        "Weight (e.g., 72.3 kg)\n",
        "\n",
        "Temperature (e.g., 37.5°C)\n",
        "\n",
        "Time (e.g., 12.45 seconds)\n",
        "\n",
        "**Characteristics:**\n",
        "\n",
        "Represent **quantitative data.**\n",
        "\n",
        "Values are usually numeric and can have decimal points.\n",
        "\n",
        "Can be used to calculate meaningful averages (mean, median) or apply mathematical operations like addition and multiplication.\n",
        "\n",
        "**Types of Continuous Variables:**\n",
        "\n",
        "Interval: No true zero point (e.g., temperature in Celsius or Fahrenheit).\n",
        "\n",
        "Ratio: Has a true zero point (e.g., height, weight, distance).\n",
        "\n",
        "**2. Categorical Variables**\n",
        "\n",
        "Definition: Variables that represent categories or groups. They take on a limited, fixed number of distinct values.\n",
        "\n",
        "Examples:\n",
        "Gender (e.g., Male, Female)\n",
        "Color (e.g., Red, Green, Blue)\n",
        "Type of car (e.g., Sedan, SUV, Truck)\n",
        "\n",
        "**Characteristics:**\n",
        "\n",
        "Represent qualitative data.\n",
        "\n",
        "Values are usually labels or classes, not numeric.\n",
        "\n",
        "Arithmetic operations (like addition or multiplication) don’t apply.\n",
        "\n",
        "**Types of Categorical Variables:**\n",
        "\n",
        "Nominal: Categories without an inherent order (e.g., blood type: A, B, AB, O).\n",
        "\n",
        "Ordinal: Categories with a meaningful order or ranking (e.g., education level: High School, Bachelor’s, Master’s).\n",
        "\n"
      ],
      "metadata": {
        "id": "431AXBa25fss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. How do we handle categorical variables in Machine Learning? What are the common techniques?**\n",
        "\n",
        "**Handling Categorical Variables in Machine Learning**\n",
        "\n",
        "Categorical variables represent discrete categories or groups, and machine learning algorithms typically require numerical input. Therefore, categorical variables need to be transformed into a numerical format before they can be fed into models. Here are the most common techniques for handling categorical variables:\n",
        "\n",
        "**1. Label Encoding**\n",
        "\n",
        "Definition: This technique assigns a unique integer to each category in a categorical variable.\n",
        "\n",
        "Use Case: Ideal for ordinal data where the categories have a meaningful order (e.g., Low, Medium, High).\n",
        "\n",
        "Example:\n",
        "Colors: Red -> 0, Green -> 1, Blue -> 2.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "Simple to implement.\n",
        "\n",
        "Works well for ordinal data.\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "It may impose an arbitrary ordinal relationship where there is none, which can affect models that interpret the integers as having some numeric meaning (e.g., Linear Regression, Decision Trees).\n",
        "\n",
        "**2. One-Hot Encoding**\n",
        "\n",
        "Definition: One-Hot Encoding creates new binary columns for each category. Each row gets a 1 for the column corresponding to its category and 0 for others.\n",
        "\n",
        "Use Case: Useful for nominal (unordered) categories, where there is no inherent ranking.\n",
        "\n",
        "**3. Frequency or Count Encoding**\n",
        "\n",
        "Definition: Each category is replaced by the frequency or count of its occurrences in the dataset.\n",
        "\n",
        "Use Case: Useful when the frequency of categories is meaningful and might help improve model performance.\n",
        "\n",
        "**Target Encoding (Mean Encoding)**\n",
        "\n",
        "\n",
        "Definition: Each category is replaced by the mean of the target variable for that category.\n",
        "\n",
        "Use Case: Typically used for categorical features in regression and classification tasks where the category’s relationship to the target variable is important."
      ],
      "metadata": {
        "id": "QitVHwHo8D56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. What do you mean by training and testing a dataset?**\n",
        "\n",
        "In machine learning, the training and testing datasets play crucial roles in evaluating and improving the performance of a model. These datasets are used to train the model, evaluate its generalization ability, and ensure it can make accurate predictions on unseen data.\n",
        "\n",
        "**1. Training Dataset**\n",
        "\n",
        "Definition: The training dataset is the subset of data used to train the model. During training, the model learns patterns, relationships, and structures within the data, adjusting its parameters to minimize the error or loss.\n",
        "\n",
        "**Purpose:**\n",
        "The model \"learns\" from this dataset by adjusting its internal parameters (weights) to find patterns or correlations between the features and the target variable.\n",
        "\n",
        "**What Happens During Training?:**\n",
        "Feeding Data: The model is presented with the features (input variables) and the corresponding target (output variable).\n",
        "Model Fitting: The model adjusts its parameters based on the patterns it identifies in the training data.\n",
        "\n",
        "Optimization: The model uses an optimization algorithm (e.g., Gradient Descent) to minimize the loss function, improving its performance on the training data.\n",
        "\n",
        "**2. Testing Dataset**\n",
        "\n",
        "Definition: The testing dataset is the subset of data that the model has never seen before. It is used to evaluate the model’s performance and generalization ability.\n",
        "\n",
        "**Purpose:**\n",
        "The testing dataset allows us to simulate how the model will perform on real-world, unseen data. It helps us assess whether the model is overfitting (memorizing training data) or underfitting (not learning enough patterns).\n",
        "\n",
        "**What Happens During Testing?:**\n",
        "\n",
        "Model Evaluation: After the model is trained, it makes predictions based on the features in the testing set.\n",
        "\n",
        "Performance Metrics: The predicted outputs are compared to the actual target values in the testing set, and various metrics (e.g., accuracy, precision, recall, RMSE) are calculated to assess the model's performance.\n",
        "\n",
        "**Why Split Data into Training and Testing?**\n",
        "\n",
        "**Overfitting and Underfitting:** If a model is trained and tested on the same data, it can \"memorize\" the training data (overfitting), leading to poor performance on new data. A separate testing dataset helps detect this issue.\n",
        "\n",
        "**Generalization:** The goal of training a model is to enable it to generalize to new, unseen data. By using a distinct testing dataset, we can evaluate how well the model generalizes and performs on data it hasn't encountered before.\n"
      ],
      "metadata": {
        "id": "KHGu4QEpAf9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. What is sklearn.preprocessing?**\n",
        "\n",
        "sklearn.preprocessing in Scikit-learn\n",
        "\n",
        "sklearn.preprocessing is a module in Scikit-learn used to transform and preprocess data for machine learning models. It provides various functions to scale, encode, and modify data, ensuring that it is in an appropriate format for algorithms to process effectively.\n",
        "\n",
        "**Common Techniques in sklearn.preprocessing:**\n",
        "\n",
        "**Scaling and Normalization:**\n",
        "\n",
        "**StandardScaler:** Scales data to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "**MinMaxScaler:** Scales features to a specified range (default [0, 1]).\n",
        "\n",
        "**RobustScaler:** Uses the median and interquartile range, making it robust to outliers.\n",
        "\n",
        "**Encoding Categorical Variables:**\n",
        "\n",
        "OneHotEncoder: Converts categorical variables into binary columns (one-hot encoding).\n",
        "\n",
        "**LabelEncoder:** Converts categorical labels into numeric values.\n",
        "Polynomial Features:\n",
        "\n",
        "**PolynomialFeatures:** Generates polynomial and interaction features to capture nonlinear relationships.\n",
        "\n",
        "**Binarization:**\n",
        "\n",
        "**Binarizer:** Converts numeric features into binary values based on a threshold.\n",
        "\n",
        "**Discretization:**\n",
        "\n",
        "**KBinsDiscretizer:** Discretizes continuous features into discrete bins for algorithms requiring categorical input.\n",
        "\n",
        "**Scaling for Sparse Matrices:**\n",
        "\n",
        "**MaxAbsScaler:** Scales features by their maximum absolute value while preserving sparsity.\n",
        "\n",
        "Preprocessing ensures that features are on a similar scale, categorical variables are encoded properly, and outliers are handled, improving model performance and compatibility with different machine learning algorithms.\n",
        "\n"
      ],
      "metadata": {
        "id": "s89kFOgEB6LU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. What is a Test set?**\n",
        "\n",
        "**Test Set in Machine Learning**\n",
        "\n",
        "A test set is a subset of the dataset that is used to evaluate the performance of a machine learning model after it has been trained on the training set. The primary purpose of the test set is to assess how well the model generalizes to unseen data, providing an estimate of its real-world performance.\n",
        "\n",
        "**Key Points About a Test Set:**\n",
        "\n",
        "**Evaluation:**\n",
        "\n",
        "The test set allows you to evaluate how well the model performs on new, unseen data, which helps determine if the model is overfitting or underfitting.\n",
        "\n",
        "**Unseen Data:**\n",
        "\n",
        "The model is not exposed to the test data during training, meaning it hasn’t \"seen\" the test examples before. This simulates how the model will perform when deployed in a real-world scenario with new data.\n",
        "\n",
        "**Metrics Calculation:**\n",
        "\n",
        "After training the model on the training data, it makes predictions on the test set. These predictions are compared to the actual values in the test set, and performance metrics such as accuracy, precision, recall, or RMSE are calculated.\n",
        "\n",
        "**Importance of a Test Set:**\n",
        "**Generalization:** The test set helps measure the generalization ability of the model, i.e., its performance on new data, which is crucial for real-world application.\n",
        "\n",
        "**Model Selection:** It helps compare different models and select the one that performs best on unseen data."
      ],
      "metadata": {
        "id": "tSKE5ocFGb6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?**\n",
        "\n",
        "In Python, the Scikit-learn library provides easy-to-use functions for splitting data into training and testing sets. The most common method is using the train_test_split function from the sklearn.model_selection module.\n",
        "\n",
        "**Steps to Split Data Using train_test_split:**"
      ],
      "metadata": {
        "id": "cgU4DAfEG_ng"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxaMnU6cXzc8"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**X:** Features (input data).\n",
        "\n",
        "**y:** Target variable (output data).\n",
        "\n",
        "**test_size=0.2:** Specifies that 20% of the data should be used as the test set (the rest is used for training).\n",
        "\n",
        "**random_state=42:** Ensures reproducibility by controlling the random shuffling of data.\n",
        "\n",
        "This splits the dataset into 80% training data (X_train, y_train) and 20% test data (X_test, y_test).\n",
        "\n",
        "**How to Approach a Machine Learning Problem**\n",
        "\n",
        "A typical approach to solving a machine learning problem involves the following steps:\n",
        "\n",
        "**1. Define the Problem**\n",
        "Understand the problem you are trying to solve (e.g., classification, regression).\n",
        "\n",
        "Define the target variable (what you're trying to predict) and the features (the input data).\n",
        "\n",
        "**2. Collect and Prepare Data**\n",
        "\n",
        "Gather relevant data from various sources.\n",
        "\n",
        "Perform data cleaning: handle missing values, remove duplicates, and deal with noisy data.\n",
        "\n",
        "Transform data into a format suitable for machine learning (e.g., encoding categorical variables, scaling numerical features).\n",
        "\n",
        "**3. Split the Data**\n",
        "Divide the data into training and test sets (commonly 80-20 or 70-30 split).\n",
        "\n",
        "**4. Choose the Right Model**\n",
        "Select a machine learning algorithm suitable for the task (e.g., Linear Regression, Decision Trees, K-Nearest Neighbors, etc.).\n",
        "\n",
        "Consider model complexity, interpretability, and the nature of the problem (e.g., binary classification, multi-class classification, regression).\n",
        "\n",
        "**5. Train the Model**\n",
        "Train the selected model on the training data (features and target variables).\n",
        "\n",
        "Tune model parameters (hyperparameters) to improve performance.\n",
        "\n",
        "**6. Evaluate the Model**\n",
        "Use the test set to evaluate how well the model generalizes to unseen data.\n",
        "\n",
        "Calculate evaluation metrics (accuracy, precision, recall, F1 score for classification, or RMSE, MAE for regression).\n",
        "\n",
        "**7. Model Tuning**\n",
        "Fine-tune the model by adjusting hyperparameters or experimenting with different algorithms.\n",
        "\n",
        "You can use techniques like GridSearchCV or RandomizedSearchCV for hyperparameter tuning.\n",
        "\n",
        "**8. Validate the Model**\n",
        "Use cross-validation (e.g., K-Fold Cross-Validation) to get a more robust estimate of the model's performance.\n",
        "\n",
        "**9. Deploy and Monitor**\n",
        "Once satisfied with the model's performance, deploy it for real-world predictions.\n",
        "\n",
        "Continuously monitor the model’s performance over time and retrain it with new data if necessary.\n",
        "\n",
        "**10. Communicate Results**\n",
        "Present findings through visualizations, metrics, and reports to stakeholders."
      ],
      "metadata": {
        "id": "JrkYrr6qHwp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. Why do we have to perform EDA before fitting a model to the data?**\n",
        "\n",
        "Exploratory Data Analysis (EDA) is a critical step in the machine learning workflow that involves examining the data to understand its structure, patterns, and potential issues. Performing EDA before fitting a model ensures that the data is clean, relevant, and in a format suitable for building effective models.\n",
        "\n",
        "**Reasons to Perform EDA**\n",
        "\n",
        "**Understand the Dataset**\n",
        "\n",
        "\n",
        "**Feature Types:** Identify the types of features (numerical, categorical, or text) to determine how to preprocess them.\n",
        "\n",
        "**Target Variable:** Understand the distribution of the target variable to choose appropriate evaluation metrics or modeling approaches.\n",
        "\n",
        "**Relationships:** Discover relationships between features and the target variable to inform feature selection and engineering.\n",
        "Identify Data Quality Issues\n",
        "\n",
        "**Missing Values:** Locate missing or incomplete data that could negatively impact model performance.\n",
        "\n",
        "**Outliers:** Detect extreme values that could distort predictions or influence model parameters.\n",
        "\n",
        "**Duplicate Records:** Identify and handle duplicate entries that may bias the model.\n",
        "\n",
        "**Detect Patterns and Trends**\n",
        "\n",
        "**Correlation:** Identify correlations between features to reduce multicollinearity or understand feature relevance.\n",
        "\n",
        "**Trends:** Observe patterns, seasonality, or anomalies in time-series data.\n",
        "\n",
        "**Choose the Right Features**\n",
        "\n",
        "**Irrelevant Features:** Remove irrelevant or redundant features that add noise.\n",
        "\n",
        "**Feature Engineering:** Create new features or transformations that better represent the data.\n",
        "\n",
        "**Guide Model Selection**\n",
        "\n",
        "**Distribution Insights:** Understand whether the data distribution suggests using specific algorithms (e.g., linear regression for linear trends, decision trees for nonlinear relationships).\n",
        "\n",
        "**Class Imbalance:** If the target variable is imbalanced, apply resampling techniques like oversampling or undersampling.\n",
        "\n",
        "**Visualize Data**\n",
        "\n",
        "Visualization helps gain insights into data distributions, relationships, and anomalies that are harder to spot numerically.\n",
        "\n",
        "**Reduce Errors**\n",
        "\n",
        "EDA helps uncover and address potential data issues early, preventing poor model performance and reducing debugging time later.\n",
        "Steps in EDA\n",
        "\n",
        "**Understand Dataset Structure:**\n",
        "\n",
        "View dataset dimensions, feature names, and data types.\n",
        "Example: data.info(), data.describe().\n",
        "Handle Missing Data:\n",
        "\n",
        "Check for null values: data.isnull().sum().\n",
        "\n",
        "**Analyze Feature Distributions:**\n",
        "\n",
        "Use histograms, box plots, and density plots.\n",
        "\n",
        "**Check Relationships:**\n",
        "\n",
        "Use scatter plots and pair plots for continuous variables.\n",
        "Use correlation matrices to identify linear relationships.\n",
        "\n",
        "**Identify Outliers:**\n",
        "\n",
        "Use box plots or z-scores.\n",
        "\n",
        "**Visualize Data:**\n",
        "\n",
        "Use bar plots, heatmaps, and line charts.\n"
      ],
      "metadata": {
        "id": "eRY39FLGIgxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. **What is correlation?**\n",
        "\n",
        "**Correlation**\n",
        "\n",
        "Correlation is a statistical measure that expresses the strength and direction of a relationship between two variables. It quantifies how changes in one variable are associated with changes in another. Correlation values range from -1 to +1, and are commonly represented by the correlation coefficient.\n",
        "\n",
        "**Positive Correlation:** When one variable increases, the other variable also increases (e.g., height and weight).\n",
        "\n",
        "**Negative Correlation:** When one variable increases, the other variable decreases (e.g., exercise time and body fat percentage).\n",
        "\n",
        "**No Correlation:** No discernible relationship between the variables (e.g., shoe size and intelligence)."
      ],
      "metadata": {
        "id": "6K6tdyyALCKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13. What does negative correlation mean?**\n",
        "\n",
        "**Negative Correlation**\n",
        "\n",
        "Negative correlation occurs when two variables move in opposite directions. It indicates that as the value of one variable increases, the value of the other decreases, and vice versa.\n",
        "\n",
        "**Key Characteristics:**\n",
        "\n",
        "Correlation coefficient: Between -1 and 0.\n",
        "\n",
        "-1 indicates a perfect negative correlation, where one variable decreases proportionally as the other increases.\n",
        "\n",
        "Closer to 0 indicates a weaker negative relationship.\n",
        "\n",
        "Example: Temperature and Coat Sales: As the temperature decreases (gets colder), coat sales tend to increase.\n",
        "\n",
        "Time Spent Studying and Errors on a Test: More study time typically leads to fewer mistakes on a test."
      ],
      "metadata": {
        "id": "jf3g9vjhLtk7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can you find correlation between variables in Python?\n",
        "\n",
        "**Finding Correlation Between Variables in Python**\n",
        "\n",
        "Correlation measures the strength and direction of a linear relationship between two variables. In Python, we can compute correlation using libraries such as Pandas and NumPy, and visualize it using Seaborn or Matplotlib.\n",
        "\n",
        "**Common Methods to Find Correlation**\n",
        "\n",
        "Using Pandas corr() Method\n",
        "\n",
        "The corr() function computes pairwise correlation between numerical features in a dataset.\n",
        "\n",
        "**Default Method:** Pearson correlation.\n",
        "**Other Methods:** Specify method='spearman' or method='kendall' for rank-based correlation."
      ],
      "metadata": {
        "id": "3LNC6UUXL8HV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "data = {\n",
        "    'Feature1': [1, 2, 3, 4, 5],\n",
        "    'Feature2': [2, 4, 6, 8, 10],\n",
        "    'Feature3': [5, 3, 4, 2, 1]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "correlation_matrix = df.corr()\n",
        "print(correlation_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj535ycdM-Oj",
        "outputId": "59b0fffd-eb25-4bc0-dbfe-39987f927709"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Feature1  Feature2  Feature3\n",
            "Feature1       1.0       1.0      -0.9\n",
            "Feature2       1.0       1.0      -0.9\n",
            "Feature3      -0.9      -0.9       1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using NumPy** corrcoef() NumPy provides a corrcoef() function to calculate the Pearson correlation coefficient.\n",
        "\n",
        "The diagonal represents self-correlation (always 1), and the off-diagonal represents the correlation between x and y."
      ],
      "metadata": {
        "id": "OFEcptJENLcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example Arrays\n",
        "x = [1, 2, 3, 4, 5]\n",
        "y = [2, 4, 6, 8, 10]\n",
        "\n",
        "# Correlation Coefficient\n",
        "correlation = np.corrcoef(x, y)\n",
        "print(correlation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vd42kHO8NAJp",
        "outputId": "103aa56b-2054-445e-da19-8f3b0c2d0265"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 1.]\n",
            " [1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing Correlation with a Heatmap Use Seaborn to create a heatmap for a better visual representation of correlations."
      ],
      "metadata": {
        "id": "AxWYX0IkNgIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Correlation Matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Heatmap\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "hCBK6k1hNfIf",
        "outputId": "e0533423-d268-4d55-a11f-66d2d97ae4f2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**15. What is causation? Explain difference between correlation and causation with an example.**\n",
        "\n",
        "**Causation and Its Difference from Correlation**\n",
        "\n",
        "Causation, also known as causality, refers to a relationship where one event or variable directly influences or causes another. For example, if an increase in variable A leads to an increase or decrease in variable B, we say A causes B. Establishing causation often requires controlled experiments or deep analysis to eliminate confounding factors.\n",
        "\n",
        "On the other hand, correlation indicates a statistical association between two variables—when one variable changes, the other tends to change too. However, correlation does not imply causation. Two variables can be correlated without one causing the other; their relationship could be coincidental or influenced by a third factor (a confounder).\n",
        "\n",
        "Example: Consider a scenario where ice cream sales and drowning incidents are positively correlated—they both increase during the summer months. This does not mean eating ice cream causes drowning. Instead, the actual causative factor is the hot weather, which leads to more people buying ice cream and swimming, thus increasing drowning risks. Here, correlation exists between ice cream sales and drowning incidents, but there is no causal link between them.\n",
        "\n",
        "In summary, correlation shows association, while causation proves a direct cause-and-effect relationship. Understanding the difference is crucial in data analysis to avoid drawing incorrect conclusions."
      ],
      "metadata": {
        "id": "4ByfMrvVNtMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**16. What is an Optimizer? What are different types of optimizers? Explain each with an example.**\n",
        "\n",
        "An optimizer is a function or algorithm used to adjust the weights and biases of a machine learning model during training to minimize the loss function. It determines how the model learns by updating the parameters based on the gradient of the loss with respect to these parameters. The goal of optimization is to find the best parameter values that lead to the lowest possible loss and the most accurate predictions.\n",
        "\n",
        "**Types of Optimizers and Their Explanation**\n",
        "\n",
        "**Gradient Descent**\n",
        "\n",
        "Description: A basic optimization algorithm that updates the parameters by moving in the direction of the negative gradient of the loss function.\n",
        "\n",
        "**Drawback:** It can be slow, especially for large datasets\n",
        "\n",
        "**Stochastic Gradient Descent (SGD)**\n",
        "\n",
        "**Description:** Instead of computing the gradient for the entire dataset, it computes the gradient for a single sample or a mini-batch, making it faster and suitable for large datasets.\n",
        "\n",
        "\n",
        "```\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "optimizer = SGD(learning_rate=0.01)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "**Drawback:** The noisy nature of updates may lead to less stable convergence.\n",
        "\n",
        "**Momentum**\n",
        "\n",
        "**Description:** Improves upon SGD by adding a momentum term that helps the optimizer accelerate in the relevant direction and dampen oscillations.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "**Benefit:** Faster convergence for optimization."
      ],
      "metadata": {
        "id": "nSj8_PjfQx_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**17. What is sklearn.linear_model ?**\n",
        "\n",
        "The sklearn.linear_model module in Scikit-learn provides a collection of linear models for regression and classification tasks. These models are based on linear relationships between input features and the target variable, offering simple, interpretable, and efficient solutions for many machine learning problems.\n",
        "\n",
        "**Key Models in sklearn.linear_model**\n",
        "\n",
        "**Linear Regression (LinearRegression)**\n",
        "\n",
        "**Purpose:** Solves regression problems by fitting a linear equation to the data.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "**Logistic Regression (LogisticRegression)**\n",
        "\n",
        "**Purpose:** Used for binary or multi-class classification problems.\n",
        "\n",
        "**Equation:** Uses the sigmoid function to map linear predictions to probabilities.\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "```\n",
        "\n",
        "**Ridge Regression (Ridge)**\n",
        "\n",
        "**Purpose:** Adds L2 regularization to Linear Regression to reduce overfitting by penalizing large coefficients.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.linear_model import Ridge\n",
        "model = Ridge(alpha=1.0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "```\n",
        "\n",
        "The sklearn.linear_model module is a powerful toolbox for solving linear regression and classification problems with options for regularization and optimization. It provides versatile models like LinearRegression, LogisticRegression, Ridge, and Lasso, making it an essential component for machine learning practitioners.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TXBobVUFeVl4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**18. What does model.fit() do? What arguments must be given?**\n",
        "\n",
        "The model.fit() function is a method used in machine learning libraries like Scikit-learn and TensorFlow to train a model on a given dataset. It fits the model to the training data by learning the relationship between the input features and the target labels, adjusting the model parameters  to minimize the loss function.\n",
        "\n",
        "**Steps Performed by model.fit()**\n",
        "\n",
        "**Initialize Parameters:** Sets up initial values for the model parameters.\n",
        "\n",
        "**Forward Pass:** Predicts outputs using the current model parameters.\n",
        "\n",
        "**Compute Loss:** Calculates the difference between predicted and actual outputs using a loss function.\n",
        "\n",
        "**Backward Pass:** Updates the model parameters using optimization algorithms (e.g., Gradient Descent).\n",
        "\n",
        "**Iterate:** Repeats the process over multiple iterations (epochs) until convergence or a stopping criterion is met.\n",
        "\n",
        "**Required Arguments for model.fit()**\n",
        "The required arguments depend on the library and type of model. In Scikit-learn:\n",
        "\n",
        "X (Input Features):\n",
        "\n",
        "The training data consisting of independent variables.\n",
        "\n",
        "**Format:** A 2D array or DataFrame where each row is a sample and each column is a feature.\n",
        "\n",
        "y (Target Labels):\n",
        "\n",
        "The dependent variable (target) corresponding to the input features.\n",
        "\n",
        "**Format:** A 1D array or Series for regression or classification problems.\n",
        "\n"
      ],
      "metadata": {
        "id": "rb7efpRbfaE9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**19. What does model.predict() do? What arguments must be given?**\n",
        "\n",
        "The model.predict() method is used in machine learning models to generate predictions based on the input data provided. After training the model using model.fit(), the predict() method applies the learned relationships (parameters like weights and biases) to new, unseen input data to predict the target variable.\n",
        "\n",
        "**Steps Performed by model.predict()**\n",
        "\n",
        "**Input Transformation:** Takes new input features and processes them into a format compatible with the trained model.\n",
        "\n",
        "**Forward Pass:** Uses the model's parameters (learned during training) to compute predictions for the given input.\n",
        "\n",
        "**Output Predictions:** Returns the predicted values, which could be:\n",
        "\n",
        "Continuous values (for regression tasks).\n",
        "\n",
        "Class probabilities or labels (for classification tasks).\n",
        "\n",
        "**Required Arguments for model.predict()**\n",
        "\n",
        "The specific arguments required depend on the library and model type, but typically include:\n",
        "\n",
        "X (Input Features):\n",
        "The data for which predictions are to be made.\n",
        "\n",
        "Format: A 2D array or DataFrame where each row is a sample and each column is a feature."
      ],
      "metadata": {
        "id": "y2vZr9BpgSPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**20. What are continuous and categorical variables?**\n",
        "\n",
        "In data analysis and machine learning, variables are classified based on the type of data they represent. The two common types are continuous and categorical variables.\n",
        "\n",
        "**1. Continuous Variables**\n",
        "\n",
        "**Definition:** Continuous variables can take any value within a given range. They are numerical and represent measurable quantities.\n",
        "\n",
        "Examples: Height, weight, temperature, salary, age, etc.\n",
        "\n",
        "**Characteristics:**\n",
        "Values are infinite within a range (e.g., 3.14, 3.141, 3.1415...).\n",
        "Can be fractions or decimals.\n",
        "\n",
        "Suitable for regression problems in machine learning.\n",
        "\n",
        "Example:\n",
        "\n",
        "A dataset containing house prices ($ 100,000, $  150,000, $ 200,000) or temperatures (25.5°C, 30.1°C).\n",
        "\n",
        "\n",
        "Continuous and Categorical Variables\n",
        "In data analysis and machine learning, variables are classified based on the type of data they represent. The two common types are continuous and categorical variables.\n",
        "\n",
        "1. Continuous Variables\n",
        "Definition: Continuous variables can take any value within a given range. They are numerical and represent measurable quantities.\n",
        "Examples: Height, weight, temperature, salary, age, etc.\n",
        "Characteristics:\n",
        "Values are infinite within a range (e.g., 3.14, 3.141, 3.1415...).\n",
        "Can be fractions or decimals.\n",
        "Suitable for regression problems in machine learning.\n",
        "Example:\n",
        "\n",
        "A dataset containing house prices ($100,000, $150,000, $200,000) or temperatures (25.5°C, 30.1°C).\n",
        "\n",
        "**2. Categorical Variables**\n",
        "\n",
        "Definition: Categorical variables represent discrete categories or groups. These values are labels rather than numbers.\n",
        "\n",
        "Examples: Gender (male/female), color (red/green/blue), customer type (new/returning).\n",
        "\n",
        "**Characteristics:**\n",
        "\n",
        "Data is qualitative, not numeric.\n",
        "Can be nominal (no inherent order, e.g., colors) or ordinal (has an order, e.g., low/medium/high).\n",
        "\n",
        "Often encoded into numerical formats (e.g., one-hot encoding) for use in machine learning.\n",
        "Example:\n",
        "\n",
        "A dataset with car brands (Toyota, Ford, BMW) or job roles (Engineer, Manager, Analyst).\n",
        "\n"
      ],
      "metadata": {
        "id": "zzqWvy0RhG3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**21.What is feature scaling? How does it help in Machine Learning?**\n",
        "\n",
        "Feature scaling is the process of transforming the values of numerical features in a dataset so that they fall within a specific range or have a standard scale. The goal is to ensure that each feature contributes equally to the model, preventing any one feature from dominating others due to its scale or units.\n",
        "\n",
        "**Why is Feature Scaling Important?**\n",
        "\n",
        "**Improves Model Performance:**\n",
        "\n",
        "Some machine learning algorithms, like Gradient Descent, are sensitive to the scale of the data. Features with larger numerical ranges can disproportionately influence the model, causing convergence issues or slow training.\n",
        "\n",
        "Distance-based algorithms (e.g., K-Nearest Neighbors (KNN), K-Means Clustering, Support Vector Machines (SVM)) are particularly sensitive to feature scales, as they rely on calculating distances between data points.\n",
        "\n",
        "**Helps Convergence in Optimization:**\n",
        "\n",
        "Algorithms that use optimization techniques (e.g., linear regression, neural networks) perform better when features are on the same scale because it helps the algorithm converge faster and more reliably.\n",
        "\n",
        "**Ensures Fair Feature Weighting:**\n",
        "\n",
        "When features are on different scales, those with larger values may dominate the model's learning process, leading to biased results. Scaling ensures all features contribute equally.\n",
        "\n",
        "**When to Apply Feature Scaling?**\n",
        "\n",
        "Before fitting algorithms like KNN, SVM, Logistic Regression, Neural Networks, and others that rely on distance or gradient-based methods.\n",
        "\n",
        "Not necessary for decision tree-based algorithms (Random Forest, Gradient Boosting) as they are not sensitive to feature scaling.\n",
        "\n",
        "Feature Scaling standardizes the range or distribution of numerical features to improve the performance and accuracy of machine learning models.\n",
        "\n",
        "Common techniques include Min-Max Scaling, Standardization, Robust Scaling, and Max Abs Scaling.\n",
        "\n",
        "Feature scaling helps with faster convergence, better model performance, and ensures fair contribution from all features.\n"
      ],
      "metadata": {
        "id": "_FoSvHJph2jp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**22. How do we perform scaling in Python?**\n",
        "\n",
        "In Python, scaling of features is commonly performed using Scikit-learn's preprocessing module. It provides several utilities to scale your data using techniques like Min-Max scaling, Standardization (Z-score normalization), and Robust scaling.\n",
        "\n",
        "Here’s how you can perform different types of feature scaling in Python:\n",
        "\n",
        "1. Min-Max Scaling (Normalization)\n",
        "Min-Max Scaling transforms the data to fit within a specific range, typically [0, 1].\n",
        "\n"
      ],
      "metadata": {
        "id": "q9dg12SCsjmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(X_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAXuLgjJNnOv",
        "outputId": "6c0b0808-0a30-41fc-96a6-34ae3afa5873"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.        ]\n",
            " [0.33333333 0.33333333]\n",
            " [0.66666667 0.66666667]\n",
            " [1.         1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardization (Z-score Normalization)\n",
        "# Standardization transforms data so that it has a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(X_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0j_P58otGgY",
        "outputId": "8b2b8ca0-c857-4435-a483-eaf87e84116e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.34164079 -1.34164079]\n",
            " [-0.4472136  -0.4472136 ]\n",
            " [ 0.4472136   0.4472136 ]\n",
            " [ 1.34164079  1.34164079]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Robust Scaling\n",
        "# Robust scaling uses the median and interquartile range (IQR), making it robust to outliers.\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "X = np.array([[1, 2], [3, 4], [5, 6], [100, 200]])\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = RobustScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(X_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udBJLzWotRdp",
        "outputId": "02aab79f-a6d3-4b98-808c-30987f5152ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.11428571 -0.05882353]\n",
            " [-0.03809524 -0.01960784]\n",
            " [ 0.03809524  0.01960784]\n",
            " [ 3.65714286  3.82352941]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Max Abs Scaling\n",
        "# Max Abs scaling scales the data by dividing each feature by its maximum absolute value, keeping the sign intact.\n",
        "\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "X = np.array([[1, -2], [3, 4], [-5, 6]])\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = MaxAbsScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(X_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chrcksN_tbcg",
        "outputId": "2e6f97eb-c3ff-4574-d614-241ff84d1314"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.2        -0.33333333]\n",
            " [ 0.6         0.66666667]\n",
            " [-1.          1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Min-Max Scaling: Transforms data into a [0, 1] range. Use MinMaxScaler.\n",
        "\n",
        "Standardization: Centers data with a mean of 0 and standard deviation of 1. Use StandardScaler.\n",
        "\n",
        "Robust Scaling: Uses the median and IQR to scale, robust to outliers. Use RobustScaler.\n",
        "\n",
        "Max Abs Scaling: Scales by the maximum absolute value, keeping sign. Use MaxAbsScaler."
      ],
      "metadata": {
        "id": "W2Oeg2JQuF3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**23. What is sklearn.preprocessing?**\n",
        "\n",
        "sklearn.preprocessing is a module within Scikit-learn (a Python library for machine learning) that provides various utilities and functions for data preprocessing, particularly for preparing and transforming your dataset before applying machine learning models.\n",
        "\n",
        "Preprocessing is an essential step in machine learning, as it involves preparing raw data into a format that improves the performance and accuracy of the models. The sklearn.preprocessing module provides tools to standardize, normalize, encode, and scale the data.\n",
        "\n",
        "**1. Scaling and Normalization**\n",
        "\n",
        "These techniques adjust the range of the data values to ensure that features contribute equally to the model.\n",
        "\n",
        "MinMaxScaler: Scales data to a given range, usually [0, 1].\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "```\n",
        "\n",
        "StandardScaler: Standardizes features by removing the mean and scaling to unit variance (Z-score normalization).\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "```\n",
        "RobustScaler: Scales data using the median and interquartile range (IQR), making it robust to outliers.\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "```\n",
        "\n",
        "MaxAbsScaler: Scales data by the maximum absolute value of each feature.\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "scaler = MaxAbsScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "```\n",
        "\n",
        "**2. Encoding Categorical Data**\n",
        "These methods convert categorical variables into a numerical format so that they can be used by machine learning algorithms.\n",
        "\n",
        "OneHotEncoder: Encodes categorical features as a one-hot numeric array (binary vector representation).\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "X_encoded = encoder.fit_transform(X)\n",
        "\n",
        "```\n",
        "\n",
        "LabelEncoder: Converts categorical labels (e.g., strings or class names) into numeric labels.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "y_encoded = encoder.fit_transform(y)\n",
        "\n",
        "```\n",
        "\n",
        "3. Polynomial Features\n",
        "Generates polynomial and interaction features from the input features, useful for algorithms like polynomial regression.\n",
        "\n",
        "PolynomialFeatures: Generates polynomial features and interaction terms.\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "```\n",
        "\n",
        "**4. Binarization**\n",
        "Binarization transforms numeric data into binary values (0 or 1), depending on whether they are above or below a given threshold.\n",
        "\n",
        "Binarizer: Converts continuous data to binary (0/1) based on a threshold.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import Binarizer\n",
        "binarizer = Binarizer(threshold=0.5)\n",
        "X_binary = binarizer.fit_transform(X)\n",
        "\n",
        "```\n",
        "\n",
        "**5. Scaling Time-Series Data**\n",
        "\n",
        "Time-series data might require specific preprocessing techniques like scaling or filling missing values.\n",
        "\n",
        "FunctionTransformer: Applies custom transformations to data.\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "transformer = FunctionTransformer(np.log1p)\n",
        "X_transformed = transformer.fit_transform(X)\n",
        "```\n",
        "\n",
        "**6. Feature Selection and Transformation**\n",
        "Used to remove or combine features that do not improve model performance.\n",
        "\n",
        "QuantileTransformer: Transforms features using quantiles for robust scaling.\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "scaler = QuantileTransformer(output_distribution='uniform')\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o-7fiS9EuM8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**24. How do we split data for model fitting (training and testing) in Python?**\n",
        "\n",
        "In Python, data is typically split into training and testing sets before fitting a model. This is done to ensure that the model is evaluated on unseen data, which helps assess its generalization ability.\n",
        "\n",
        "The most common way to split data is using Scikit-learn's train_test_split() function.\n",
        "\n",
        "**Steps to Split Data for Model Fitting:**\n",
        "Import the necessary libraries: You need train_test_split from Scikit-learn and a dataset (which could be a pandas DataFrame, NumPy array, or any other array-like structure).\n",
        "\n",
        "**Split the data:** You use train_test_split() to split your features (X) and target variable (y) into training and testing sets.\n",
        "\n",
        "**Fit the model:** You can then fit the model using the training data and evaluate it on the test data.\n",
        "\n",
        "Explanation of train_test_split() Parameters:\n",
        "\n",
        "1. X and y:\n",
        "\n",
        "X represents the feature matrix (input data).\n",
        "y represents the target variable (output labels).\n",
        "test_size:\n",
        "\n",
        "2. Specifies the proportion of the dataset to include in the test split.\n",
        "For example, test_size=0.2 means 20% of the data will be used for testing, and the remaining 80% will be used for training.\n",
        "train_size:\n",
        "\n",
        "3 .train_size Specifies the proportion of the dataset to include in the training split. It can be used instead of test_size but not both.\n",
        "\n",
        "4. random_state:\n",
        "\n",
        " Sets a random seed to ensure reproducibility. If the same random_state value is provided, you will get the same train-test split every time.\n",
        "\n",
        "5. shuffle:\n",
        "\n",
        "By default, the data is shuffled before splitting. Set shuffle=False if you do not want to shuffle the data.\n",
        "\n",
        "6. stratify:\n",
        "\n",
        " Ensures that the class distribution in the target variable (y) is similar in both the training and testing sets. This is especially important in classification problems with imbalanced datasets."
      ],
      "metadata": {
        "id": "0rtnQYDG84yz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**25. Explain data encoding?**\n",
        "\n",
        "Data encoding refers to the process of converting categorical data into a numerical format that machine learning algorithms can interpret and process. Since most machine learning models require numerical input, categorical variables (such as strings or categories) need to be transformed into numerical values to be used effectively in modeling.\n",
        "\n",
        "Encoding is essential when working with algorithms like Logistic Regression, K-Nearest Neighbors (KNN), Support Vector Machines (SVM), or Neural Networks, as they cannot directly handle categorical data.\n",
        "\n",
        "Common Types of Data Encoding Techniques:\n",
        "Here are the most widely used encoding techniques for categorical variables:\n",
        "\n",
        "1. Label Encoding\n",
        "Label Encoding converts each category in a categorical variable to a unique integer label. It is simple and works well when there is an ordinal relationship between the categories (e.g., \"Low\", \"Medium\", \"High\").\n",
        "\n",
        "2. One-Hot Encoding\n",
        "One-Hot Encoding creates binary columns for each category, where each column represents a unique category. For each sample, the corresponding category is marked with a 1, and all other categories are marked as 0. This is used when there is no ordinal relationship between categories (e.g., \"dog\", \"cat\", \"fish\").\n",
        "\n",
        "3. Binary Encoding\n",
        "Binary Encoding is a compromise between Label Encoding and One-Hot Encoding. First, it assigns a unique integer to each category, and then converts these integers to binary code. This is particularly useful when the number of categories is large, as it reduces dimensionality compared to One-Hot Encoding.\n",
        "\n",
        "4. Frequency (or Count) Encoding\n",
        "Frequency Encoding assigns a numerical value to each category based on the frequency of that category in the dataset. Each category is replaced by the count or proportion of its occurrences.\n",
        "\n",
        "5. Target Encoding (Mean Encoding)\n",
        "Target Encoding involves encoding categorical features based on the mean of the target variable for each category. It is often used in regression or classification problems where the target variable is numeric or categorical.\n",
        "\n",
        "**Summary**\n",
        "Label Encoding: Assigns a unique integer to each category (useful for ordinal categories).\n",
        "\n",
        "One-Hot Encoding: Creates binary columns for each category (useful for nominal categories).\n",
        "\n",
        "Binary Encoding: Converts categories to binary values, reducing dimensionality.\n",
        "\n",
        "Frequency Encoding: Encodes categories based on their frequency in the data.\n",
        "\n",
        "Target Encoding: Encodes categories based on the mean of the target variable.\n",
        "\n",
        "Choosing the right encoding technique depends on the nature of your data and the model you're using."
      ],
      "metadata": {
        "id": "XytXHKCc-Bgi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U5eZGVp8uArv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}