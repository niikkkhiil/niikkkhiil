{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJraeus6Hf/KXR1CLnZbWQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niikkkhiil/niikkkhiil/blob/main/Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is Simple Linear Regression**\n",
        "\n",
        "**Simple Linear Regression**\n",
        "\n",
        "Simple Linear Regression is a statistical method used to model the relationship between a dependent variable (Y) and a single independent variable (X) using a straight line. The goal is to find the best-fitting line that predicts the dependent variable based on the independent variable.\n",
        "\n",
        "Mathematical Equation\n",
        "The equation of a simple linear regression model is:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝜖\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑌\n",
        " = Dependent variable (target/output)\n",
        "\n",
        "𝑋\n",
        " = Independent variable (predictor/input)\n",
        "\n",
        "𝑏\n",
        "0\n",
        "​\n",
        "  = Intercept (value of Y when X = 0)\n",
        "\n",
        "𝑏\n",
        "1\n",
        "​\n",
        "  = Slope (change in Y for a unit increase in X)\n",
        "\n",
        "𝜖 = Error term (captures unexplained variations)\n",
        "\n",
        "\n",
        "**Use Cases**\n",
        "\n",
        "Predicting house prices based on square footage.\n",
        "\n",
        "Estimating sales revenue based on advertising budget.\n",
        "\n",
        "Forecasting stock prices based on past performance."
      ],
      "metadata": {
        "id": "hRnBXdnNzOe6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. What are the key assumptions of Simple Linear Regression**\n",
        "\n",
        "\n",
        "Simple Linear Regression relies on several key assumptions to ensure the validity and reliability of the model. These assumptions are:\n",
        "\n",
        "**1. Linearity**\n",
        "\n",
        "The relationship between the dependent variable (response) and the independent variable (predictor) is linear. This means the change in the response variable is proportional to the change in the predictor.\n",
        "\n",
        "**2. Independence**\n",
        "\n",
        "The observations are independent of each other. For example, the residuals (errors) should not be correlated.\n",
        "\n",
        "**3. Homoscedasticity** (Constant Variance)\n",
        "\n",
        "The variance of the residuals is constant across all levels of the independent variable. This means the spread of residuals should remain consistent, not increasing or decreasing with changes in the predictor variable.\n",
        "\n",
        "**4. Normality of Residuals**\n",
        "\n",
        "The residuals (errors) are normally distributed. This is particularly important for hypothesis testing and confidence interval estimation.\n",
        "\n",
        "**5. No Perfect Multicollinearity** (for Multiple Regression)\n",
        "\n",
        "While this assumption specifically applies to multiple regression, it implies that predictor variables should not be perfectly correlated with each other.\n",
        "\n",
        "**6. No Autocorrelation**\n",
        "\n",
        "This applies particularly when dealing with time-series data. Residuals should not show patterns or correlations with each other over time.\n",
        "\n",
        "**7. Exogeneity**\n",
        "The independent variable is measured without error and is not correlated with the error term.\n",
        "\n",
        "\n",
        "**Diagnostic Checks:**\n",
        "\n",
        "Residual Plots: To check linearity and homoscedasticity.\n",
        "\n",
        "Q-Q Plots: To check normality of residuals.\n",
        "\n",
        "Durbin-Watson Test: To test for autocorrelation.\n",
        "\n",
        "Variance Inflation Factor (VIF): To detect multicollinearity (in multiple regression).\n"
      ],
      "metadata": {
        "id": "Tr-ShBIN2uD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. What does the coefficient m represent in the equation Y=mX+c**\n",
        "\n",
        "In the equation\n",
        "𝑌\n",
        "=\n",
        "𝑚\n",
        "𝑋\n",
        "+\n",
        "𝑐\n",
        ", the coefficient\n",
        "𝑚 represents the slope of the line. It quantifies the rate of change of the dependent variable\n",
        "𝑌\n",
        "Y with respect to the independent variable\n",
        "𝑋.\n",
        "\n",
        "What\n",
        "𝑚\n",
        "Means:\n",
        "\n",
        "Magnitude of\n",
        "𝑚:\n",
        "∣\n",
        "𝑚\n",
        "∣ (the absolute value of\n",
        "𝑚\n",
        ") indicates the steepness of the line. A larger\n",
        "∣\n",
        "𝑚\n",
        "∣\n",
        " means a steeper slope.\n",
        "\n",
        "Sign of\n",
        "𝑚\n",
        ":\n",
        "\n",
        "Positive\n",
        "𝑚: Indicates a positive relationship between\n",
        "𝑋 and\n",
        "𝑌 (as\n",
        "𝑋\n",
        " increases,\n",
        "𝑌 increases).\n",
        "\n",
        "Negative\n",
        "𝑚: Indicates a negative relationship between\n",
        "𝑋\n",
        " and\n",
        "𝑌\n",
        " (as\n",
        "𝑋\n",
        " increases,\n",
        "𝑌\n",
        " decreases).\n",
        "\n",
        "Interpretation:\n",
        "𝑚\n",
        " represents the change in\n",
        "𝑌\n",
        " for a one-unit increase in\n",
        "𝑋\n",
        ".\n"
      ],
      "metadata": {
        "id": "MizNMDQNL1Ls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. What does the intercept c represent in the equation Y=mX+c**\n",
        "\n",
        "In the equation\n",
        "𝑌\n",
        "=\n",
        "𝑚\n",
        "𝑋\n",
        "+\n",
        "𝑐\n",
        ", the intercept\n",
        "𝑐\n",
        " represents the value of\n",
        "𝑌\n",
        " when\n",
        "𝑋\n",
        "=\n",
        "0.\n",
        "\n",
        "In other words:\n",
        "\n",
        "It is the point where the line crosses the\n",
        "𝑌-axis in a graph.\n",
        "\n",
        "It is also called the vertical intercept or simply the intercept.\n",
        "\n",
        "Example\n",
        "\n",
        "If the equation is\n",
        "Y=2X+5:\n",
        "\n",
        "When\n",
        "X=0,\n",
        "Y=5.\n",
        "\n",
        "So, the intercept\n",
        "c=5, meaning the line crosses the\n",
        "Y-axis at\n",
        "(0,5).\n",
        "\n",
        "In practical contexts:\n",
        "c could represent a starting value or baseline, depending on the situation being modeled (e.g., a fixed cost in a business context or an initial condition in physics).\n"
      ],
      "metadata": {
        "id": "LagUf6hbHdeB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. How do we calculate the slope m in Simple Linear Regression.**\n",
        "\n",
        "The slope in a regression equation can be found by calculating the coefficient associated with the independent variable(s) in the regression model.\n",
        "To find the slope in a regression equation, you typically perform a regression analysis, which estimates the relationship between variables in a dataset. Here's how you can find the slope in detail:\n",
        "\n",
        "**Choose a Regression Model:**\n",
        "Depending on the nature of your data and the relationship you're trying to model, you may choose from various regression models like linear regression, polynomial regression, or multiple regression.\n",
        "\n",
        "**Fit the Regression Model:**\n",
        "Use statistical methods or machine learning algorithms to fit the regression model to your dataset. This involves finding the parameters (including the slope) that best describe the relationship between the independent and dependent variables.\n",
        "\n",
        "**Extract the Slope Coefficient:**\n",
        "Once the regression model is fitted, examine the coefficients associated with the independent variables. In a simple linear regression model with one independent variable, the slope coefficient represents the change in the dependent variable for a one-unit change in the independent variable.\n",
        "For example, in the equation of a simple linear regression model Y = β0 ​+β1​X+ϵ, the coefficient β1​ represents the slope.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ftXVbOug_1Od"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. What is the purpose of the least squares method in Simple Linear Regression.**\n",
        "\n",
        "The purpose of the least squares method in Simple Linear Regression is to find the line (regression line) that best fits the data by minimizing the sum of the squared differences (errors) between the actual values (\n",
        "𝑌\n",
        "𝑖\n",
        "Y\n",
        "i\n",
        "​\n",
        " ) and the predicted values (\n",
        "𝑌\n",
        "^\n",
        "𝑖\n",
        "Y\n",
        "^\n",
        "  \n",
        "i\n",
        "​\n",
        " ) from the regression line.\n",
        "\n",
        "Key Goals:\n",
        "\n",
        "**Minimize Prediction Errors:** The method minimizes the total squared errors:\n",
        "\n",
        "Sum of Squared Errors (SSE)\n",
        "=\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "(\n",
        "𝑌\n",
        "𝑖\n",
        "−\n",
        "𝑌\n",
        "^\n",
        "𝑖\n",
        ")\n",
        "\n",
        "\n",
        "2.\n",
        "Sum of Squared Errors (SSE)=\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " (Y\n",
        "i\n",
        "​\n",
        " −\n",
        "Y\n",
        "^\n",
        "i\n",
        "​\n",
        " )\n",
        "2\n",
        "\n",
        "Here,\n",
        "\n",
        "Y is the actual value, and\n",
        "\n",
        "Y\n",
        "^i is the predicted value using the equation:\n",
        "\n",
        "Y\n",
        "^i =mX i+c\n",
        "\n",
        "Find the Best Fit Line: By minimizing the SSE, the least squares method ensures the line fits the data points as closely as possible.\n",
        "\n",
        "Why Minimize Squared Errors?\n",
        "\n",
        "Error Emphasis: Squaring amplifies larger errors, ensuring the model prioritizes reducing significant deviations.\n",
        "\n",
        "Mathematical Convenience: Squared terms are differentiable, making it easier to calculate the slope (\n",
        "m) and intercept (\n",
        "c).\n",
        "\n",
        "Avoid Cancellation: Squaring prevents positive and negative errors from canceling each other out."
      ],
      "metadata": {
        "id": "59VWBveuBRzL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7 How is the coefficient of determination (R²) interpreted in Simple Linear Regression.**\n",
        "\n",
        "The coefficient of determination (\n",
        "R\n",
        "2\n",
        " ) in Simple Linear Regression is a statistical measure that indicates the proportion of the variance in the dependent variable (Y) that is explained by the independent variable (\n",
        "X).\n",
        "\n",
        "Formula:\n",
        "𝑅\n",
        "2\n",
        "=\n",
        "1\n",
        "−\n",
        "SSE /\n",
        "SST\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "SSE: Sum of Squared Errors (\n",
        "∑(Y\n",
        "i\n",
        "​\n",
        " −\n",
        "Y\n",
        "^\n",
        "i\n",
        " )\n",
        "2\n",
        " ) – the variation in\n",
        "Y not explained by the model.\n",
        "\n",
        "\n",
        "SST: Total Sum of Squares (\n",
        "∑(Y\n",
        "i\n",
        " −\n",
        "Y\n",
        "ˉ )\n",
        "2\n",
        " ) – the total variation in\n",
        "Y\n",
        "\n",
        "If you are predicting house prices based on square footage and\n",
        "R2 =0.85, this means that 85% of the variation in house prices is explained by square footage, while 15% is due to other factors (e.g., location, age of the house, etc.).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "okgnda_aFQuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8 What is Multiple Linear Regression.**\n",
        "\n",
        "Multiple Linear Regression (MLR) is an extension of Simple Linear Regression that models the relationship between a dependent variable (\n",
        "𝑌\n",
        "Y) and two or more independent variables (\n",
        "𝑋\n",
        "1\n",
        ",\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑋\n",
        "𝑛\n",
        "X\n",
        "1\n",
        "​\n",
        " ,X\n",
        "2\n",
        "​\n",
        " ,…,X\n",
        "n\n",
        "​\n",
        " ).\n",
        "\n",
        "\n",
        "**Purpose of MLR:**\n",
        "\n",
        "To understand the relationship between multiple predictors and the response variable.\n",
        "\n",
        "To predict the value of the dependent variable (\n",
        "Y) based on multiple independent variables (\n",
        "X).\n",
        "\n",
        "To evaluate the contribution of each independent variable in explaining the variation in the dependent variable.\n",
        "\n",
        "\n",
        "**Assumptions of MLR:**\n",
        "\n",
        "Linearity: The relationship between the dependent variable and independent variables is linear.\n",
        "\n",
        "Independence: Observations are independent of each other.\n",
        "\n",
        "Homoscedasticity: The variance of residuals (errors) is constant across all levels of the independent variables.\n",
        "\n",
        "Normality: Residuals are normally distributed.\n",
        "\n",
        "No Multicollinearity: Independent variables should not be highly correlated with each other.\n",
        "\n",
        "\n",
        "**Applications:**\n",
        "\n",
        "Business: Forecasting sales based on multiple factors like advertising spend, price, and seasonality.\n",
        "\n",
        "Economics: Studying the impact of education, income, and experience on employment outcomes.\n",
        "\n",
        "Healthcare: Predicting patient outcomes based on age, medical history, and treatment type."
      ],
      "metadata": {
        "id": "hGVqwfLMGO9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. What is the main difference between Simple and Multiple Linear Regression?**\n",
        "\n",
        "\n",
        "The main difference between Simple Linear Regression and Multiple Linear Regression lies in the number of independent variables used to predict the dependent variable.\n",
        "\n",
        "**Simple Linear Regression (SLR):**\n",
        "Definition: Models the relationship between one dependent variable (\n",
        "Y) and a single independent variable (X).\n",
        "\n",
        "Equation:\n",
        "Y=mX+c\n",
        "\n",
        "Key Feature: Only one predictor variable (X) is used to explain or predict the outcome (Y).\n",
        "\n",
        "Example: Predicting house price (Y) based solely on square footage (X):\n",
        "\n",
        "Price=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " (Square Footage)+ϵ\n",
        "\n",
        "\n",
        " Multiple Linear Regression (MLR):\n",
        "Definition: Models the relationship between one dependent variable (\n",
        "𝑌\n",
        "Y) and two or more independent variables (\n",
        "𝑋\n",
        "1\n",
        ",\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑋\n",
        "𝑛\n",
        "X\n",
        "1\n",
        "​\n",
        " ,X\n",
        "2\n",
        "​\n",
        " ,…,X\n",
        "n\n",
        "​\n",
        " ).\n",
        "\n",
        "\n",
        "Equation:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "+\n",
        "𝜖\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "\n",
        " +ϵ\n",
        "\n",
        "\n",
        "Key Feature: Multiple predictor variables are used to explain or predict the outcome (\n",
        "\n",
        "Y).\n",
        "\n",
        "Example: Predicting house price (Y) based on square footage (\n",
        "X\n",
        "1\n",
        "​\n",
        " ), number of bedrooms (\n",
        "X\n",
        "2\n",
        "​\n",
        " ), and location score (\n",
        "X\n",
        "3\n",
        "​\n",
        " ):\n",
        "\n",
        "Price\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "(\n",
        "Square Footage\n",
        ")\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "(\n",
        "Bedrooms\n",
        ")\n",
        "+\n",
        "𝛽\n",
        "3\n",
        "(\n",
        "Location Score\n",
        ")\n",
        "+\n",
        "𝜖\n",
        "Price=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " (Square Footage)+β\n",
        "2\n",
        "​\n",
        " (Bedrooms)+β\n",
        "3\n",
        "​\n",
        " (Location Score)+ϵ"
      ],
      "metadata": {
        "id": "O5-Lae9IGvuu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. What are the key assumptions of Multiple Linear Regression?**\n",
        "\n",
        "The key assumptions of Multiple Linear Regression (MLR) ensure the validity of the model's results. These assumptions must be checked before interpreting the coefficients or using the model for prediction.\n",
        "\n",
        "\n",
        "1. Linearity:\n",
        "Assumes a linear relationship between the dependent variable (\n",
        "𝑌\n",
        "Y) and the independent variables (\n",
        "𝑋\n",
        "1\n",
        ",\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑋\n",
        "𝑛\n",
        "X\n",
        "1\n",
        "​\n",
        " ,X\n",
        "2\n",
        "​\n",
        " ,…,X\n",
        "n\n",
        "​\n",
        " ).\n",
        "\n",
        "\n",
        "This means:\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "+\n",
        "𝜖\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        " +ϵ\n",
        "Check: Scatterplots or residual plots.\n",
        "\n",
        "**2. Independence:**\n",
        "\n",
        "Observations must be independent of each other.\n",
        "\n",
        "Violations can occur in time-series or clustered data where observations are correlated.\n",
        "\n",
        "Check: Use the Durbin-Watson test for autocorrelation.\n",
        "\n",
        "\n",
        "**3. Homoscedasticity:**\n",
        "\n",
        "The variance of the residuals (errors) should remain constant across all levels of the independent variables.\n",
        "\n",
        "If the residuals' spread varies, the model might not perform well.\n",
        "\n",
        "Check: Residual vs. fitted value plots; heteroscedasticity tests like Breusch-Pagan.\n",
        "\n",
        "\n",
        "**4. Normality of Residuals:**\n",
        "\n",
        "Residuals (errors) should be approximately normally distributed.\n",
        "\n",
        "This is especially important for small sample sizes, as it affects hypothesis tests and confidence intervals.\n",
        "\n",
        "Check: Q-Q plots, Shapiro-Wilk test, or histogram of residuals.\n",
        "\n",
        "**5. No Multicollinearity:**\n",
        "\n",
        "Independent variables should not be highly correlated with each other.\n",
        "\n",
        "Multicollinearity can inflate standard errors, making it difficult to determine the true effect of each predictor.\n",
        "\n",
        "Check: Variance Inflation Factor (VIF); pairwise correlation matrix.\n",
        "\n",
        "**6. No Omitted Variable Bias:**\n",
        "\n",
        "The model should include all relevant variables that affect the dependent variable.\n",
        "\n",
        "Missing key predictors can bias the coefficients of included variables.\n",
        "\n",
        "\n",
        "**7. Outliers and Influential Points:**\n",
        "\n",
        "Outliers or high-leverage points can disproportionately affect the regression line.\n",
        "\n",
        "Check: Cook's distance, leverage statistics, or standardized residuals.\n",
        "\n",
        "\n",
        "**8. Measurement Accuracy:**\n",
        "\n",
        "Independent variables should be measured accurately without significant error (no measurement error)."
      ],
      "metadata": {
        "id": "HgEDEoMzHoBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model.**\n",
        "\n",
        "**What is Heteroscedasticity?**\n",
        "\n",
        "Heteroscedasticity occurs in a regression model when the variance of the residuals (errors) is not constant across all levels of the independent variables. In other words, the spread of the residuals increases or decreases systematically as the predicted values (\n",
        "𝑌\n",
        "^ ) change.\n",
        "\n",
        "Homoscedasticity (the opposite of heteroscedasticity) means that the residuals have a constant variance, which is a key assumption in Multiple Linear Regression.\n",
        "\n",
        "\n",
        "How Does Heteroscedasticity Affect the Results?\n",
        "\n",
        "**Invalid Statistical Tests:**\n",
        "\n",
        "Heteroscedasticity violates the assumption of constant variance, which affects the reliability of hypothesis tests (e.g., t-tests for coefficients and F-tests for overall model significance).\n",
        "The standard errors of the coefficients become unreliable, leading to:\n",
        "\n",
        "Incorrect p-values: Coefficients may appear significant when they are not, or vice versa.\n",
        "\n",
        "Unreliable confidence intervals.\n",
        "\n",
        "Biased Estimates of Variance:\n",
        "\n",
        "Although the coefficients (\n",
        "𝛽\n",
        "0\n",
        ",\n",
        "𝛽\n",
        "1\n",
        ",\n",
        "…\n",
        "β\n",
        "0\n",
        "​\n",
        " ,β\n",
        "1\n",
        "​\n",
        " ,…) remain unbiased, the estimates of their variability are incorrect, affecting inference and predictions.\n",
        "\n",
        "Reduced Model Efficiency:\n",
        "\n",
        "Heteroscedasticity reduces the efficiency of Ordinary Least Squares (OLS) estimates, making the model less optimal.\n",
        "\n",
        "Misleading Predictions:\n",
        "\n",
        "If the variance of the errors varies greatly, predictions for certain ranges of the independent variables may be less accurate.\n",
        "\n",
        "\n",
        "**Detecting Heteroscedasticity**\n",
        "\n",
        "Visual Inspection: Plot residuals vs. predicted values or independent variables. Look for patterns or non-random spread.\n",
        "\n",
        "**Statistical Tests:**\n",
        "\n",
        "Breusch-Pagan Test: Tests for a relationship between the residual variance and the independent variables.\n",
        "\n",
        "White Test: A more general test for heteroscedasticity.\n",
        "\n",
        "Goldfeld-Quandt Test: Compares variance in two subsets of data.\n",
        "\n",
        "**Addressing Heteroscedasticity**\n",
        "\n",
        "**Transforming Variables:**\n",
        "\n",
        "Apply a transformation (e.g., log, square root, or reciprocal) to the dependent variable or independent variables to stabilize variance.\n",
        "\n",
        "Example: Replace\n",
        "Y with\n",
        "log(Y) if the spread of residuals increases with\n",
        "Y.\n",
        "\n",
        "Weighted Least Squares (WLS):\n",
        "\n",
        "Assign weights to data points inversely proportional to their variance. This adjusts for varying error sizes.\n",
        "Robust Standard Errors:\n",
        "\n",
        "Use heteroscedasticity-consistent standard errors (e.g., White’s robust standard errors) to correct p-values and confidence intervals without transforming the model.\n",
        "\n",
        "Redefine the Model:\n",
        "\n",
        "Add missing variables or interaction terms that may be causing heteroscedasticity."
      ],
      "metadata": {
        "id": "tgWkAbfEI_1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12. How can you improve a Multiple Linear Regression model with high multicollinearity.**\n",
        "\n",
        "**Multicollinearity** occurs when two or more independent variables in a regression model are highly correlated, leading to unstable coefficient estimates and unreliable statistical inferences. It doesn't affect the model's ability to predict but makes it hard to interpret the contribution of each variable.\n",
        "\n",
        "**Methods to Address High Multicollinearity**\n",
        "\n",
        "**Remove Highly Correlated Predictors:**\n",
        "\n",
        "Identify pairs of independent variables with high correlation coefficients (e.g., > 0.7 or 0.8) and remove one of them.\n",
        "\n",
        "Example: If both age and years of experience are highly correlated, consider keeping one.\n",
        "\n",
        "Combine Correlated Variables:\n",
        "\n",
        "Use Principal Component Analysis (PCA) to create new uncorrelated components from the original variables.\n",
        "\n",
        "Example: If height and weight are highly correlated, PCA can combine them into a single principal component.\n",
        "\n",
        "Standardize or Normalize Variables:\n",
        "\n",
        "Scaling variables (mean=0, variance=1) won't reduce multicollinearity but can help stabilize the coefficients, especially when using ridge regression.\n",
        "\n",
        "Regularization Techniques:\n",
        "\n",
        "Apply regression techniques that penalize large coefficients:\n",
        "\n",
        "Ridge Regression: Shrinks coefficients of correlated variables without eliminating any.\n",
        "\n",
        "Lasso Regression: Shrinks some coefficients to zero, effectively performing variable selection.\n",
        "\n",
        "Increase Sample Size:\n",
        "\n",
        "With more data, the model can better estimate the coefficients of correlated predictors, reducing the impact of multicollinearity.\n",
        "\n",
        "Centering Variables:\n",
        "\n",
        "Subtract the mean of each independent variable from its values. This helps reduce multicollinearity, especially in models with interaction terms.\n",
        "\n",
        "Check for Omitted Variables:\n",
        "\n",
        "Missing relevant variables can create a false impression of multicollinearity by inflating the correlation among the included predictors.\n",
        "\n",
        "Use Domain Knowledge:\n",
        "\n",
        "Instead of relying solely on statistical criteria, evaluate which predictors make the most sense theoretically or practically and remove redundant variables accordingly.\n",
        "\n"
      ],
      "metadata": {
        "id": "D6-WF68PJvXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13. What are some common techniques for transforming categorical variables for use in regression models.**\n",
        "\n",
        "When incorporating categorical variables into regression models, they must be transformed into numerical formats since regression algorithms typically cannot handle non-numeric data. Below are common techniques for transforming categorical variables:\n",
        "\n",
        "**1. One-Hot Encoding**\n",
        "\n",
        "Description: Converts each category of a variable into a new binary column (0 or 1). For a variable with\n",
        "k categories,\n",
        "k−1 columns are typically used to avoid multicollinearity.\n",
        "\n",
        "Use Case: Best for variables with a small number of distinct categories.\n",
        "\n",
        "**2. Label Encoding**\n",
        "\n",
        "Description: Assigns a unique integer to each category.\n",
        "\n",
        "Use Case: Suitable for ordinal variables where the order of categories matters.\n",
        "\n",
        "Caution: Avoid for nominal variables, as it may falsely imply an ordinal relationship.\n",
        "\n",
        "**3. Binary Encoding**\n",
        "\n",
        "Description: Converts categories into binary numbers and encodes them as columns. It is more compact than one-hot encoding.\n",
        "\n",
        "Use Case: Useful for high-cardinality variables (many categories).\n",
        "\n",
        "**4. Frequency Encoding**\n",
        "\n",
        "Description: Replaces each category with its frequency or proportion in the dataset.\n",
        "\n",
        "Use Case: Useful when category frequency carries meaningful information.\n",
        "\n",
        "**5. Target Encoding (Mean Encoding)**\n",
        "\n",
        "Description: Replaces each category with the mean of the target variable for that category.\n",
        "\n",
        "Use Case: Suitable for supervised learning when the target variable is continuous or categorical.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JbsnCdcvKHTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**14. What is the role of interaction terms in Multiple Linear Regression.**\n",
        "\n",
        "In Multiple Linear Regression (MLR), interaction terms are used to model the combined effect of two or more independent variables on the dependent variable. Interaction terms help capture situations where the effect of one predictor variable on the dependent variable depends on the value of another predictor variable.\n",
        "\n",
        "Without interaction terms, the model assumes that each predictor affects the outcome independently, which might not always be the case in real-world data.\n",
        "\n",
        "\n",
        "**Why Include Interaction Terms?**\n",
        "\n",
        "Non-additive Effects: Sometimes, the effect of one predictor on the dependent variable changes depending on the level of another predictor. Interaction terms help capture this non-additive relationship.\n",
        "\n",
        "Improved Model Fit: Adding interaction terms can improve the model's predictive power and explain more variance in the dependent variable, especially when relationships between predictors are complex.\n",
        "\n",
        "Interaction between Categorical and Continuous Variables:\n",
        "\n",
        "Interaction terms can capture relationships between categorical and continuous variables, which could otherwise be overlooked.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EPRbitPnKzWI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression.**\n",
        "\n",
        "**Interpretation of the Intercept in Simple vs. Multiple Linear Regression**\n",
        "The intercept (denoted as\n",
        "c or\n",
        "β\n",
        "0\n",
        "​\n",
        " ) in both Simple Linear Regression and Multiple Linear Regression represents the predicted value of the dependent variable when all independent variables are equal to zero. However, the interpretation can differ between the two types of regression, especially when more than one predictor is involved.\n",
        "\n",
        " Simple Linear Regression\n",
        "In Simple Linear Regression, there is only one independent variable (\n",
        "𝑋\n",
        "X), so the model is typically expressed as:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝜖\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X+ϵ\n",
        "Intercept (\n",
        "β\n",
        "0\n",
        "​\n",
        " ): The intercept represents the predicted value of the dependent variable\n",
        "\n",
        "Y when the independent variable\n",
        "\n",
        "X is zero.\n",
        "\n",
        "Interpretation:\n",
        "If X represents something like \"years of experience,\" the intercept would indicate the predicted\n",
        "Y (e.g., salary) when an individual has zero years of experience (although this may not always be meaningful in a real-world context).\n",
        "\n",
        "**Multiple Linear Regression**\n",
        "\n",
        "In Multiple Linear Regression, there are multiple independent variables (\n",
        "𝑋\n",
        "1\n",
        ",\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑋\n",
        "𝑛\n",
        "X\n",
        "1\n",
        "​\n",
        " ,X\n",
        "2\n",
        "​\n",
        " ,…,X\n",
        "n\n",
        "​\n",
        " ), so the model is typically expressed as:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "+\n",
        "𝜖\n",
        "Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        " +ϵ\n",
        "Intercept (\n",
        "𝛽\n",
        "0\n",
        "​\n",
        " ): The intercept represents the predicted value of the dependent variable\n",
        "𝑌\n",
        "Y when all independent variables (\n",
        "𝑋\n",
        "1\n",
        ",\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑋\n",
        "𝑛\n",
        "X\n",
        "1\n",
        "​\n",
        " ,X\n",
        "2\n",
        "​\n",
        " ,…,X\n",
        "n\n",
        "​\n",
        " ) are equal to zero.\n",
        "\n",
        "Interpretation: In a multiple regression model, the intercept is interpreted as the expected value of\n",
        "Y when each predictor variable is held constant at zero. However, this interpretation may not always be meaningful, especially if zero is not a feasible value for some of the independent variables."
      ],
      "metadata": {
        "id": "H3-2oTLPLIuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**16.What is the significance of the slope in regression analysis, and how does it affect predictions.**\n",
        "\n",
        "\n",
        "**Significance of the Slope in Regression Analysis**\n",
        "\n",
        "The slope in regression analysis (denoted as\n",
        "β\n",
        "1\n",
        "​\n",
        "  in the linear regression equation\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝜖) represents the rate of change in the dependent variable\n",
        "Y for a one-unit change in the independent variable\n",
        "X.\n",
        "\n",
        "In essence, the slope tells you how much the dependent variable is expected to increase or decrease as the independent variable changes. The slope is crucial because it shows the strength and direction of the relationship between the predictor (independent variable) and the outcome (dependent variable).\n",
        "\n",
        "\n",
        "How the Slope Affects Predictions\n",
        "The slope directly influences the model’s predictions. Here’s how it works:\n",
        "\n",
        "Positive Slope (\n",
        "𝛽\n",
        "1>0β 1​>0):\n",
        "\n",
        "When the slope is positive, it indicates that as the independent variable X increases, the dependent variable 𝑌 also increases.\n",
        "\n",
        "Example: If you're predicting a person's salary based on years of experience, a positive slope means that as years of experience increase, the salary is expected to increase as well.\n",
        "Negative Slope (\n",
        "𝛽\n",
        "1\n",
        "<\n",
        "0\n",
        "β\n",
        "1\n",
        "​<0):\n",
        "\n",
        "When the slope is negative, it means that as the independent variable X increases, the dependent variable Y decreases.\n",
        "\n",
        "Example: If you're predicting fuel efficiency based on the weight of a car, a negative slope means that as the weight of the car increases, the fuel efficiency (miles per gallon) is expected to decrease.\n",
        "Magnitude of the Slope:\n",
        "\n",
        "The magnitude of the slope indicates the strength of the relationship between\n",
        "X and Y.\n",
        "A larger absolute value of the slope means that for each unit increase in X, there is a larger change in Y.\n",
        "\n",
        "Example: If the slope is 5, it means that for each one-unit increase in\n",
        "𝑋, 𝑌 increases by 5 units. If the slope is 0.5,\n",
        "𝑌will increase by only 0.5 units for each unit increase in\n",
        "X.\n",
        "\n"
      ],
      "metadata": {
        "id": "qhmoAXnPvAMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**17. How does the intercept in a regression model provide context for the relationship between variables.**\n",
        "\n",
        "The intercept (\n",
        "𝛽\n",
        "0) in a regression model provides a crucial reference point for understanding the relationship between the independent variable(s) and the dependent variable. It helps to anchor the regression equation and offers insights into the behavior of the dependent variable when the independent variable(s) are at certain values—usually zero.\n",
        "\n",
        "Interpretation of the Intercept\n",
        "\n",
        "**1. In Simple Linear Regression:**\n",
        "\n",
        "Model:\n",
        "𝑌\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝜖\n",
        "\n",
        "The intercept\n",
        "𝛽\n",
        "0\n",
        "represents the predicted value of\n",
        "Y when the independent variable\n",
        "X is zero.\n",
        "\n",
        "Context: The intercept essentially gives the baseline or starting point for the dependent variable. It is where the regression line crosses the Y-axis (i.e., when\n",
        "X=0).\n",
        "\n",
        "Example: Suppose you're modeling a person's salary (\n",
        "Y) based on years of experience (\n",
        "X) with the equation\n",
        "Y=30,000+5,000X.\n",
        "\n",
        "Intercept:\n",
        "𝛽\n",
        "0\n",
        "=\n",
        "30\n",
        ",\n",
        "000\n",
        "β\n",
        "0\n",
        "​\n",
        " =30,000. This means that when a person has zero years of experience (i.e., they are just starting), their baseline salary is expected to be $30,000.\n",
        "This baseline provides context for understanding the salary expectations at different experience levels.\n",
        "\n",
        "**Providing Context for the Relationship Between Variables**\n",
        "\n",
        "Baseline for Predictions: The intercept sets the baseline level of the dependent variable. When predictors are at zero, the intercept gives the model's best estimate for the outcome. This is important because it allows for a meaningful context when interpreting how changes in the predictors affect the dependent variable.\n",
        "\n",
        "Context for Comparing Predictor Effects: The intercept works alongside the slopes of the predictors to give a complete picture of the relationship between variables. While the slopes tell you the strength and direction of the relationship between the predictors and the outcome, the intercept defines the starting point of that relationship.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EqpmZfvAv9cQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**18. What are the limitations of using R² as a sole measure of model performance.**\n",
        "\n",
        "\n",
        "**1. Does Not Indicate Model Accuracy**\n",
        "𝑅\n",
        "2\n",
        "tells you how much of the variance in the dependent variable is explained by the model, but it doesn't tell you how accurate the model's predictions are. A high\n",
        "𝑅\n",
        "2\n",
        "  might suggest a good fit, but it doesn't guarantee that the model is making correct predictions, especially for individual data points.\n",
        "\n",
        "Example: A model may have a high\n",
        "𝑅\n",
        "2\n",
        ", but if it systematically overestimates or underestimates the dependent variable, the predictions may still be unreliable.\n",
        "\n",
        "Alternative: Metrics like Mean Squared Error (MSE), Root Mean Squared Error (RMSE), or Mean Absolute Error (MAE) are better at capturing the model's prediction accuracy.\n",
        "\n",
        "**2. Does Not Account for Overfitting**\n",
        "\n",
        "Overfitting happens when a model is too complex, capturing noise or random fluctuations in the training data, leading to high\n",
        "𝑅\n",
        "2\n",
        "values. However, an overfitted model may not generalize well to new, unseen data.\n",
        "\n",
        "Example: A model that includes many predictors may show an extremely high\n",
        "𝑅\n",
        "2\n",
        "on the training data, but it could perform poorly on the test data because it has \"learned\" irrelevant patterns.\n",
        "\n",
        "Alternative: Using Adjusted\n",
        "𝑅\n",
        "2\n",
        "  can help mitigate this issue, as it adjusts\n",
        "𝑅\n",
        "2\n",
        "  based on the number of predictors in the model. Additionally, cross-validation is a better approach to assess model generalizability.\n",
        "\n",
        "**3. Insensitive to Model Misspecification**\n",
        "\n",
        "𝑅\n",
        "2\n",
        "  can be high even if the model is misspecified (i.e., it does not correctly capture the true relationship between variables). For example, the relationship between the predictors and the dependent variable might be non-linear, but a linear regression model might still yield a reasonable\n",
        "𝑅\n",
        "2\n",
        "  value.\n",
        "\n",
        "Example: If you're modeling a non-linear relationship using a linear model,\n",
        "𝑅\n",
        "2\n",
        "  might still be high even though the model does not adequately represent the underlying relationship.\n",
        "\n",
        "Alternative: Visual diagnostics (e.g., residual plots) and tests for model assumptions (e.g., normality of residuals, heteroscedasticity) are better for detecting misspecification.\n",
        "\n",
        "\n",
        "**4. Insensitive to Outliers**\n",
        "\n",
        "𝑅\n",
        "2  can be sensitive to outliers in the data, and a few extreme points can significantly influence its value, making the model appear better or worse than it actually is.\n",
        "\n",
        "\n",
        "**5. No Information About Causality**\n",
        "\n",
        "𝑅\n",
        "2\n",
        "measures the correlation between the predictors and the dependent variable, but it does not imply any causal relationship. A high\n",
        "𝑅\n",
        "2 does not necessarily mean that one variable causes the other.\n",
        "\n",
        "**6. Limited to Linear Relationships**\n",
        "\n",
        "𝑅\n",
        "2\n",
        "  is mainly used for evaluating linear regression models. It does not capture the quality of fit for non-linear models (like decision trees, neural networks, or other complex models) well, because the relationship between variables may not be linear."
      ],
      "metadata": {
        "id": "6wXIK3M5xPOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**19. How would you interpret a large standard error for a regression coefficient.**\n",
        "\n",
        "**Interpretation of a Large Standard Error for a Regression Coefficient**\n",
        "\n",
        "The standard error (SE) of a regression coefficient measures the precision of the estimated coefficient. A large standard error for a regression coefficient indicates that the estimate of the coefficient is relatively imprecise and suggests some issues with the data or the model that need to be addressed.\n",
        "\n",
        "\n",
        "**1. Imprecision of the Estimate**\n",
        "\n",
        "A large standard error implies that there is a lot of variability or uncertainty in the estimated value of the coefficient. This means that the model is not confident in the specific value of that coefficient.\n",
        "\n",
        "**2. Weak Relationship Between the Predictor and the Outcome**\n",
        "\n",
        "A large standard error might suggest that the independent variable has a weak or unstable relationship with the dependent variable. In such cases, the model is struggling to distinguish a clear connection between the predictor and the outcome.\n",
        "\n",
        "**3. Multicollinearity**\n",
        "\n",
        "A large standard error can be a sign of multicollinearity among the predictors in the regression model. Multicollinearity occurs when independent variables are highly correlated with each other, which makes it difficult to estimate the individual effect of each variable.\n",
        "\n",
        "**4. Small Sample Size**\n",
        "\n",
        "A large standard error may result from a small sample size, which typically leads to less reliable estimates of the coefficients. With fewer data points, the model has less information to accurately estimate the relationships between the predictors and the outcome.\n",
        "\n",
        "**5. Model Misspecification**\n",
        "\n",
        "A large standard error can also occur if the model is misspecified, meaning that the functional form of the relationship between the independent and dependent variables is incorrect (e.g., using a linear model when the relationship is non-linear).\n",
        "\n",
        "Example: If you try to model a non-linear relationship with a linear regression, the large standard errors may signal that the model is not fitting the data well.\n",
        "\n",
        "**6. Outliers or Influential Points**\n",
        "\n",
        "Outliers or influential data points can have a disproportionate effect on the regression results, causing variability in the coefficient estimates and leading to large standard errors."
      ],
      "metadata": {
        "id": "JdG0WhsWyKFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**20. How can heteroscedasticity be identified in residual plots, and why is it important to address it.**\n",
        "\n",
        "Heteroscedasticity refers to a situation where the variance of the errors (residuals) is not constant across all levels of the independent variable(s) in a regression model. In simpler terms, it means that the spread or variability of the residuals increases or decreases as the value of the independent variable changes.\n",
        "\n",
        "\n",
        "**1. Residuals vs. Fitted (Predicted) Values Plot**\n",
        "\n",
        "A residual vs. fitted plot is one of the most effective ways to detect heteroscedasticity.\n",
        "\n",
        "In this plot:\n",
        "The x-axis shows the fitted (predicted) values of the dependent variable.\n",
        "\n",
        "The y-axis shows the residuals (the difference between observed and predicted values).\n",
        "\n",
        "**Signs of Heteroscedasticity:**\n",
        "\n",
        "Fan-shaped pattern: If the residuals have a \"cone\" or \"fan\" shape, with the spread of residuals increasing or decreasing as the fitted values change, this is a clear indication of heteroscedasticity.\n",
        "\n",
        "For example, residuals that are more spread out at higher fitted values and tighter at lower fitted values suggest heteroscedasticity.\n",
        "\n",
        "Non-random patterns: If the residuals show some structured pattern (like curves, systematic trends, or clusters), this might suggest misspecification of the model or a different type of non-linearity, not necessarily heteroscedasticity, but it still requires investigation.\n",
        "\n",
        "**2. Scale-Location (Spread-Location) Plot**\n",
        "\n",
        "This plot displays the square root of the absolute value of the residuals on the y-axis and the fitted values on the x-axis.\n",
        "The goal is to check if the variance of residuals increases or decreases as the fitted values change.\n",
        "\n",
        "Signs of Heteroscedasticity:\n",
        "\n",
        "Non-horizontal line: If the spread of the residuals is increasing or decreasing as the fitted values change (indicated by a trend or funnel shape), this is a sign of heteroscedasticity.\n",
        "\n",
        "**3. Histogram or Q-Q Plot of Residuals**\n",
        "\n",
        "While histograms and Q-Q plots are generally used to check for normality of residuals, you can sometimes identify heteroscedasticity indirectly.\n",
        "\n",
        "Signs of Heteroscedasticity:\n",
        "\n",
        "Non-normal residual distribution: If the histogram or Q-Q plot indicates skewness or outliers, it could suggest that there is heteroscedasticity present.\n",
        "\n",
        "Heteroscedasticity violates one of the key assumptions of ordinary least squares (OLS) regression, which assumes that the residuals have constant variance (i.e., homoscedasticity). Addressing heteroscedasticity is crucial for the following reasons:\n",
        "\n",
        "**1. Impact on Efficiency of Estimates**\n",
        "\n",
        "When heteroscedasticity is present, the OLS estimates of the regression coefficients remain unbiased, but they are no longer efficient. This means that the estimates might have larger standard errors than they should, leading to less precise coefficient estimates.\n",
        "\n",
        "Consequence: Larger standard errors reduce the reliability of hypothesis tests and confidence intervals, making it harder to determine whether predictor variables are statistically significant.\n",
        "\n",
        "**2. Impact on Statistical Inference**\n",
        "\n",
        "The presence of heteroscedasticity leads to incorrect significance tests. Specifically:\n",
        "\n",
        "t-tests and F-tests may be invalid because they assume constant variance in the residuals.\n",
        "\n",
        "This could result in incorrect conclusions about the importance of predictor variables or the overall model fit.\n",
        "\n",
        "Consequence: You may make misleading decisions, such as incorrectly rejecting or failing to reject hypotheses about relationships between variables.\n",
        "\n",
        "**3. Distorted Confidence Intervals**\n",
        "\n",
        "Heteroscedasticity can widen or narrow confidence intervals for regression coefficients, making them less reliable. This compromises the ability to make accurate predictions or estimate the uncertainty around the model’s predictions.\n",
        "\n",
        "Consequence: This reduces the precision of any predictions or estimates based on the regression model, impacting decision-making and policy recommendations.\n",
        "\n",
        "**4. Unreliable Predictions**\n",
        "\n",
        "The lack of constant variance in the residuals means that the model may be giving overly optimistic or overly pessimistic predictions depending on the value of the independent variable.\n",
        "\n",
        "Consequence: The model may not generalize well to new data, especially in regions where the residual variance is high."
      ],
      "metadata": {
        "id": "B9SzhqhRywKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R².**\n",
        "\n",
        "In multiple linear regression, both\n",
        "𝑅\n",
        "2\n",
        "  and Adjusted\n",
        "𝑅\n",
        "2\n",
        "  are metrics used to evaluate the fit of the model, but they reflect different aspects of model performance. Here's what it means if a model has a high\n",
        "𝑅\n",
        "2 but a low Adjusted\n",
        "𝑅\n",
        "2:\n",
        "\n",
        "1. High\n",
        "𝑅\n",
        "2\n",
        "Value.\n",
        "\n",
        "\n",
        "𝑅\n",
        "2, the coefficient of determination, measures the proportion of the variance in the dependent variable that is explained by the independent variables in the model.\n",
        "\n",
        "A high\n",
        "𝑅\n",
        "2\n",
        "suggests that the model is explaining a large proportion of the variance in the dependent variable, meaning the predictors in the model are relatively good at predicting the outcome.\n",
        "\n",
        "2. Low Adjusted\n",
        "𝑅\n",
        "2\n",
        "Value.\n",
        "\n",
        "Adjusted\n",
        "𝑅\n",
        "2\n",
        "is a modification of\n",
        "𝑅\n",
        "2\n",
        "that adjusts for the number of predictors in the model.\n",
        "It penalizes the model for adding unnecessary predictors that do not improve the model’s ability to explain the variance.\n",
        "Adjusted\n",
        "𝑅\n",
        "2\n",
        "is especially important in multiple regression, where the number of independent variables can significantly affect\n",
        "𝑅\n",
        "2\n",
        "\n",
        "\n",
        " When you add more predictors to the model,\n",
        "𝑅\n",
        "2\n",
        "will always increase (or stay the same), even if the added predictors do not have any real relationship with the outcome variable. Adjusted\n",
        "𝑅\n",
        "2\n",
        ", on the other hand, decreases if the new predictors do not improve the model substantially.\n",
        "\n",
        "\n",
        "What Does It Mean?\n",
        "\n",
        "High\n",
        "𝑅\n",
        "2 but Low Adjusted\n",
        "𝑅\n",
        "2\n",
        "  suggests that the model might be overfitting the data.\n",
        "  \n",
        "Overfitting occurs when a model becomes too complex by including too many predictors, even ones that don’t significantly contribute to explaining the variance in the dependent variable.\n",
        "\n",
        "This results in a high\n",
        "𝑅\n",
        "2\n",
        " , because adding more predictors will always increase\n",
        "𝑅\n",
        "2\n",
        "\n",
        " , but the low Adjusted\n",
        "𝑅\n",
        "2\n",
        "  indicates that the added predictors are not helping to improve the model’s performance in a meaningful way. It’s a sign that the model is likely overfitting and might not generalize well to new, unseen data."
      ],
      "metadata": {
        "id": "_kmWWen1zR_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**22. Why is it important to scale variables in Multiple Linear Regression.**\n",
        "\n",
        "\n",
        "\n",
        "**Why Scaling Variables Is Important in Multiple Linear Regression**\n",
        "\n",
        "In Multiple Linear Regression, scaling variables can significantly impact the performance, interpretability, and stability of the model. Here's why it's important:\n",
        "\n",
        "**1. Prevents Variables with Larger Magnitudes from Dominating the Model**\n",
        "\n",
        "Scaling ensures that all variables are on the same scale, which prevents predictors with larger magnitudes (e.g., income in thousands vs. age in years) from disproportionately influencing the regression model.\n",
        "\n",
        "**2. Improves Numerical Stability of the Model**\n",
        "\n",
        "Scaling can help with the numerical stability of the model, especially when there are predictors with very different units or orders of magnitude. Without scaling, the regression algorithm may struggle with very large or small values, leading to computational issues or poorly estimated coefficients.\n",
        "\n",
        "**3. Facilitates the Interpretation of Coefficients**\n",
        "\n",
        "When variables are scaled (typically by standardizing them to have a mean of 0 and standard deviation of 1), the coefficients of the model can be interpreted in a more consistent and comparable way.\n",
        "\n",
        "\n",
        "**4. Necessary for Regularization Techniques (Lasso, Ridge Regression)**\n",
        "\n",
        "Regularization methods like Ridge regression and Lasso regression penalize the magnitude of coefficients to prevent overfitting.\n",
        "\n",
        "These methods are sensitive to the scale of the variables because the penalty term depends on the size of the coefficients.\n",
        "\n",
        "Without scaling, variables with larger ranges or higher magnitudes could receive more penalty than those with smaller ranges, leading to biased shrinkage of coefficients.\n",
        "\n",
        "**5. Helps with Convergence of Optimization Algorithms**\n",
        "\n",
        "If you're using iterative methods like gradient descent to fit the model, unscaled data can cause slow convergence or even failure to converge. This is because the optimization process could take longer to reach the optimal coefficients if the predictors vary in scale."
      ],
      "metadata": {
        "id": "KDcQYmPK2dxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**23. What is polynomial regression.**\n",
        "\n",
        "Polynomial Regression is a type of regression analysis used when the relationship between the independent variable(s) and the dependent variable is not linear but can be modeled as a polynomial. In simple terms, polynomial regression allows the model to fit curves instead of just straight lines.\n",
        "\n",
        "In Polynomial Regression, the independent variable(s) (or predictor variables) are raised to higher powers, creating new features (or predictors) that capture the non-linear relationships in the data.\n",
        "\n",
        "\n",
        "**Why Use Polynomial Regression?**\n",
        "\n",
        "Polynomial regression is useful when:\n",
        "\n",
        "The relationship between variables is non-linear: In many real-world scenarios, the relationship between the dependent and independent variables isn't a straight line, but a curve. Polynomial regression can capture these curves.\n",
        "\n",
        "It fits curves or bends in data: When data shows patterns like acceleration or deceleration, polynomial regression helps model such patterns.\n",
        "\n",
        "Types of Polynomial Regression\n",
        "\n",
        "Quadratic Regression (degree 2): This involves adding a squared term (e.g., X\n",
        "2\n",
        " ) to the model.\n",
        "Example: A U-shaped curve.\n",
        "Cubic Regression (degree 3): Adds a cubic term (e.g.,\n",
        "X\n",
        "3\n",
        " ) to the model, creating more flexibility in fitting complex curves.\n",
        "\n",
        "Example: A cubic curve that can have multiple bends.\n",
        "Higher-Degree Polynomial Regression (degree\n",
        "n): Models with even higher powers of\n",
        "X can fit more complex data patterns, but the model may become prone to overfitting if the degree is too high."
      ],
      "metadata": {
        "id": "NZpY0Rti34fZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**24. How does polynomial regression differ from linear regression.**\n",
        "\n",
        "**1. Type of Relationship**\n",
        "Linear Regression assumes a linear relationship between the dependent and independent variables, meaning the data can be modeled by a straight line.\n",
        "\n",
        "Polynomial Regression models a non-linear relationship by introducing polynomial terms (e.g.,\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "𝑋\n",
        "3\n",
        "X\n",
        "2\n",
        " ,X\n",
        "3\n",
        " ), allowing it to fit curves to the data.\n",
        "\n",
        "2. Model Equation\n",
        "Linear Regression is represented as:\n",
        "\n",
        "𝑦\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝜖\n",
        "y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X+ϵ\n",
        "Where\n",
        "𝑦\n",
        "y is the dependent variable,\n",
        "𝑋\n",
        "X is the independent variable,\n",
        "𝛽\n",
        "0\n",
        "β\n",
        "0\n",
        "​\n",
        "  is the intercept, and\n",
        "𝛽\n",
        "1\n",
        "β\n",
        "1\n",
        "​\n",
        "  is the slope.\n",
        "\n",
        "Polynomial Regression is represented as:\n",
        "\n",
        "𝑦\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝛽\n",
        "3\n",
        "𝑋\n",
        "3\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "+\n",
        "𝜖\n",
        "y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X+β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +β\n",
        "3\n",
        "​\n",
        " X\n",
        "3\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        " +ϵ\n",
        "Where\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "𝑋\n",
        "3\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑋\n",
        "𝑛\n",
        "X\n",
        "2\n",
        " ,X\n",
        "3\n",
        " ,…,X\n",
        "n\n",
        "  are the polynomial terms that introduce curvature to the model.\n",
        "\n",
        "3. Complexity\n",
        "\n",
        "Linear Regression is simpler, requiring fewer parameters (just the slope and intercept for a single predictor).\n",
        "\n",
        "Polynomial Regression is more complex, as it adds additional parameters for each polynomial degree (e.g.,\n",
        "𝛽\n",
        "2\n",
        "for\n",
        "𝑋\n",
        "2\n",
        "\n",
        " ,\n",
        "𝛽\n",
        "3\n",
        "\n",
        "​\n",
        "  for\n",
        "𝑋\n",
        "3\n",
        "\n",
        " , etc.).\n",
        "\n",
        "\n",
        "4. Model Flexibility\n",
        "\n",
        "Linear Regression is best for data with a constant rate of change (a straight line).\n",
        "\n",
        "Polynomial Regression allows for more flexible curves, capturing non-linear patterns like acceleration, deceleration, or turning points in the data.\n",
        "\n",
        "5. Overfitting\n",
        "\n",
        "Linear Regression is less prone to overfitting, especially when the data truly follows a linear trend.\n",
        "\n",
        "Polynomial Regression is more prone to overfitting, especially when higher-degree polynomials are used, as it can fit noise in the data rather than the underlying trend.\n",
        "\n",
        "6. Interpretation\n",
        "\n",
        "Linear Regression is easier to interpret because the coefficient\n",
        "𝛽\n",
        "1\n",
        "directly represents the rate of change of the dependent variable for a one-unit change in the independent variable.\n",
        "\n",
        "Polynomial Regression becomes harder to interpret with higher-degree terms, as each additional term (e.g.,\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "𝑋\n",
        "3\n",
        "X\n",
        "2\n",
        " ,X\n",
        "3\n",
        " ) introduces additional complexities in understanding how each term influences the dependent variable.\n"
      ],
      "metadata": {
        "id": "ykVSSTvD4Wwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**25. When is polynomial regression used.**\n",
        "\n",
        "Polynomial regression is used when the relationship between the independent variable(s) and the dependent variable is non-linear and cannot be adequately captured by a straight line. It is appropriate in situations where the data exhibits patterns such as curves, bends, acceleration, or deceleration. Here are some specific scenarios where polynomial regression is useful:\n",
        "\n",
        "1. Non-linear Relationships Between Variables\n",
        "\n",
        "When the data shows a curved relationship, polynomial regression can capture these patterns. For example, if the relationship between the dependent and independent variables follows a parabolic (U-shaped or inverted U-shaped) trend, polynomial regression can model this relationship.\n",
        "\n",
        "2. Data with Multiple Turning Points\n",
        "\n",
        "If the data has multiple inflection points (i.e., points where the slope of the curve changes), polynomial regression can accommodate these changes by adding higher-degree terms.\n",
        "\n",
        "3. Modeling Accelerated or Decelerated Growth\n",
        "\n",
        "Polynomial regression is ideal for situations where the dependent variable grows or decays at an increasing or decreasing rate. Linear regression would not be able to capture this dynamic, but polynomial regression can fit curves that reflect acceleration or deceleration.\n",
        "\n",
        "4. When the Data Exhibits a U-shaped or Inverted U-shaped Trend\n",
        "\n",
        "If the relationship between variables resembles a U-shaped (concave up) or inverted U-shaped (concave down) curve, polynomial regression can model these kinds of relationships.\n",
        "\n",
        "5. Situations Where Higher-Degree Terms are Needed\n",
        "\n",
        "Polynomial regression is used when adding higher-degree terms (like\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "𝑋\n",
        "3\n",
        ",\n",
        "…\n",
        "X\n",
        "2\n",
        " ,X\n",
        "3\n",
        " ,…) is necessary to model the complexity of the data. It allows the model to be flexible enough to fit various curves.\n",
        "\n"
      ],
      "metadata": {
        "id": "EvfZu5c15Lv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**26. What is the general equation for polynomial regression.**\n",
        "\n",
        "General Equation for Polynomial Regression\n",
        "The general equation for Polynomial Regression with a single independent variable\n",
        "X and a dependent variable\n",
        "y is:\n",
        "\n",
        "𝑦\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "𝛽\n",
        "3\n",
        "𝑋\n",
        "3\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "+\n",
        "𝜖\n",
        "\n",
        "Where:\n",
        "\n",
        "y is the dependent variable (the target you're trying to predict).\n",
        "\n",
        "\n",
        "X is the independent variable (the predictor or feature).\n",
        "\n",
        "\n",
        "𝛽\n",
        "0\n",
        "is the intercept of the model (the value of\n",
        "y when\n",
        "𝑋\n",
        "=\n",
        "0\n",
        "X=0).\n",
        "\n",
        "𝛽\n",
        "1\n",
        ",\n",
        "𝛽\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝛽\n",
        "𝑛\n",
        "β\n",
        "1\n",
        "​\n",
        " ,β\n",
        "2\n",
        "​\n",
        " ,…,β\n",
        "n\n",
        "​\n",
        "  are the coefficients (also called parameters) for each term in the polynomial.\n",
        "  \n",
        "  These coefficients are estimated during the model fitting process.\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "𝑋\n",
        "3\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑋\n",
        "𝑛\n",
        "X\n",
        "2\n",
        " ,X\n",
        "3\n",
        " ,…,X\n",
        "n\n",
        "  are the higher-degree polynomial terms that introduce curvature into the model.\n",
        "\n",
        "n is the degree of the polynomial, which determines how many terms (or powers of\n",
        "X) are included in the model.\n",
        "\n",
        "ϵ is the error term (residuals), which accounts for the difference between the predicted and actual values.\n",
        "\n"
      ],
      "metadata": {
        "id": "o0xWTjlH5moT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**27. Can polynomial regression be applied to multiple variables.**\n",
        "\n",
        "Yes, Polynomial Regression can be applied to multiple variables (features), and it is often referred to as multivariate polynomial regression in such cases. This involves extending the idea of polynomial terms to multiple independent variables.\n",
        "\n",
        "\n",
        "Key Features of Polynomial Regression with Multiple Variables\n",
        "Polynomial Terms for Each Variable:\n",
        "\n",
        "Individual terms like\n",
        "\n",
        "X\n",
        "1\n",
        "2\n",
        "​\n",
        " ,\n",
        "\n",
        "X\n",
        "2\n",
        "3\n",
        "​\n",
        " , etc., capture non-linear effects for each variable.\n",
        "Interaction Terms:\n",
        "\n",
        "Terms like\n",
        "𝑋\n",
        "1\n",
        "𝑋\n",
        "2\n",
        "X\n",
        "1\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        "  capture how changes in one variable interact with changes in another.\n",
        "\n",
        "Degree of the Polynomial:\n",
        "\n",
        "The degree specifies the highest power of the variables and interactions (e.g., degree 2 includes squared terms; degree 3 includes cubic terms).\n",
        "\n",
        "\n",
        "Overfitting: As the number of variables and the polynomial degree increase, the model may overfit the training data.\n",
        "\n",
        "Interpretability: With many polynomial and interaction terms, interpreting the model becomes more complex.\n",
        "\n",
        "Computational Cost: Generating a large number of terms for high-degree polynomials and many variables can increase computational demands.\n",
        "\n",
        "\n",
        "Polynomial regression can effectively model non-linear relationships in datasets with multiple variables by including polynomial terms and interactions. However, care must be taken to balance model flexibility with interpretability and to avoid overfitting by using regularization or feature selection techniques.\n"
      ],
      "metadata": {
        "id": "cgclWYeN5QP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**28. What are the limitations of polynomial regression.**\n",
        "\n",
        "Polynomial regression is a powerful technique for modeling non-linear relationships, but it has several limitations that can affect its practicality and effectiveness:\n",
        "\n",
        "**1. Overfitting**\n",
        "\n",
        "Description: Higher-degree polynomials can overfit the data, capturing noise instead of the underlying trend.\n",
        "\n",
        "Impact: While the model may perform well on the training data, it can generalize poorly to unseen data, leading to low predictive accuracy.\n",
        "\n",
        "**2. Extrapolation Issues**\n",
        "\n",
        "Description: Polynomial regression is unreliable for predictions outside the range of the training data.\n",
        "\n",
        "Impact: Polynomial functions tend to diverge rapidly (especially for high-degree polynomials), producing unrealistic predictions for extreme values of the independent variable.\n",
        "\n",
        "**3. Lack of Interpretability**\n",
        "\n",
        "Description: As the degree of the polynomial increases, the coefficients become harder to interpret, and the relationship between variables becomes less clear.\n",
        "\n",
        "Impact: This makes it difficult to explain the model's behavior or understand the contribution of each term to the dependent variable.\n",
        "\n",
        "**4. Computational Complexity**\n",
        "\n",
        "Description: For large datasets with many features, generating high-degree polynomial terms can lead to a significant increase in computational cost and memory usage.\n",
        "\n",
        "Impact: This can make polynomial regression inefficient or impractical for large-scale problems.\n",
        "\n",
        "**5. Multicollinearity**\n",
        "Description: Polynomial terms (e.g.,\n",
        "𝑋\n",
        ",\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "𝑋\n",
        "3\n",
        ") are often highly correlated with each other, leading to multicollinearity.\n",
        "\n",
        "Impact: Multicollinearity can make coefficient estimates unstable and reduce the reliability of the model.\n",
        "\n",
        "**6. Curse of Dimensionality**\n",
        "\n",
        "Description: For multivariate polynomial regression, the number of terms grows rapidly with the number of variables and the degree of the polynomial.\n",
        "\n",
        "Impact: This can result in overfitting, increased computational requirements, and difficulties in managing the model.\n",
        "\n",
        "**7. Sensitive to Outliers**\n",
        "\n",
        "Description: Polynomial regression is highly sensitive to outliers, as it tries to minimize the sum of squared errors. Outliers can disproportionately affect the fit of the model.\n",
        "\n",
        "Impact: This can lead to misleading results and poor generalization.\n",
        "\n",
        "\n",
        "**8. Prone to Oscillations**\n",
        "\n",
        "Description: High-degree polynomials can create oscillations (e.g., in Runge's phenomenon), where the curve fluctuates wildly between data points.\n",
        "\n",
        "Impact: This reduces the smoothness and reliability of the fitted model, particularly at the edges of the data.\n",
        "\n",
        "**9. Risk of Overparameterization**\n",
        "\n",
        "Description: Adding too many polynomial terms can create an unnecessarily complex model.\n",
        "\n",
        "Impact: Overparameterization increases the risk of overfitting and makes the model difficult to validate and interpret.\n",
        "\n",
        "**10. Requires Feature Scaling**\n",
        "\n",
        "Description: Polynomial regression often requires proper scaling of variables to prevent numerical instability due to large differences in the magnitudes of polynomial terms.\n",
        "\n",
        "Impact: Without scaling, the optimization process might fail or produce incorrect results.\n",
        "\n",
        "\n",
        "While polynomial regression is useful for modeling non-linear relationships, it comes with significant limitations such as overfitting, sensitivity to outliers, and poor interpretability. It should be applied carefully, especially when high-degree terms are involved, and evaluated against alternative methods for non-linear modeling."
      ],
      "metadata": {
        "id": "EzeLMdGJ5_6q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**29. What methods can be used to evaluate model fit when selecting the degree of a polynomial.**\n",
        "\n",
        "Choosing the degree of a polynomial in regression is critical to balancing underfitting and overfitting. Several evaluation methods can guide this decision by assessing how well the model fits the data. Below are some common approaches:\n",
        "\n",
        "**1. Visual Inspection**\n",
        "\n",
        "Description: Plot the data points and the fitted curve for different polynomial degrees to visually assess how well the model captures the underlying trend.\n",
        "\n",
        "What to Look For:\n",
        "\n",
        "If the curve is too simple (linear or underfitting), it may not capture key patterns.\n",
        "\n",
        "If the curve is too complex (overfitting), it may oscillate or hug the data points too tightly.\n",
        "\n",
        "Limitation: Subjective and less effective for high-dimensional data.\n",
        "\n",
        "**2. Residual Analysis**\n",
        "\n",
        "Description: Analyze the residuals (differences between observed and predicted values) to check for systematic patterns.\n",
        "\n",
        "What to Look For:\n",
        "\n",
        "Residuals should be randomly distributed with no clear pattern.\n",
        "\n",
        "Large, systematic patterns indicate the model is underfitting.\n",
        "\n",
        "Very small residuals that fluctuate excessively can indicate overfitting.\n",
        "\n",
        "Limitation: Requires careful interpretation and is harder to apply in multivariate settings.\n",
        "\n",
        "\n",
        "**3.\n",
        "𝑅\n",
        "2 (Coefficient of Determination)**\n",
        "\n",
        "Description: Measures the proportion of variance in the dependent variable explained by the model.\n",
        "\n",
        "How to Use:\n",
        "\n",
        "Higher\n",
        "𝑅\n",
        "2\n",
        "values indicate better fit, but be cautious of overfitting with very high degrees.\n",
        "\n",
        "Limitation:\n",
        "𝑅\n",
        "2 always increases with more parameters, even if the additional complexity doesn't improve generalizability.\n",
        "\n",
        "**Mean Squared Error (MSE) or Root Mean Squared Error (RMSE)**\n",
        "\n",
        "Description: Directly calculates the average squared (or square root of squared) differences between observed and predicted values.\n",
        "\n",
        "How to Use:\n",
        "\n",
        "Calculate MSE or RMSE for both training and validation sets.\n",
        "\n",
        "Look for the degree that balances low error in both sets.\n",
        "\n",
        "Strength: Simple and effective; aligns with the objective of minimizing prediction error.\n",
        "\n",
        "\n",
        "Start with a low-degree polynomial and gradually increase until the model balances fit and generalizability.\n",
        "\n",
        "Use multiple evaluation methods to ensure robustness in the selection process.\n",
        "\n",
        "Consider domain knowledge to avoid unnecessarily high degrees if the real-world relationship is likely simpler."
      ],
      "metadata": {
        "id": "ZKOIgVL17TSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**29. What methods can be used to evaluate model fit when selecting the degree of a polynomial.**\n",
        "\n",
        "Selecting the appropriate degree of a polynomial is crucial to achieving a balance between underfitting and overfitting. The following methods can be used to evaluate model fit and guide the selection of the polynomial degree:\n",
        "\n",
        "**1. Residual Analysis**\n",
        "\n",
        "Description: Analyze the residuals (differences between observed and predicted values) for systematic patterns.\n",
        "\n",
        "What to Look For:\n",
        "\n",
        "Residuals should be randomly distributed with no clear structure.\n",
        "\n",
        "Systematic patterns indicate underfitting or incorrect model specification.\n",
        "\n",
        "Strength: Provides insights into whether the model is capturing the relationship correctly.\n",
        "\n",
        "**2. Cross-Validation**\n",
        "\n",
        "Description: Use techniques like k-fold cross-validation to assess model performance on unseen data.\n",
        "\n",
        "How to Use:\n",
        "\n",
        "Train the model on training folds and validate on test folds for each polynomial degree.\n",
        "\n",
        "Calculate the average validation error (e.g., MSE or RMSE) for each degree.\n",
        "\n",
        "Select the degree with the lowest validation error.\n",
        "\n",
        "Strength: Ensures the model generalizes well to unseen data.\n",
        "\n",
        "\n",
        "**3. Mean Squared Error (MSE) or Root Mean Squared Error (RMSE)**\n",
        "\n",
        "Description: Measure the average squared error (or its square root) between observed and predicted values.\n",
        "\n",
        "How to Use:\n",
        "\n",
        "Compute MSE or RMSE for training and validation sets.\n",
        "\n",
        "Select the degree where the validation error is minimized without overfitting (training error much lower than validation error).\n",
        "\n",
        "Strength: Simple and directly evaluates prediction accuracy.\n",
        "\n",
        "\n",
        "Use multiple methods to evaluate model fit, as relying on a single metric can be misleading.\n",
        "\n",
        "The degree should balance model complexity and generalization performance.\n",
        "\n",
        "Test the final model on unseen data to ensure robustness."
      ],
      "metadata": {
        "id": "xs-b74OB73g0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**30. Why is visualization important in polynomial regression.**\n",
        "\n",
        "Visualization is a critical tool in polynomial regression as it provides intuitive insights into the relationship between variables and the performance of the model. Below are the key reasons why visualization is important:\n",
        "\n",
        "**Understanding the Data and Trends**\n",
        "\n",
        "Purpose: Helps in identifying the underlying relationship between the dependent and independent variables.\n",
        "\n",
        "How It Helps:\n",
        "\n",
        "Reveals whether the relationship is linear, non-linear, or complex.\n",
        "\n",
        "Guides the decision to use polynomial regression and the degree of the polynomial.\n",
        "\n",
        "**Detecting Underfitting and Overfitting**\n",
        "\n",
        "Purpose: Visualizing the fitted curve alongside the data points can indicate if the model is too simple (underfitting) or too complex (overfitting).\n",
        "\n",
        "How It Helps:\n",
        "\n",
        "Underfitting: The curve fails to capture the trend in the data (e.g., a straight line when the data follows a curve).\n",
        "\n",
        "Overfitting: The curve oscillates excessively, closely hugging all the data points, including noise.\n",
        "\n",
        "\n",
        "**Assessing Model Fit**\n",
        "\n",
        "Purpose: Enables quick evaluation of how well the polynomial function fits the observed data.\n",
        "\n",
        "How It Helps:\n",
        "\n",
        "A well-fitted curve smoothly follows the general pattern of the data without unnecessary complexity.\n",
        "\n",
        "Shows any regions where the model fails to capture trends.\n",
        "\n",
        "**Identifying Outliers**\n",
        "\n",
        "Purpose: Visual plots can highlight outliers that deviate significantly from the fitted curve.\n",
        "\n",
        "How It Helps:\n",
        "\n",
        "Outliers can disproportionately affect polynomial regression, especially for high-degree polynomials.\n",
        "\n",
        "Helps in deciding whether to remove or address outliers to improve model performance.\n",
        "\n",
        "**Choosing the Degree of the Polynomial**\n",
        "\n",
        "Purpose: Visual comparison of curves fitted with different polynomial degrees helps select the most appropriate degree.\n",
        "\n",
        "How It Helps:\n",
        "\n",
        "A lower-degree polynomial may oversimplify the data.\n",
        "\n",
        "A higher-degree polynomial may introduce unnecessary complexity.\n",
        "\n",
        "**Communicating Results**\n",
        "\n",
        "Purpose: Visualization simplifies explaining the model's behavior and findings to stakeholders.\n",
        "\n",
        "How It Helps:\n",
        "\n",
        "Non-technical audiences can better understand the relationship between variables and the impact of the model.\n",
        "\n",
        "**Evaluating Residuals**\n",
        "\n",
        "Purpose: Residual plots (differences between observed and predicted values) help assess the quality of the model.\n",
        "\n",
        "How It Helps:\n",
        "\n",
        "Randomly distributed residuals indicate a good fit.\n",
        "\n",
        "Patterns in residuals suggest issues with the model, such as incorrect polynomial degree.\n",
        "\n",
        "**Highlighting Model Limitations**\n",
        "\n",
        "Purpose: Visual plots reveal areas where the model might fail, such as at the edges of the data range.\n",
        "\n",
        "How It Helps:\n",
        "\n",
        "Shows divergence or unrealistic predictions when extrapolating beyond the data range.\n"
      ],
      "metadata": {
        "id": "4yzY9fCY9HrA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**31. How is polynomial regression implemented in Python?**\n",
        "\n",
        "Polynomial regression can be implemented in Python using libraries like NumPy, scikit-learn, and matplotlib for data preprocessing, modeling, and visualization. Below is a step-by-step guide:"
      ],
      "metadata": {
        "id": "L2e7_TDE9wzt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KtWGZQeUx3y-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample dataset\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Independent variable\n",
        "y = np.array([2.1, 4.3, 6.5, 8.0, 10.8])     # Dependent variable\n"
      ],
      "metadata": {
        "id": "g_-qQoJB-Dgi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "degree = 2  # Specify the degree of the polynomial\n",
        "poly = PolynomialFeatures(degree=degree)\n",
        "X_poly = poly.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "52fPRizY-F2A"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "LEcvCW4X-IIp",
        "outputId": "579a9fb2-ba2f-4415-948c-d232b36420cb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LinearRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LinearRegression.html\">?<span>Documentation for LinearRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LinearRegression()</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_poly)\n"
      ],
      "metadata": {
        "id": "R2fZvKBq-J3a"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse = mean_squared_error(y, y_pred)\n",
        "r2 = model.score(X_poly, y)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R^2 Score: {r2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkz_0eOc-LaV",
        "outputId": "ff06aaa1-8676-4370-f962-4e99add34970"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.05462857142857149\n",
            "R^2 Score: 0.9939046938957677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a smooth curve for visualization\n",
        "X_smooth = np.linspace(min(X), max(X), 100).reshape(-1, 1)\n",
        "X_smooth_poly = poly.transform(X_smooth)\n",
        "y_smooth_pred = model.predict(X_smooth_poly)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, y, color='blue', label='Data Points')\n",
        "plt.plot(X_smooth, y_smooth_pred, color='red', label='Polynomial Fit')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Polynomial Regression')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "YEAhN0cq-L7A",
        "outputId": "be4ecdc2-b5dc-477e-953c-6875a05250c1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "54yp7uOg-RBq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}