{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2xWiovllcHqaKDPRoN7S6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niikkkhiil/niikkkhiil/blob/main/Logistic_Regresssion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1 What is Logistic Regression, and how does it differ from Linear Regression.**\n",
        "\n",
        "Logistic Regression and Linear Regression are both statistical models used for prediction, but they serve different purposes and are used in different contexts. Here's a breakdown of both:\n",
        "\n",
        "Linear Regression: Purpose: Used for predicting a continuous dependent variable. Output: Produces a continuous output. For example, predicting house prices, temperature, or sales. Model: The model predicts a value by fitting a linear relationship between input variables and the output. It assumes that the relationship between the independent variable (input) and the dependent variable (output) is linear. Formula: 𝑌\n",
        "\n",
        "𝛽 0\n",
        "\n",
        "𝛽 1 𝑋\n",
        "\n",
        "𝜖 Y=β 0​+β 1​X+ϵ Where: 𝑌 Y is the predicted value\n",
        "\n",
        "(dependent variable). 𝑋 X is the input feature(s) (independent variable). 𝛽 0 β 0​is the intercept. 𝛽 1 β 1​is the coefficient or slope of the line. 𝜖 ϵ is the error term. Objective: Minimize the sum of squared differences between the predicted values and the actual values (minimizing error).\n",
        "Logistic Regression: Purpose: Used for classification tasks, where the output is categorical (e.g., \"yes\" or \"no\", \"spam\" or \"not spam\"). Output: Produces probabilities that are mapped to discrete categories (binary or multi-class). The output is typically between 0 and 1, representing the probability of a particular class. Model: It uses a logistic (sigmoid) function to model the probability that a given input belongs to a particular class. This is why it's often used for binary classification tasks (e.g., predicting whether an email is spam or not).\n",
        "\n",
        " Formula: 𝑃 ( 𝑌\n",
        "1 ∣ 𝑋 )\n",
        "1 1\n",
        "𝑒 − ( 𝛽 0\n",
        "𝛽 1 𝑋 ) P(Y=1∣X)= 1+e −(β 0​+β 1​X)\n",
        "1​\n",
        "\n",
        "Where: 𝑃 ( 𝑌\n",
        "1 ∣ 𝑋 ) P(Y=1∣X) is the probability that the dependent variable 𝑌 Y equals 1 given the input feature 𝑋 X. 𝑒 e is the base of the natural logarithm (around 2.718). 𝛽 0 β 0​and 𝛽 1 β 1​are the model parameters. Objective: Minimize the difference between the predicted probabilities and the actual class labels by using methods like maximum likelihood estimation. Key Differences: Aspect Linear Regression Logistic Regression Purpose Predict a continuous outcome (regression). Predict a categorical outcome (classification). Output Continuous values (e.g., price, temperature). Probabilities (between 0 and 1), often converted to binary outcomes. Function Linear relationship ( 𝑦\n",
        "𝑚 𝑥 + 𝑐 y=mx+c). Sigmoid (logistic) function, outputs probabilities.\n",
        "\n",
        "Type of Problem Used for regression tasks. Used for classification tasks. Assumptions Assumes a linear relationship between inputs and output.\n",
        "\n",
        " Assumes a nonlinear relationship with log-odds. When to Use: Linear Regression is best suited when the output is a continuous variable (e.g., predicting income, stock price, etc.). Logistic Regression is used when the output is a binary or categorical variable (e.g., whether a customer buys a product or not, spam detection). Despite their names, Logistic Regression is primarily used for classification, not regression."
      ],
      "metadata": {
        "id": "TVJ0vxQpWaxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2 What is the mathematical equation of Logistic Regression.**\n",
        "\n",
        "The mathematical equation of Logistic Regression involves the logistic (sigmoid) function to model the probability of a binary outcome (i.e., the probability that the output variable 𝑌 Y is 1, given the input features 𝑋 X).\n",
        "\n",
        "The sigmoid function is given by:\n",
        "\n",
        "𝑃 ( 𝑌\n",
        "1 ∣ 𝑋 )\n",
        "1 1 + 𝑒 − ( 𝛽 0 + 𝛽 1 𝑋 1 + 𝛽 2 𝑋 2 + ⋯ + 𝛽 𝑛 𝑋 𝑛 ) P(Y=1∣X)= 1+e −(β 0​+β 1​X 1​+β 2​X 2​+⋯+β n​X n​)\n",
        "\n",
        "1​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑃 ( 𝑌\n",
        "1 ∣ 𝑋 ) P(Y=1∣X) is the probability that the dependent variable 𝑌 Y equals 1, given the input features 𝑋 1 , 𝑋 2 , … , 𝑋 𝑛 X 1​,X 2​,…,X n​. 𝑋 1 , 𝑋 2 , … , 𝑋 𝑛 X 1​,X 2​,…,X n​are the input features (independent variables). 𝛽 0 β 0​is the intercept or bias term. 𝛽 1 , 𝛽 2 , … , 𝛽 𝑛 β 1​,β 2​,…,β n​are the coefficients (weights) for each input feature. 𝑒 e is the base of the natural logarithm (approximately 2.718). This function maps the linear combination of inputs ( 𝛽 0 + 𝛽 1 𝑋 1 + 𝛽 2 𝑋 2 + ⋯ + 𝛽 𝑛 𝑋 𝑛 ) (β 0​+β 1​X 1​+β 2​X 2​+⋯+β n​X n​) to a value between 0 and 1, representing the probability of the positive class ( 𝑌\n",
        "1 Y=1).\n",
        "\n",
        "Odds and Log-Odds: The linear combination of inputs ( 𝛽 0 + 𝛽 1 𝑋 1 + 𝛽 2 𝑋 2 + ⋯ + 𝛽 𝑛 𝑋 𝑛 ) (β 0​+β 1​X 1​+β 2​X 2​+⋯+β n​X n​) is also known as the log-odds. The odds of the event 𝑌\n",
        "1 Y=1 can be calculated by the following equation:\n",
        "\n",
        "Odds ( 𝑌\n",
        "1 )\n",
        "𝑃 ( 𝑌\n",
        "1 ) 1 − 𝑃 ( 𝑌\n",
        "1 )\n",
        "𝑒 𝛽 0 + 𝛽 1 𝑋 1 + ⋯ + 𝛽 𝑛 𝑋 𝑛 Odds(Y=1)= 1−P(Y=1) P(Y=1)​=e β 0​+β 1​X 1​+⋯+β n​X n​\n",
        "\n",
        "So, logistic regression essentially models the log-odds as a linear combination of the input features. This is what allows it to output probabilities after applying the sigmoid function.\n",
        "\n",
        "Summary: The model computes a linear combination of input features. The result of the linear combination is then passed through the sigmoid function to produce a probability between 0 and 1. If the probability 𝑃 ( 𝑌\n",
        "1 ∣ 𝑋 ) P(Y=1∣X) is greater than 0.5, the model predicts 𝑌\n",
        "1 Y=1; otherwise, it predicts 𝑌\n",
        "0 Y=0. This is the foundation of Logistic Regression, which is commonly used for binary classification problems."
      ],
      "metadata": {
        "id": "GZzKNAg7XY2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3 Why do we use the Sigmoid function in Logistic Regression.**\n",
        "\n",
        "The sigmoid function is used in Logistic Regression for several important reasons, mainly to ensure that the output is a valid probability and to map the linear combination of features to a range between 0 and 1. Here's why it's crucial:\n",
        "\n",
        "Probability Mapping: The key goal in Logistic Regression is to model the probability of an event occurring (i.e., the probability that the output variable 𝑌\n",
        "1 Y=1). Probabilities, by definition, must always lie between 0 and 1.\n",
        "The sigmoid function, defined as:\n",
        "\n",
        "𝜎 ( 𝑧 )\n",
        "1 1 + 𝑒 − 𝑧 σ(z)= 1+e −z\n",
        "\n",
        "1​\n",
        "\n",
        "maps any real-valued input 𝑧 z (which could be the linear combination of features) to a value between 0 and 1. This makes it perfect for representing probabilities.\n",
        "\n",
        "The sigmoid function's S-shaped curve ensures that extreme values of 𝑧 z (either very large or very small) result in outputs very close to 1 or 0, respectively. This is useful in classification problems where we want a definite probability for each class.\n",
        "\n",
        "Nonlinearity: Logistic Regression models the relationship between the input features and the outcome using a logistic function (or log-odds). A linear model like linear regression wouldn't provide a natural way to predict probabilities for classification tasks because the output could be any real number, including negative values or values greater than 1, which are not valid probabilities.\n",
        "The sigmoid's nonlinear nature introduces the necessary transformation, ensuring that outputs are probabilities that can be interpreted meaningfully for classification (e.g., the likelihood of class \"1\"). 3. Interpretability in Terms of Odds: Logistic Regression works in terms of log-odds. The linear combination of features is transformed into log-odds of the probability of the positive class (i.e., 𝑃 ( 𝑌\n",
        "1 ) P(Y=1)):\n",
        "\n",
        "log-odds\n",
        "𝛽 0 + 𝛽 1 𝑋 1 + 𝛽 2 𝑋 2 + ⋯ + 𝛽 𝑛 𝑋 𝑛 log-odds=β 0​+β 1​X 1​+β 2​X 2​+⋯+β n​X n​\n",
        "\n",
        "The sigmoid function converts these log-odds into a probability that is more interpretable and usable for decision-making. This makes it a natural fit for binary classification problems.\n",
        "\n",
        "Gradient Descent and Optimization: The sigmoid function has a very useful mathematical property—it has a derivative that is easy to compute and works well with optimization algorithms like gradient descent. Specifically, the derivative of the sigmoid function is:\n",
        "𝜎 ′ ( 𝑧 )\n",
        "𝜎 ( 𝑧 ) ( 1 − 𝜎 ( 𝑧 ) ) σ ′ (z)=σ(z)(1−σ(z)) This makes it computationally efficient and helps in minimizing the log-likelihood (the objective function) during the training of the model.\n",
        "\n",
        "Smoothing and Confidence Levels: The sigmoid function helps in expressing confidence levels about predictions. For example:\n",
        "A predicted probability close to 0.9 means the model is highly confident that 𝑌\n",
        "1 Y=1. A predicted probability close to 0.1 means the model is highly confident that 𝑌\n",
        "0 Y=0. This smooth transition from 0 to 1 allows the model to give a sense of uncertainty when making predictions, which is useful in practice (e.g., in medical diagnosis, where a high confidence prediction could guide action).\n",
        "\n",
        "Summary of Why the Sigmoid is Used: Probabilistic Output: It transforms the linear output into a valid probability, constrained between 0 and 1. Nonlinearity: It enables the model to capture complex relationships between inputs and outputs, which wouldn't be possible with a simple linear function. Interpretation of Odds: It allows easy interpretation of the model in terms of odds and log-odds.\n",
        "\n",
        "Optimization: The mathematical properties of the sigmoid function make it compatible with optimization techniques like gradient descent. Smooth Confidence Levels: It gives the model a smooth transition between the two classes, providing valuable information on the model’s confidence. In summary, the sigmoid function makes Logistic Regression suitable for classification tasks by mapping linear outputs to a range of probabilities, ensuring interpretability, and enabling efficient training."
      ],
      "metadata": {
        "id": "8mS7QnkNXhvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4 What is the cost function of Logistic Regression.**\n",
        "\n",
        "The cost function in Logistic Regression is designed to measure how well the model's predicted probabilities match the actual class labels (either 0 or 1). This cost function is also known as the log-loss or binary cross-entropy.\n",
        "\n",
        "Cost Function Formula: The cost function for Logistic Regression is the log-likelihood function. It calculates the difference between the predicted probability and the actual outcome (0 or 1). The cost function for a single training example 𝑖 i can be written as:\n",
        "Cost ( ℎ 𝜃 ( 𝑥 ( 𝑖 ) ) , 𝑦 ( 𝑖 ) )\n",
        "− [ 𝑦 ( 𝑖 ) log ⁡ ( ℎ 𝜃 ( 𝑥 ( 𝑖 ) ) ) + ( 1 − 𝑦 ( 𝑖 ) ) log ⁡ ( 1 − ℎ 𝜃 ( 𝑥 ( 𝑖 ) ) ) ] Cost(h θ​(x (i) ),y (i) )=−[y (i) log(h θ​(x (i) ))+(1−y (i) )log(1−h θ​(x (i) ))] Where:\n",
        "\n",
        "ℎ 𝜃 ( 𝑥 ( 𝑖 ) ) h θ​(x (i) ) is the predicted probability for the 𝑖 i-th training example, given by the sigmoid function: ℎ 𝜃 ( 𝑥 ( 𝑖 ) )\n",
        "1 1 + 𝑒 − 𝜃 𝑇 𝑥 ( 𝑖 ) h θ​(x (i) )= 1+e −θ T x (i)\n",
        "\n",
        "1​\n",
        "\n",
        "𝑦 ( 𝑖 ) y (i) is the actual label (0 or 1) for the 𝑖 i-th training example. 𝑥 ( 𝑖 ) x (i) is the input feature vector for the 𝑖 i-th training example. 𝜃 θ represents the model parameters (weights). 2. Total Cost (Cost Function for the Entire Dataset): To compute the total cost over all the training examples, we take the average of the individual costs across all 𝑚 m training examples:\n",
        "\n",
        "𝐽 ( 𝜃 )\n",
        "1 𝑚 ∑ 𝑖\n",
        "1 𝑚 [ − 𝑦 ( 𝑖 ) log ⁡ ( ℎ 𝜃 ( 𝑥 ( 𝑖 ) ) ) − ( 1 − 𝑦 ( 𝑖 ) ) log ⁡ ( 1 − ℎ 𝜃 ( 𝑥 ( 𝑖 ) ) ) ] J(θ)= m 1​\n",
        "\n",
        "i=1 ∑ m​[−y (i) log(h θ​(x (i) ))−(1−y (i) )log(1−h θ​(x (i) ))] Where:\n",
        "\n",
        "𝐽 ( 𝜃 ) J(θ) is the total cost function. 𝑚 m is the number of training examples. The sum is taken over all 𝑚 m examples in the dataset. This is the cost function for binary classification (when the output is either 0 or 1).\n",
        "\n",
        "Interpretation: For 𝑦 ( 𝑖 )\n",
        "1 y (i) =1 (positive class): The cost function penalizes the model more if the predicted probability ℎ 𝜃 ( 𝑥 ( 𝑖 ) ) h θ​(x (i) ) is low (i.e., the model predicts a low probability for class 1 when the true label is 1). For 𝑦 ( 𝑖 )\n",
        "0 y (i) =0 (negative class): The cost function penalizes the model more if the predicted probability ℎ 𝜃 ( 𝑥 ( 𝑖 ) ) h θ​(x (i) ) is high (i.e., the model predicts a high probability for class 1 when the true label is 0).\n",
        "Why This Cost Function? The logarithmic form is chosen because it penalizes incorrect classifications with a large cost. When the predicted probability is very different from the actual outcome (0 or 1), the log function increases the cost significantly. This cost function is differentiable, which is important for optimization algorithms like gradient descent, as it allows us to calculate gradients and minimize the cost efficiently.\n",
        "Gradient of the Cost Function: The goal of training the model is to minimize the total cost function 𝐽 ( 𝜃 ) J(θ). We typically use gradient descent to do this. The gradient of 𝐽 ( 𝜃 ) J(θ) with respect to the parameters 𝜃 θ is computed as follows:\n",
        "∂ 𝐽 ( 𝜃 ) ∂ 𝜃 𝑗\n",
        "1 𝑚 ∑ 𝑖\n",
        "1 𝑚 [ ℎ 𝜃 ( 𝑥 ( 𝑖 ) ) − 𝑦 ( 𝑖 ) ] 𝑥 𝑗 ( 𝑖 ) ∂θ j​\n",
        "\n",
        "∂J(θ)​\n",
        "m 1​\n",
        "\n",
        "i=1 ∑ m​[h θ​(x (i) )−y (i) ]x j (i)​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝜃 𝑗 θ j​is the parameter corresponding to the feature 𝑥 𝑗 x j​. This gradient is used to update the model parameters iteratively in the direction that reduces the cost function. Summary: The cost function of Logistic Regression is the log-loss or binary cross-entropy function. It penalizes large discrepancies between predicted probabilities and actual class labels. The goal is to minimize this cost function using optimization techniques like gradient descent to find the optimal parameters 𝜃 θ. The cost function ensures that the model produces accurate probabilities for classification tasks."
      ],
      "metadata": {
        "id": "pLf7thfcn_Fs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5 What is Regularization in Logistic Regression? Why is it needed.**\n",
        "\n",
        "Regularization in Logistic Regression is a technique used to prevent overfitting by adding a penalty term to the cost function. Overfitting occurs when the model becomes too complex and fits the noise in the training data, resulting in poor generalization to unseen data.\n",
        "\n",
        "Regularization helps improve the model's ability to generalize by discouraging the model from fitting excessively large coefficients (weights), which can lead to overfitting.\n",
        "\n",
        "Why Regularization is Needed: Overfitting Prevention: Without regularization, a logistic regression model might fit the training data too well, capturing noise and random fluctuations that don't generalize well to new, unseen data. Regularization introduces a penalty for large coefficients, preventing the model from becoming too complex.\n",
        "\n",
        "Improved Generalization: Regularization helps the model to focus on the most important features and not rely too heavily on less significant ones. This results in a more robust model that performs better on test data.\n",
        "\n",
        "Simplification of the Model: By penalizing large weights, regularization encourages the model to choose simpler solutions with smaller coefficients, which is often desirable for interpretability and efficiency.\n",
        "\n",
        "Types of Regularization in Logistic Regression: There are two main types of regularization used in logistic regression:\n",
        "\n",
        "L2 Regularization (Ridge Regularization): Formula for Regularized Cost Function: 𝐽 ( 𝜃 )\n",
        "1 𝑚 ∑ 𝑖\n",
        "1 𝑚 [ − 𝑦 ( 𝑖 ) log ⁡ ( ℎ 𝜃 ( 𝑥 ( 𝑖 ) ) ) − ( 1 − 𝑦 ( 𝑖 ) ) log ⁡ ( 1 − ℎ 𝜃 ( 𝑥 ( 𝑖 ) ) ) ]\n",
        "𝜆 2 𝑚 ∑ 𝑗\n",
        "1 𝑛 𝜃 𝑗 2 J(θ)= m 1​\n",
        "i=1 ∑ m​[−y (i) log(h θ​(x (i) ))−(1−y (i) )log(1−h θ​(x (i) ))]+ 2m λ​\n",
        "\n",
        "j=1 ∑ n​θ j 2​\n",
        "\n",
        "Where: 𝜆 λ is the regularization parameter that controls the amount of regularization applied. The second term 𝜆 2 𝑚 ∑ 𝑗\n",
        "1 𝑛 𝜃 𝑗 2 2m λ​∑ j=1 n​θ j 2​is the penalty term added to the cost function. The penalty term adds a cost proportional to the square of the magnitude of the weights 𝜃 𝑗 θ j​. Effect: The L2 regularization term penalizes large weights by adding a cost proportional to the square of the magnitude of the coefficients. This encourages the model to learn smaller coefficients, leading to a smoother decision boundary and better generalization. 2. L1 Regularization (Lasso Regularization): Formula for Regularized Cost Function: 𝐽 ( 𝜃 )\n",
        "1 𝑚 ∑ 𝑖\n",
        "1 𝑚 [ − 𝑦 ( 𝑖 ) log ⁡ ( ℎ 𝜃 ( 𝑥 ( 𝑖 ) ) ) − ( 1 − 𝑦 ( 𝑖 ) ) log ⁡ ( 1 − ℎ 𝜃 ( 𝑥 ( 𝑖 ) ) ) ] + 𝜆 ∑ 𝑗\n",
        "1 𝑛 ∣ 𝜃 𝑗 ∣ J(θ)= m 1​\n",
        "\n",
        "i=1 ∑ m​[−y (i) log(h θ​(x (i) ))−(1−y (i) )log(1−h θ​(x (i) ))]+λ j=1 ∑ n​∣θ j​∣ Where: 𝜆 λ is the regularization parameter. The penalty term 𝜆 ∑ 𝑗\n",
        "1 𝑛 ∣ 𝜃 𝑗 ∣ λ∑ j=1 n​∣θ j​∣ adds a cost proportional to the absolute value of the weights. Effect: The L1 regularization term has a different effect in that it can shrink some coefficients exactly to zero. This means it can perform feature selection by removing irrelevant features from the model, making it sparse and easier to interpret. The Role of the Regularization Parameter 𝜆 λ: The regularization parameter 𝜆 λ controls the trade-off between fitting the training data well and keeping the model simple (preventing overfitting). When 𝜆\n",
        "0 λ=0: There is no regularization, and the model behaves like standard logistic regression without any penalty. When 𝜆 λ is large: The regularization term dominates, and the model is forced to have small coefficients, potentially underfitting the data. When 𝜆 λ is small: The model focuses more on fitting the data and may overfit, especially if the data is noisy. Why Regularization Works: Bias-Variance Trade-off: Regularization helps to balance the bias (error due to overly simple models) and variance (error due to overly complex models). By penalizing large weights, regularization reduces variance and helps improve the model’s performance on unseen data.\n",
        "\n",
        "Control of Model Complexity: Regularization effectively reduces the complexity of the model by discouraging overly large coefficients. This results in a simpler, more interpretable model.\n",
        "\n",
        "Regularization is a technique used in Logistic Regression to prevent overfitting by adding a penalty term to the cost function. L2 Regularization (Ridge) penalizes the sum of the squared weights, leading to smaller coefficients and smoother models. L1 Regularization (Lasso) penalizes the sum of the absolute weights, and it can shrink some coefficients to zero, effectively performing feature selection. The regularization parameter 𝜆 λ controls the trade-off between fitting the data well and keeping the model simple."
      ],
      "metadata": {
        "id": "b0rjQaQUoG7e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6 Explain the difference between Lasso, Ridge, and Elastic Net regression.**\n",
        "\n",
        "Lasso, Ridge, and Elastic Net regression are all regularized versions of linear regression that help to prevent overfitting by adding a penalty term to the cost function. They differ in the type of regularization they use, which affects how they shrink the coefficients. Here’s a breakdown of each:\n",
        "\n",
        "Ridge Regression (L2 Regularization): Regularization Term: Ridge regression adds a penalty term to the cost function proportional to the square of the coefficients: Cost Function (Ridge)\n",
        "1 𝑚 ∑ 𝑖\n",
        "1 𝑚 ( 𝑦 ( 𝑖 ) − 𝑦 ^ ( 𝑖 ) ) 2\n",
        "𝜆 ∑ 𝑗\n",
        "1 𝑛 𝜃 𝑗 2 Cost Function (Ridge)= m 1​\n",
        "i=1 ∑ m​(y (i) − y ^​\n",
        "\n",
        "(i) ) 2 +λ j=1 ∑ n​θ j 2​\n",
        "\n",
        "Where: 𝜆 λ is the regularization parameter that controls the strength of the penalty. 𝜃 𝑗 θ j​are the coefficients of the model. The term ∑ 𝑗\n",
        "1 𝑛 𝜃 𝑗 2 ∑ j=1 n​θ j 2​is the sum of the squared coefficients. Effect: Ridge regression penalizes large coefficients, encouraging the model to shrink them towards zero. However, it does not set coefficients exactly to zero. All features remain in the model, but with smaller coefficients. Use Case: Ridge is useful when you believe all features are important but want to reduce the impact of large coefficients, which can cause overfitting. 2. Lasso Regression (L1 Regularization): Regularization Term: Lasso regression adds a penalty term to the cost function proportional to the absolute value of the coefficients: Cost Function (Lasso)\n",
        "1 𝑚 ∑ 𝑖\n",
        "1 𝑚 ( 𝑦 ( 𝑖 ) − 𝑦 ^ ( 𝑖 ) ) 2 + 𝜆 ∑ 𝑗\n",
        "1 𝑛 ∣ 𝜃 𝑗 ∣ Cost Function (Lasso)= m 1​\n",
        "\n",
        "i=1 ∑ m​(y (i) − y ^​\n",
        "\n",
        "(i) ) 2 +λ j=1 ∑ n​∣θ j​∣ Where: 𝜆 λ is the regularization parameter. 𝜃 𝑗 θ j​are the coefficients of the model. The term ∑ 𝑗\n",
        "1 𝑛 ∣ 𝜃 𝑗 ∣ ∑ j=1 n​∣θ j​∣ is the sum of the absolute values of the coefficients. Effect: Lasso regression penalizes large coefficients and can shrink some coefficients exactly to zero. This leads to sparse solutions, effectively performing feature selection by removing less important features from the model. Use Case: Lasso is useful when you suspect that some features are irrelevant or should be excluded from the model. 3. Elastic Net Regression: Regularization Term: Elastic Net combines both L1 (Lasso) and L2 (Ridge) regularization. The penalty term is a linear combination of the L1 and L2 penalties: Cost Function (Elastic Net)\n",
        "1 𝑚 ∑ 𝑖\n",
        "1 𝑚 ( 𝑦 ( 𝑖 ) − 𝑦 ^ ( 𝑖 ) ) 2 + 𝜆 1 ∑ 𝑗\n",
        "1 𝑛 ∣ 𝜃 𝑗 ∣ + 𝜆 2 ∑ 𝑗\n",
        "1 𝑛 𝜃 𝑗 2 Cost Function (Elastic Net)= m 1​\n",
        "\n",
        "i=1 ∑ m​(y (i) − y ^​\n",
        "\n",
        "(i) ) 2 +λ 1​\n",
        "\n",
        "j=1 ∑ n​∣θ j​∣+λ 2​\n",
        "\n",
        "j=1 ∑ n​θ j 2​\n",
        "\n",
        "Where: 𝜆 1 λ 1​controls the strength of the L1 penalty (Lasso component). 𝜆 2 λ 2​controls the strength of the L2 penalty (Ridge component). Effect: Elastic Net performs both L1 and L2 regularization simultaneously, combining the benefits of both methods. It can shrink coefficients (like Ridge) and set some coefficients to zero (like Lasso). This makes it effective when there are many correlated features or when you want to balance between feature selection and regularization. Use Case: Elastic Net is particularly useful when you have a large number of correlated features and want a combination of Ridge's smoothness and Lasso's feature selection. Summary of Key Differences: Feature Ridge Regression (L2) Lasso Regression (L1) Elastic Net Regularization Type L2 regularization (sum of squared coefficients). L1 regularization (sum of absolute coefficients). Combination of L1 and L2 regularization. Effect on Coefficients Shrinks coefficients towards zero, but none are set to zero. Can shrink coefficients to zero (performs feature selection). Shrinks coefficients and can set some to zero (feature selection). Penalty Term 𝜆 ∑ 𝑗\n",
        "1 𝑛 𝜃 𝑗 2 λ∑ j=1 n​θ j 2​( \\lambda \\sum_{j=1}^{n} \\theta_j Use Case When all features are expected to contribute, and you want to avoid large coefficients. When some features are expected to be irrelevant, and you want to perform feature selection. When you want a balance between feature selection and regularization, especially with correlated features. Interpretation All features remain in the model with smaller coefficients. Some features are excluded from the model (coefficients set to zero). Balances between Ridge and Lasso, so it can perform feature selection with regularization. In Summary: Ridge (L2) regularization is best when you have many features and don't necessarily want to exclude any, but just need to reduce the influence of large coefficients. Lasso (L1) regularization is ideal when you suspect that many features are irrelevant and you want to remove them entirely (feature selection). Elastic Net combines the strengths of both Ridge and Lasso and is useful when you have many correlated features and want both regularization and feature selection. The choice between these methods depends on the nature of your dataset and the relationships between your features."
      ],
      "metadata": {
        "id": "nzPcqMQ7oNR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7 When should we use Elastic Net instead of Lasso or Ridge.**\n",
        "\n",
        "Elastic Net should be used instead of Lasso or Ridge when certain conditions arise that make the combined strengths of both L1 (Lasso) and L2 (Ridge) regularization particularly useful. Here’s when you should choose Elastic Net over Lasso or Ridge:\n",
        "\n",
        "When There Are Many Correlated Features: Problem with Lasso: Lasso (L1 regularization) tends to select only one variable from a group of correlated features and set the others to zero. This can lead to instability in the model because if the correlation between features changes slightly, the model might arbitrarily choose a different feature from the group. Elastic Net Solution: Elastic Net is better suited for situations with correlated features because it uses both L1 and L2 regularization. The L2 component helps group correlated features together, allowing the model to retain more than one feature from the group, even if they are correlated. When to Use: If your dataset contains many features that are highly correlated, Elastic Net can help by maintaining a group of features, rather than discarding most of them, as Lasso might.\n",
        "\n",
        "When You Have a Large Number of Features Relative to the Number of Observations (High-dimensional Data): Problem with Lasso: When the number of features 𝑝 p is large compared to the number of observations 𝑛 n, Lasso can sometimes perform poorly, especially if there are fewer observations than features. This is because Lasso tends to select a subset of features, which can lead to overfitting or instability. Elastic Net Solution: Elastic Net performs better in high-dimensional settings by providing a balance between L1 and L2 penalties. The L2 part ensures that the coefficients are not overly sparse, which is particularly useful when dealing with datasets where there are many more features than observations. When to Use: If your dataset has many features (especially when 𝑝\n",
        "\n",
        "𝑛 p>n), Elastic Net is a good choice because it stabilizes the feature selection process and prevents overfitting.\n",
        "\n",
        "When You Need a Balance Between Feature Selection and Regularization: Problem with Lasso: Lasso performs feature selection by setting some coefficients to exactly zero. While this is useful for eliminating irrelevant features, it might discard useful features, especially if they are highly correlated with others. Problem with Ridge: Ridge doesn't perform feature selection since it only shrinks coefficients. It keeps all features in the model, which is useful when you believe all features are relevant but doesn't reduce model complexity as much as Lasso. Elastic Net Solution: Elastic Net allows you to combine both regularization approaches, so it can both shrink coefficients (like Ridge) and set some coefficients to zero (like Lasso). This is particularly useful when you believe some features should be excluded, but others need to be kept, especially in cases where there is correlation between features. When to Use: If you need both feature selection and regularization at the same time, and want to find a balance between them, Elastic Net is a good choice. It works well when you have a mix of irrelevant features (which you want to discard) and important features (which you want to shrink but retain).\n",
        "\n",
        "When You Want to Improve Model Stability and Performance: Problem with Lasso: Lasso can sometimes be unstable when the features are highly correlated, as it tends to arbitrarily select one feature from a group, which might change as the data changes. Elastic Net Solution: Elastic Net provides more stable results, especially in the presence of multicollinearity. The L2 regularization in Elastic Net encourages stability, as it distributes the penalty across all correlated features, rather than discarding most of them like Lasso. When to Use: If you need more robust and stable models when features are correlated, or when you're concerned about Lasso’s instability due to multicollinearity, Elastic Net is a good alternative.\n",
        "\n",
        "When You Have a Small Dataset with Many Features: Problem with Lasso: Lasso can lead to overfitting in cases where you have very few data points compared to the number of features, especially if features are highly correlated. Elastic Net Solution: Elastic Net helps prevent overfitting in small datasets by combining the strengths of Lasso (feature selection) and Ridge (regularization and stability). It reduces the chance of overfitting while still performing feature selection. When to Use: If you have a small dataset with many features, especially when those features are correlated, Elastic Net can help balance the trade-off between overfitting and feature selection."
      ],
      "metadata": {
        "id": "dri4vbTaoaqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8 What is the impact of the regularization parameter (λ) in Logistic Regression.**\n",
        "\n",
        "The regularization parameter ( 𝜆 λ) in Logistic Regression controls the strength of the regularization and has a significant impact on the model's performance. Regularization is used to prevent overfitting by discouraging overly complex models. The value of 𝜆 λ determines the trade-off between fitting the training data well and keeping the model simple. Here's a breakdown of how the regularization parameter affects Logistic Regression:\n",
        "\n",
        "Effect of 𝜆 λ on the Cost Function: The cost function in Logistic Regression with L2 regularization (Ridge) looks like this:\n",
        "𝐽 ( 𝜃 )\n",
        "1 𝑚 ∑ 𝑖\n",
        "1 𝑚 [ − 𝑦 ( 𝑖 ) log ⁡ ( ℎ 𝜃 ( 𝑥 ( 𝑖 ) ) ) − ( 1 − 𝑦 ( 𝑖 ) ) log ⁡ ( 1 − ℎ 𝜃 ( 𝑥 ( 𝑖 ) ) ) ] + 𝜆 2 𝑚 ∑ 𝑗\n",
        "1 𝑛 𝜃 𝑗 2 J(θ)= m 1​\n",
        "\n",
        "i=1 ∑ m​[−y (i) log(h θ​(x (i) ))−(1−y (i) )log(1−h θ​(x (i) ))]+ 2m λ​\n",
        "\n",
        "j=1 ∑ n​θ j 2​\n",
        "\n",
        "For L1 regularization (Lasso), the penalty term is:\n",
        "\n",
        "𝐽 ( 𝜃 )\n",
        "1 𝑚 ∑ 𝑖\n",
        "1 𝑚 [ − 𝑦 ( 𝑖 ) log ⁡ ( ℎ 𝜃 ( 𝑥 ( 𝑖 ) ) ) − ( 1 − 𝑦 ( 𝑖 ) ) log ⁡ ( 1 − ℎ 𝜃 ( 𝑥 ( 𝑖 ) ) ) ] + 𝜆 ∑ 𝑗\n",
        "1 𝑛 ∣ 𝜃 𝑗 ∣ J(θ)= m 1​\n",
        "\n",
        "i=1 ∑ m​[−y (i) log(h θ​(x (i) ))−(1−y (i) )log(1−h θ​(x (i) ))]+λ j=1 ∑ n​∣θ j​∣ In both cases, the regularization term is controlled by 𝜆 λ, which dictates how heavily the model penalizes large coefficients.\n",
        "\n",
        "Impact of 𝜆 λ on Model Behavior: When 𝜆\n",
        "0 λ=0 (No Regularization): Impact: The model becomes a standard Logistic Regression model without regularization. It tries to fit the training data as perfectly as possible, which may lead to overfitting if the data is noisy or has a high variance. The coefficients 𝜃 θ can grow large, causing the model to become too complex and less generalizable to new data. Effect on Coefficients: The coefficients 𝜃 𝑗 θ j​are free to take any value, and the model focuses entirely on minimizing the error on the training data. When 𝜆 λ is very large (Strong Regularization): Impact: As 𝜆 λ increases, the regularization term dominates the cost function, forcing the coefficients 𝜃 𝑗 θ j​to become smaller, and eventually, close to zero. This leads to a simpler model with lower variance but potentially higher bias. Effect on Coefficients: Coefficients shrink towards zero, and the model might exclude some features (particularly in Lasso), making it underfit the training data. It will not capture the underlying patterns of the data well, resulting in poor performance on both the training and test datasets. Model Behavior: The model becomes too simple, focusing on the most important features while ignoring less important ones. When 𝜆 λ is small (Weak Regularization): Impact: A small value of 𝜆 λ means less penalty on large coefficients, allowing the model to fit the training data more closely. However, this increases the risk of overfitting, especially if the dataset is noisy or has many irrelevant features. Effect on Coefficients: Coefficients 𝜃 𝑗 θ j​can grow larger, and the model will pay less attention to penalizing large values. This allows the model to better fit the training data, but it may sacrifice generalization to unseen data. Model Behavior: The model is more complex, with the ability to fit the training data very well, but may perform poorly on new, unseen data due to overfitting."
      ],
      "metadata": {
        "id": "JRow6vfMuyAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9 What are the key assumptions of Logistic Regression.**\n",
        "\n",
        "Logistic Regression, like many statistical models, relies on a set of assumptions to ensure that the model works effectively and provides meaningful results. While it is more flexible than linear regression, it still makes several key assumptions about the data. Here are the key assumptions of Logistic Regression:\n",
        "\n",
        "Binary Outcome (Dependent Variable) Assumption: Logistic Regression is typically used when the dependent variable is binary (i.e., it takes on two possible outcomes such as 0 or 1, true or false, success or failure). It models the probability of the outcome being one of the two classes. Why it matters: Logistic Regression uses the log-odds transformation, which is designed to handle binary outcomes. If the dependent variable has more than two categories, you may need to use Multinomial Logistic Regression.\n",
        "Independence of Observations Assumption: The observations in the dataset must be independent of each other. This means that the outcome for one observation should not influence or depend on the outcome of another observation. Why it matters: If there is correlation between observations, such as in time-series data or clustered data (e.g., patients from the same hospital), the independence assumption is violated. In such cases, specialized techniques like Generalized Estimating Equations (GEE) or Mixed Effects Models may be more appropriate.\n",
        "Linear Relationship Between the Log-Odds and the Predictors Assumption: Logistic Regression assumes that there is a linear relationship between the log-odds of the dependent variable and the independent variables (predictors). In other words, the logit function (the log-odds) of the probability is a linear combination of the predictors. Why it matters: This assumption is crucial for the model to work. If the relationship between the predictors and the log-odds is not linear, the model's predictions may be biased. You can check this assumption by looking at residual plots or using transformations for predictors (e.g., polynomial or interaction terms).\n",
        "No or Little Multicollinearity Assumption: The independent variables should not be highly correlated with each other (i.e., no multicollinearity). Why it matters: When predictors are highly correlated, it becomes difficult to estimate the individual effect of each predictor on the outcome, leading to unstable coefficient estimates and inflated standard errors. You can check for multicollinearity using the Variance Inflation Factor (VIF) and remove or combine highly correlated predictors if necessary.\n",
        "Sufficient Sample Size Assumption: Logistic Regression requires a sufficient sample size to provide reliable and valid results. Why it matters: The larger the sample size, the more accurately the model can estimate the coefficients. A small sample size can lead to overfitting, bias, and unstable estimates of the model parameters. A general rule of thumb is to have at least 10 events per predictor variable.\n",
        "No Outliers or High Leverage Points Assumption: Logistic Regression assumes there are no influential outliers or high-leverage points that disproportionately affect the model's estimates. Why it matters: Outliers and high-leverage points can have a strong influence on the model's predictions, leading to biased or incorrect coefficients. It's important to detect and address them using diagnostic tools such as Cook's Distance or leverage plots.\n",
        "Homoscedasticity is Not Required Assumption: Unlike Linear Regression, Logistic Regression does not require homoscedasticity (constant variance of the residuals). Why it matters: The variance of the error terms is not assumed to be constant in Logistic Regression, because the dependent variable is categorical, and the variance depends on the probability of the event occurring. However, checking for goodness of fit and using Hosmer-Lemeshow tests can help assess if the model fits well.\n",
        "No Strongly Influential Outliers Assumption: While outliers are generally a concern, Logistic Regression assumes that extreme values of the predictor variables are not unduly influencing the estimates. Why it matters: Highly influential observations can distort the coefficients. You can identify such points using leverage statistics and Cook’s Distance and decide whether to exclude them from the analysis.\n",
        "Additivity of Effects (Optional) Assumption: Logistic Regression assumes that the relationship between the predictors and the log-odds is additive. This means that the effect of each predictor is independent of the others (i.e., the effect of one predictor is not altered by the presence of another predictor). Why it matters: If there are significant interactions between predictors, the model may not capture this interaction effect unless you explicitly model it. You can check for interactions by including interaction terms in the model.\n",
        "Absence of Perfect Separation Assumption: Logistic Regression assumes that there is no perfect separation in the data, meaning that the predictors cannot perfectly separate the two outcome classes. Why it matters: Perfect separation occurs when one or more predictors can completely predict the outcome (i.e., one class for every unique value of the predictor). In this case, the model cannot estimate coefficients for the predictors, leading to infinite estimates of the coefficients. This can be checked using cross-validation and adjustments such as Firth's method to handle small sample sizes."
      ],
      "metadata": {
        "id": "tazFwB6_u-G6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. What are some alternatives to Logistic Regression for classification tasks.**\n",
        "\n",
        "There are several alternatives to Logistic Regression for classification tasks, each with its own strengths, weaknesses, and use cases. These alternatives often provide better performance when Logistic Regression assumptions (e.g., linear decision boundary, independence of features) are violated, or when more complex relationships exist in the data.\n",
        "\n",
        "Here’s an overview of some popular alternatives:\n",
        "\n",
        "Decision Trees Overview: Decision Trees are a non-linear classifier that splits the data into branches based on feature values, forming a tree-like structure. Each internal node represents a decision based on a feature, and each leaf node represents a class label. Pros: Can model non-linear relationships. Easy to interpret and visualize. Does not require feature scaling or normalization. Cons: Prone to overfitting, especially with deep trees. Sensitive to noisy data. When to use: Useful when you need an interpretable model and the relationship between features and the outcome is complex or non-linear.\n",
        "Random Forests Overview: Random Forests are an ensemble method that builds multiple decision trees and combines their predictions (using majority voting for classification). The trees are built on different subsets of data and feature subsets, reducing variance and overfitting compared to individual decision trees. Pros: High performance (works well in many scenarios). Handles non-linear relationships well. Less prone to overfitting than a single decision tree. Cons: Can be computationally expensive, especially for large datasets. Model interpretability can be limited due to the complexity of multiple trees. When to use: Use when you need high predictive accuracy and can afford some complexity in terms of computational cost.\n",
        "Support Vector Machines (SVM) Overview: SVM is a powerful classifier that tries to find a hyperplane that best separates the data into different classes. It uses kernel functions to handle non-linear decision boundaries. Pros: Can create complex decision boundaries using kernels. Effective in high-dimensional spaces. Well-suited for smaller, complex datasets. Cons: Can be computationally expensive, especially with large datasets. Sensitive to the choice of kernel and hyperparameters (e.g., regularization parameter 𝐶 C, kernel type). Hard to interpret the model. When to use: Use SVM for high-dimensional data, especially in cases where you want to model a complex, non-linear decision boundary.\n",
        "k-Nearest Neighbors (k-NN) Overview: k-NN is a non-parametric, lazy learning algorithm that assigns a class label based on the majority class of the k nearest data points in the feature space. Pros: Simple and intuitive. Non-parametric, so no assumptions about the data distribution. Can model complex decision boundaries. Cons: Computationally expensive at prediction time (needs to compute distances to all training points). Sensitive to irrelevant or redundant features (curse of dimensionality). Performance depends on the choice of 𝑘 k and distance metric. When to use: Suitable for smaller datasets where you need a simple, non-parametric model, or when relationships between classes are very localized.\n",
        "Naive Bayes Overview: Naive Bayes is a probabilistic classifier based on Bayes' theorem. It assumes that the features are conditionally independent given the class (hence \"naive\"). Pros: Simple and fast. Works well with small datasets and high-dimensional data (e.g., text classification). Performs well when the independence assumption holds or approximately holds. Cons: The independence assumption is often unrealistic and may lead to suboptimal performance when features are highly correlated. Not well-suited for complex non-linear relationships. When to use: Great for text classification or when you have many features and need a fast, simple probabilistic model.\n",
        "Gradient Boosting Machines (GBM) Overview: Gradient Boosting is an ensemble learning technique that builds multiple decision trees sequentially, with each tree trying to correct the errors of the previous one. The most popular variant is XGBoost. Pros: High predictive accuracy. Can model complex, non-linear relationships. Handles both classification and regression tasks. Cons: Can be computationally intensive, especially with large datasets. Sensitive to overfitting (though regularization techniques can help). Hyperparameter tuning can be complex. When to use: Use when predictive performance is critical and you are willing to invest time in hyperparameter tuning. Suitable for structured/tabular data.\n",
        "Neural Networks (Artificial Neural Networks) Overview: Neural Networks are computational models inspired by the human brain, consisting of layers of interconnected neurons (nodes). They can learn highly complex relationships in the data. Pros: Extremely flexible and powerful, capable of modeling very complex, non-linear relationships. Performs well with large datasets and can be fine-tuned for specific tasks. Cons: Requires large amounts of data to train effectively. Computationally expensive, especially for deep neural networks. Requires careful tuning of architecture (e.g., number of layers, neurons, activation functions). When to use: Use when you have large datasets, complex patterns to model (e.g., image or speech data), and computational resources to train the model.\n",
        "Logistic Regression with Regularization (Lasso/Ridge/Elastic Net) Overview: Although Logistic Regression is a commonly used model, applying regularization (Lasso, Ridge, or Elastic Net) can improve its performance in the presence of multicollinearity, overfitting, or feature selection needs. Pros: Can handle feature selection (Lasso) and multicollinearity (Ridge). Efficient and interpretable, especially when using regularization. Cons: Assumes linearity between the predictors and log-odds, which may not always hold. When to use: Use when you want a linear model but need regularization to deal with overfitting, feature selection, or multicollinearity."
      ],
      "metadata": {
        "id": "hKnjI3A1vETw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11 What are Classification Evaluation Metrics.**\n",
        "\n",
        "Classification Evaluation Metrics Classification evaluation metrics are essential for assessing how well a classification model is performing. These metrics help quantify the model's ability to predict classes accurately and provide insights into areas that may need improvement, such as false positives or negatives. Below is an overview of the key classification evaluation metrics:\n",
        "\n",
        "Accuracy Definition: Accuracy is the ratio of the number of correct predictions to the total number of predictions. Formula: Accuracy\n",
        "True Positives\n",
        "True Negatives Total Predictions Accuracy= Total Predictions True Positives+True Negatives​\n",
        "Interpretation: Accuracy is a simple metric, and it works well when the classes are balanced (i.e., each class has approximately the same number of instances). However, it can be misleading in cases of class imbalance (e.g., when one class is much more frequent than the other). Use case: Best for balanced datasets where each class has roughly the same number of examples. 2. Precision (Positive Predictive Value) Definition: Precision measures the proportion of true positive predictions out of all predictions that were classified as positive. Formula: Precision\n",
        "True Positives True Positives + False Positives Precision= True Positives+False Positives True Positives​\n",
        "\n",
        "Interpretation: Precision is particularly useful when the cost of false positives is high. For example, in medical diagnostics, a false positive (e.g., diagnosing a healthy person as sick) may be undesirable. Use case: Important in scenarios where false positives are costly or undesirable. 3. Recall (Sensitivity, True Positive Rate) Definition: Recall measures the proportion of true positive predictions out of all actual positive instances in the data. Formula: Recall\n",
        "True Positives True Positives + False Negatives Recall= True Positives+False Negatives True Positives​\n",
        "\n",
        "Interpretation: Recall is critical when the cost of false negatives is high. For instance, in fraud detection, failing to detect fraud (false negative) could be more detrimental than flagging a legitimate transaction (false positive). Use case: Important when you want to capture as many positive instances as possible, even at the cost of increasing false positives. 4. F1-Score (F1-Measure) Definition: The F1-score is the harmonic mean of precision and recall. It is a balanced metric that combines both precision and recall into one number, making it particularly useful when you need to balance both false positives and false negatives. Formula: 𝐹 1\n",
        "2 × Precision × Recall Precision + Recall F1=2× Precision+Recall Precision×Recall​\n",
        "\n",
        "Interpretation: The F1-score is valuable when you have imbalanced datasets or when both false positives and false negatives are important to minimize. It provides a balance between precision and recall. Use case: Often used when there is an imbalance in the class distribution and when both precision and recall are equally important. 5. Specificity (True Negative Rate) Definition: Specificity measures the proportion of true negatives correctly identified out of all actual negative instances. Formula: Specificity\n",
        "True Negatives True Negatives + False Positives Specificity= True Negatives+False Positives True Negatives​\n",
        "\n",
        "Interpretation: Specificity is often used in conjunction with recall, as it helps measure how well the model is at identifying negative cases. It is important when false positives have serious consequences. Use case: Typically used alongside recall in situations where both false positives and false negatives need to be balanced (e.g., in medical diagnostics where identifying negative cases is as important as identifying positives). 6. Confusion Matrix Definition: A confusion matrix is a tabular summary of the model's predictions, showing the counts of actual vs. predicted classes. It provides a comprehensive view of the performance, showing true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN). Structure: Predicted Positive Predicted Negative Actual Positive TP FN Actual Negative FP TN Actual Positive Actual Negative​\n",
        "\n",
        "Predicted Positive TP FP​\n",
        "\n",
        "Predicted Negative FN TN​\n",
        "\n",
        "​\n",
        "\n",
        "Interpretation: The confusion matrix allows you to visually assess how well the model is classifying the instances of each class, and it is the foundation for calculating many of the evaluation metrics like accuracy, precision, recall, etc. Use case: Ideal for understanding the overall classification performance and the trade-offs between different types of errors (false positives vs. false negatives). 7. ROC Curve (Receiver Operating Characteristic Curve) Definition: The ROC curve plots the True Positive Rate (Recall) against the False Positive Rate at various threshold settings. It shows the trade-off between sensitivity and specificity. Interpretation: A curve closer to the top-left corner indicates better performance. The area under the curve (AUC) can be used to summarize the ROC curve. A model with an AUC of 1 is perfect, while an AUC of 0.5 indicates random guessing. Use case: Use the ROC curve when you want to evaluate model performance across different thresholds, and AUC is particularly useful for comparing classifiers. 8. AUC-ROC (Area Under the Curve - Receiver Operating Characteristic) Definition: AUC is the area under the ROC curve, which quantifies the overall ability of the model to discriminate between positive and negative classes. Formula: AUC is a single value representing the area under the ROC curve. Interpretation: AUC ranges from 0 to 1. AUC of 0.5 indicates a model performing no better than random guessing, while an AUC of 1 indicates a perfect model. Use case: AUC is useful when you want an aggregate measure of performance across all classification thresholds, particularly for imbalanced datasets. 9. Logarithmic Loss (Log Loss) Definition: Log Loss measures the performance of a classification model where the output is a probability value between 0 and 1. It evaluates how close the predicted probabilities are to the actual class labels. Formula: Log Loss\n",
        "− 1 𝑁 ∑ 𝑖\n",
        "1 𝑁 ( 𝑦 𝑖 log ⁡ ( 𝑝 𝑖 ) + ( 1 − 𝑦 𝑖 ) log ⁡ ( 1 − 𝑝 𝑖 ) ) Log Loss=− N 1​\n",
        "\n",
        "i=1 ∑ N​(y i​log(p i​)+(1−y i​)log(1−p i​)) where 𝑦 𝑖 y i​is the true label (0 or 1), and 𝑝 𝑖 p i​is the predicted probability for class 1. Interpretation: The lower the Log Loss, the better the model's predicted probabilities align with the true class labels. A log loss of 0 indicates perfect predictions. Use case: Often used when the model outputs probabilities, such as in neural networks or probabilistic classifiers. 10. Matthews Correlation Coefficient (MCC) Definition: MCC is a measure that takes into account true positives, true negatives, false positives, and false negatives. It is particularly useful for imbalanced datasets. Formula: MCC\n",
        "TP × TN − FP × FN ( TP + FP ) ( TP + FN ) ( TN + FP ) ( TN + FN ) MCC= (TP+FP)(TP+FN)(TN+FP)(TN+FN)​\n",
        "\n",
        "TP×TN−FP×FN​\n",
        "\n",
        "Interpretation: MCC returns a value between -1 and 1. An MCC of 1 indicates a perfect classifier, 0 indicates no better than random guessing, and -1 indicates total disagreement between prediction and truth. Use case: Particularly useful for binary classification tasks with imbalanced classes."
      ],
      "metadata": {
        "id": "sL8eerxLvLK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12 How does class imbalance affect Logistic Regression.**\n",
        "\n",
        "How Class Imbalance Affects Logistic Regression Class imbalance occurs when the number of instances in one class significantly outweighs the number of instances in the other class. In the context of Logistic Regression, class imbalance can lead to several issues that impact the model's performance, evaluation, and interpretation.\n",
        "\n",
        "Bias Toward the Majority Class Effect: Logistic Regression, like many machine learning models, is sensitive to the distribution of the classes. In the case of class imbalance, the model tends to be biased toward the majority class because the loss function (e.g., log-loss or cross-entropy) is dominated by the majority class. This means that the model may predict the majority class much more often than the minority class, even if this results in a higher number of false negatives for the minority class. Example: If 95% of your data belongs to class \"0\" (majority) and 5% to class \"1\" (minority), the model might predict \"0\" most of the time to maximize accuracy, even if this means it fails to identify many instances of class \"1.\"\n",
        "\n",
        "Inaccurate Evaluation Metrics Effect: When there is significant class imbalance, accuracy can be misleading because a high accuracy can be achieved by simply predicting the majority class for every instance. For example, predicting the majority class (class \"0\" in the above example) for every sample would result in 95% accuracy, even though the model fails to detect any instances of class \"1.\" Better Alternatives: In imbalanced datasets, metrics like precision, recall, F1-score, and AUC-ROC are more useful than accuracy. These metrics account for the performance on both classes and help measure how well the model is detecting the minority class.\n",
        "\n",
        "Misleading Predictions Effect: In the case of class imbalance, the decision threshold of Logistic Regression (usually set at 0.5 for predicting the positive class) may not be appropriate. A model that predicts the majority class most of the time might still assign a probability of 0.5 or higher to the minority class, but with much lower confidence. This can lead to suboptimal decision-making, especially in critical applications like medical diagnoses or fraud detection. Example: If the model's decision threshold isn't adjusted, it might assign very low probabilities to the minority class, causing it to predict the minority class incorrectly or fail to predict it at all.\n",
        "\n",
        "Reduced Sensitivity to the Minority Class Effect: Logistic Regression may struggle to identify the minority class, leading to a low recall for the minority class (i.e., many false negatives). Since the model focuses on maximizing overall accuracy, it may fail to detect true positive instances of the minority class, resulting in an underperforming model for that class. Example: In fraud detection, where fraudulent transactions are rare, Logistic Regression might miss a large proportion of fraud cases (i.e., high false negatives) because of the class imbalance. Ways to Address Class Imbalance in Logistic Regression Several strategies can be used to mitigate the impact of class imbalance in Logistic Regression:\n",
        "\n",
        "Resampling Techniques Oversampling the Minority Class: This involves replicating the minority class instances to balance the number of examples in both classes. SMOTE (Synthetic Minority Over-sampling Technique) is a popular oversampling method that creates synthetic instances of the minority class by interpolating between existing instances. Undersampling the Majority Class: This involves randomly removing examples from the majority class to balance the class distribution. Effectiveness: Resampling can make the model more sensitive to the minority class, but it may lead to overfitting (in the case of oversampling) or loss of valuable information (in the case of undersampling).\n",
        "\n",
        "Adjusting the Decision Threshold Effect: By adjusting the decision threshold (e.g., from 0.5 to a lower value), you can make the model more likely to classify instances as belonging to the minority class. Lowering the threshold increases recall for the minority class, but it may also increase the number of false positives. Implementation: The decision threshold can be optimized based on the evaluation metrics (e.g., maximizing the F1-score or precision-recall curve) rather than just accuracy.\n",
        "\n",
        "Using Class Weights Effect: Logistic Regression models can be adjusted to give higher weight to the minority class, penalizing the model more for misclassifying instances of the minority class. This helps the model focus more on the minority class during training. Implementation: Most machine learning libraries (e.g., Scikit-learn) allow you to set the class_weight parameter to \"balanced\", which automatically assigns higher weights to the minority class. Result: By increasing the weight of the minority class, you force the model to be more sensitive to it, improving recall and potentially F1-score.\n",
        "\n",
        "Collecting More Data Effect: If feasible, obtaining more data for the minority class can help balance the dataset and improve the performance of the model. More balanced data can help the Logistic Regression model to learn better representations of both classes. Challenge: This might not always be practical, especially for rare events like fraud detection or disease diagnosis, where collecting additional data for the minority class may be difficult or expensive.\n",
        "\n",
        "Using Alternative Models Effect: Logistic Regression may not always be the best model for imbalanced data. You might consider using more complex models that are inherently better at handling class imbalance, such as Random Forests or Gradient Boosting Machines. These models typically perform better with imbalanced data due to their ability to learn non-linear decision boundaries and their built-in mechanisms for handling class imbalance."
      ],
      "metadata": {
        "id": "uPDpeJcTvTX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13 What is Hyperparameter Tuning in Logistic Regression.**\n",
        "\n",
        "What is Hyperparameter Tuning in Logistic Regression? Hyperparameter tuning refers to the process of selecting the best combination of hyperparameters that will result in the optimal performance of a machine learning model. In the context of Logistic Regression, hyperparameters are parameters that are set before the learning process begins, and they control how the model is trained. Unlike model parameters (such as weights and intercepts), hyperparameters cannot be learned directly from the data and must be manually chosen or optimized.\n",
        "\n",
        "Key Hyperparameters in Logistic Regression: Regularization Strength (C)\n",
        "\n",
        "Description: The C parameter controls the regularization strength in logistic regression. Regularization helps prevent overfitting by penalizing large coefficients (weights). Effect: A small value of C indicates strong regularization (more penalty on the weights), while a larger value of C indicates weaker regularization (less penalty). Interpretation: High C: The model is more likely to overfit as it places less emphasis on regularization. Low C: The model is more likely to underfit as it heavily penalizes large coefficients. Typical Range: A typical value for C ranges from 0.001 to 1000. Regularization Type (penalty)\n",
        "\n",
        "Description: Logistic Regression supports different types of regularization. The two most common are: L1 Regularization (Lasso): Encourages sparsity in the model, driving some coefficients to zero. L2 Regularization (Ridge): Penalizes large coefficients but does not shrink them to zero. Effect: The choice of regularization type affects the model's complexity and the resulting coefficients. Common values for penalty: l1, l2, elasticnet (combination of L1 and L2), or none (no regularization). Typical use case: L1 (Lasso): When you want to produce a sparse model (few non-zero coefficients). L2 (Ridge): When you want to penalize large weights without removing features entirely. ElasticNet: When you need a mix of both L1 and L2 regularization. Solver\n",
        "\n",
        "Description: The solver determines how the optimization problem is solved. Logistic Regression uses different optimization algorithms to find the best-fitting model. Common solvers in Scikit-learn: 'liblinear': Uses a coordinate descent algorithm and works well for small datasets. 'newton-cg', 'lbfgs', 'saga': These solvers use more advanced optimization techniques (Newton’s method, Limited-memory BFGS, Stochastic Average Gradient). 'saga': Supports both L1 and L2 regularization and is suitable for large datasets. Effect: The choice of solver can affect the speed and convergence of the optimization process. Typical use case: Use 'liblinear' for small datasets and 'saga' for large datasets. Max Iterations (max_iter)\n",
        "\n",
        "Description: The max_iter hyperparameter specifies the maximum number of iterations the optimization algorithm is allowed to run before stopping. Effect: If the solver cannot converge before reaching the maximum number of iterations, the model may not be fully optimized. Typical values: The default is usually 100, but you can increase it (e.g., to 500 or 1000) if the solver is having difficulty converging. Tolerance (tol)\n",
        "\n",
        "Description: The tol (tolerance) hyperparameter determines the stopping criteria for the optimization process. It specifies the minimum change in the cost function between iterations required to continue the optimization process. Effect: A smaller tol will make the solver more precise, but it may also increase computation time. A larger tol might speed up convergence but could lead to less accurate results. Typical values: Common values are around 1e-4 to 1e-6. Intercept Scaling (intercept_scaling)\n",
        "\n",
        "Description: The intercept_scaling parameter is used when the solver is set to 'liblinear'. It controls the scaling of the intercept term (bias) in the optimization process. Effect: This is only relevant when using the ‘liblinear’ solver. Increasing the scaling can help the optimization algorithm converge better in certain cases. Why is Hyperparameter Tuning Important in Logistic Regression? Improves Model Performance: Tuning hyperparameters allows you to find the optimal balance between bias and variance, ensuring that the model neither overfits nor underfits the data. Prevents Overfitting and Underfitting: Properly selecting the regularization strength (C) and type (e.g., L1 or L2) helps avoid both overfitting (model is too complex) and underfitting (model is too simple). Optimizes Computation: Choosing the right solver and iteration parameters can improve computational efficiency, especially for large datasets. Improves Convergence: Proper tuning of parameters like max_iter and tol ensures that the optimization process converges effectively and within a reasonable time frame. Methods for Hyperparameter Tuning\n",
        "\n",
        "Grid Search Description: Grid Search is an exhaustive search over a specified parameter grid. It trains a model for every combination of hyperparameters from the given grid and selects the combination that gives the best model performance. Advantages: Comprehensive but computationally expensive.\n",
        "from sklearn.model_selection import GridSearchCV param_grid = { 'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l1', 'l2'], 'solver': ['liblinear'], 'max_iter': [100, 200, 300] } grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5) grid_search.fit(X_train, y_train) print(grid_search.best_params_) 2. Randomized Search Description: Randomized Search performs a random search over a specified hyperparameter space. Instead of testing every combination, it samples a subset of the hyperparameter space, which can be faster than Grid Search, especially for large hyperparameter spaces. Advantages: Faster than Grid Search and can still find optimal parameters.\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV from scipy.stats import uniform param_distributions = { 'C': uniform(0.001, 10), 'penalty': ['l1', 'l2'], 'solver': ['liblinear'], 'max_iter': [100, 200, 300] } random_search = RandomizedSearchCV(LogisticRegression(), param_distributions, n_iter=10, cv=5) random_search.fit(X_train, y_train) print(random_search.best_params_) 3. Bayesian Optimization Description: Bayesian Optimization is a probabilistic model-based optimization technique. It builds a surrogate model to predict which hyperparameters might yield the best results and iteratively refines this model based on observed outcomes. Advantages: More efficient than Grid and Random Search for complex hyperparameter spaces. How to Use: You can use libraries like Hyperopt, Optuna, or Spearmint for Bayesian optimization. Conclusion Hyperparameter tuning in Logistic Regression involves selecting the optimal values for parameters like regularization strength (C), regularization type, solver, max_iter, and others. Proper tuning can significantly."
      ],
      "metadata": {
        "id": "7cVwgltUvvgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**14 What are different solvers in Logistic Regression? Which one should be used.**\n",
        "\n",
        "Different Solvers in Logistic Regression In Logistic Regression, the solver is the optimization algorithm used to minimize the cost function (usually log-loss or cross-entropy) and find the best set of weights. The scikit-learn library supports several solvers, each with different characteristics, advantages, and computational efficiencies. Here are the most commonly used solvers in Logistic Regression:\n",
        "\n",
        "'liblinear' Description:\n",
        "'liblinear' is a coordinate descent algorithm and works well for small datasets. It is efficient for problems where the number of features is not excessively large. It supports both L1 (Lasso) and L2 (Ridge) regularization. It uses a limited-memory BFGS (quasi-Newton) method for optimization in the case of L2 regularization. Advantages:\n",
        "\n",
        "Works well with small to medium-sized datasets. Supports both L1 and L2 regularization. Can be used with sparse datasets (datasets with many zero values). Disadvantages:\n",
        "\n",
        "Can be slow for large datasets with a large number of features. When to Use:\n",
        "\n",
        "Use 'liblinear' when you have a smaller dataset or when you need to use L1 regularization (Lasso). Example:\n",
        "\n",
        "python Copy Edit from sklearn.linear_model import LogisticRegression model = LogisticRegression(solver='liblinear') model.fit(X_train, y_train) 2. 'newton-cg' Description:\n",
        "\n",
        "The Newton's method (or Newton-CG) is an optimization algorithm that uses second-order information (the Hessian matrix) to update the coefficients. It’s generally faster and more accurate for large datasets compared to gradient descent-based methods. This solver supports only L2 regularization. Advantages:\n",
        "\n",
        "Faster convergence for large datasets with many features. Can handle multinomial logistic regression (multi-class classification). Disadvantages:\n",
        "\n",
        "Requires more memory and computational resources compared to solvers like 'liblinear'. Supports only L2 regularization. When to Use:\n",
        "\n",
        "Use 'newton-cg' when you have large datasets and need faster convergence or when you are working with multi-class classification. Example:\n",
        "\n",
        "python Copy Edit model = LogisticRegression(solver='newton-cg') model.fit(X_train, y_train) 3. 'lbfgs' Description:\n",
        "\n",
        "Limited-memory BFGS (LBFGS) is an approximation of Newton's method that uses limited memory to reduce computational complexity. It's effective for large datasets with many features. This solver supports only L2 regularization and works well with dense datasets. Advantages:\n",
        "\n",
        "Efficient for large datasets. Can handle multi-class logistic regression problems. Faster convergence compared to gradient descent methods for large datasets. Disadvantages:\n",
        "\n",
        "It doesn't support L1 regularization. Like 'newton-cg', it requires more memory than simpler solvers. When to Use:\n",
        "\n",
        "Use 'lbfgs' when you have large datasets and require fast convergence or when you're dealing with multi-class problems. Example:\n",
        "\n",
        "python Copy Edit model = LogisticRegression(solver='lbfgs') model.fit(X_train, y_train) 4. 'saga' Description:\n",
        "\n",
        "The SAGA (Stochastic Average Gradient) algorithm is a variant of the stochastic gradient descent (SGD) method. It is particularly suited for large datasets and supports both L1 and L2 regularization. It also supports multi-class classification through a one-vs-rest scheme and can work well with sparse data. Advantages:\n",
        "\n",
        "Supports both L1 and L2 regularization (ElasticNet). Works efficiently with large datasets and sparse datasets. Faster convergence for large datasets compared to traditional gradient descent methods. Multi-class support via one-vs-rest classification. Disadvantages:\n",
        "\n",
        "More computationally expensive compared to 'liblinear' and slower for smaller datasets. When to Use:\n",
        "\n",
        "Use 'saga' when you're working with large datasets, need ElasticNet regularization (a mix of L1 and L2), or have sparse data. Example:\n",
        "\n",
        "python Copy Edit model = LogisticRegression(solver='saga') model.fit(X_train, y_train) 5. 'liblinear' vs 'saga' for L1 Regularization Important Note: If you need L1 regularization, the 'liblinear' solver is usually the best choice. The 'saga' solver also supports L1 regularization and is suitable for large datasets or when you require both L1 and L2 regularization (ElasticNet). When to Use Each Solver: Solver Regularization Best For Use Case 'liblinear' L1, L2 Small datasets, sparse data Good for small datasets and L1 regularization (Lasso) 'newton-cg' L2 Large datasets Fast convergence for large datasets with L2 regularization 'lbfgs' L2 Large datasets Efficient for large datasets and multi-class classification 'saga' L1, L2 (ElasticNet) Large datasets, sparse data Suitable for large datasets and ElasticNet regularization"
      ],
      "metadata": {
        "id": "jwSBuqidwLBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**15 How is Logistic Regression extended for multiclass classification.**\n",
        "\n",
        "How Logistic Regression is Extended for Multiclass Classification Logistic Regression is fundamentally a binary classification algorithm, meaning it predicts two possible outcomes (e.g., 0 or 1). However, it can be extended to handle multiclass classification (where the target variable has more than two classes) using specific strategies. The two most common methods for extending Logistic Regression to multiclass problems are:\n",
        "\n",
        "One-vs-Rest (OvR) or One-vs-All (OvA) Multinomial Logistic Regression (Softmax Regression) Let’s explore these two approaches in detail.\n",
        "\n",
        "One-vs-Rest (OvR) / One-vs-All (OvA) In the One-vs-Rest (OvR) or One-vs-All (OvA) approach, the idea is to treat the multiclass classification problem as multiple binary classification tasks. Specifically, it builds a separate binary classifier for each class, where the classifier is trained to distinguish that class from all the other classes.\n",
        "How It Works: For K classes in the dataset, you create K separate binary classifiers. For each classifier, the data for the current class is labeled as 1, and all other classes are labeled as 0. During prediction, each classifier predicts the probability of its class, and the class with the highest probability is chosen as the predicted class. Steps: Train K classifiers, one for each class, where each classifier is trained to predict \"is this sample from class i or not?\" When making a prediction, each classifier computes a probability for its class (using logistic regression). The class with the highest predicted probability is chosen as the final prediction. Advantages: Simple to implement and works well when the number of classes is not very large. Can be used with any binary classification algorithm, and is supported by most machine learning libraries. Disadvantages: If the classes are imbalanced, OvR can lead to biased predictions toward the larger classes. Computationally expensive for large numbers of classes, as you need to train multiple classifiers. When to Use: Good for problems with many classes, especially when you don't want to modify the logistic regression model too much. It's also effective when the classes are not mutually exclusive (i.e., one sample may belong to multiple classes). Example: In scikit-learn, the default behavior of LogisticRegression() is One-vs-Rest when you use the 'liblinear', 'lbfgs', or 'saga' solvers for multiclass classification.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression model = LogisticRegression(multi_class='ovr') # Default is 'ovr' model.fit(X_train, y_train) predictions = model.predict(X_test) 2. Multinomial Logistic Regression (Softmax Regression) Multinomial Logistic Regression (also known as Softmax Regression) directly generalizes Logistic Regression for multiclass problems by modeling the relationship between the features and all the classes simultaneously.\n",
        "\n",
        "How It Works: Instead of using a binary decision function for each class (like in OvR), Multinomial Logistic Regression uses the softmax function to compute probabilities for all classes at once. The softmax function ensures that the predicted probabilities for all classes sum to 1, which makes them interpretable as probabilities for each class. The softmax function is an extension of the sigmoid function. It converts raw prediction scores (logits) from the model into probability values for each class.\n",
        "\n",
        "The mathematical form for multinomial logistic regression for a given input X is:\n",
        "\n",
        "𝑃 ( 𝑦\n",
        "𝑘 ∣ 𝑋 )\n",
        "𝑒 𝛽 𝑘 𝑋 ∑ 𝑗\n",
        "1 𝐾 𝑒 𝛽 𝑗 𝑋 P(y=k∣X)= ∑ j=1 K​e β j​X\n",
        "\n",
        "e β k​X\n",
        "\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝛽 𝑘 β k​is the coefficient for class 𝑘 k, 𝑋 X is the feature vector, 𝐾 K is the total number of classes. Steps: Train a single model with K classes using the softmax function to directly model probabilities for each class. The model learns a separate weight vector for each class and computes the class probabilities based on these weights. During prediction, the class with the highest probability is chosen as the predicted class. Advantages: Single classifier: Unlike OvR, where you need to train multiple classifiers, multinomial logistic regression involves just one model. Efficient: Computationally more efficient than OvR for problems with many classes because only one model is trained. Disadvantages: Requires careful handling of class imbalance, as softmax may still be sensitive to this issue. More complex than OvR and harder to explain when compared to multiple binary classifiers. When to Use: Ideal when classes are mutually exclusive and you want to model the full distribution of classes at once. Works better when you have a large number of classes and prefer one classifier instead of multiple classifiers. Example: In scikit-learn, you can use multinomial logistic regression by setting the multi_class='multinomial' parameter:\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression model = LogisticRegression(multi_class='multinomial', solver='lbfgs') model.fit(X_train, y_train) predictions = model.predict(X_test) Comparison Between OvR and Multinomial Logistic Regression Feature One-vs-Rest (OvR) Multinomial Logistic Regression Number of classifiers K separate classifiers One classifier for all classes Regularization Regularization applied to each classifier separately Regularization applied jointly to all classes Prediction The class with the highest probability from K classifiers The class with the highest softmax probability Computational Complexity More computationally expensive (K classifiers) More efficient (one classifier) Handling of classes Each class is treated independently Treats classes simultaneously, using softmax Best for Small to medium-sized datasets with many classes Large datasets with mutually exclusive classes Which Approach to Choose? Use One-vs-Rest (OvR) when: You have a relatively small number of classes. You want simplicity and interpretability. You may want to apply L1 regularization (Lasso) or need sparse classifiers. Use Multinomial Logistic Regression (Softmax) when: You have a larger number of classes. You want to optimize everything in one step with a single classifier. You want the model to better capture the interdependencies between classes."
      ],
      "metadata": {
        "id": "Le3Uj0O8wTRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**16 What are the advantages and disadvantages of Logistic Regression.**\n",
        "\n",
        "Advantages and Disadvantages of Logistic Regression Logistic Regression is one of the most widely used algorithms for binary and multiclass classification tasks. Like any machine learning model, it has its pros and cons, which make it suitable for certain scenarios and less effective in others. Here’s an overview of the advantages and disadvantages:\n",
        "\n",
        "Advantages of Logistic Regression:\n",
        "\n",
        "Simplicity and Interpretability: Easy to implement: Logistic Regression is simple to understand and easy to implement. Interpretability: The coefficients of a logistic regression model indicate the importance of each feature in the decision-making process. This makes it easy to interpret the results, especially in fields like healthcare, finance, and business.\n",
        "Efficient for Linearly Separable Data: Logistic Regression works well when the data is linearly separable or nearly linearly separable. It can achieve high accuracy in these cases with minimal computation.\n",
        "Probability Output: Logistic Regression provides probability scores as outputs, which is valuable for decision-making processes that require probabilities (e.g., risk prediction). The sigmoid function ensures that the output is between 0 and 1, making it useful when you need to assess the likelihood of a particular class.\n",
        "Performs Well with Small Datasets: Logistic Regression is effective for smaller datasets where you don’t have enough data to train more complex models. It avoids the risk of overfitting compared to more complex algorithms like deep learning, especially when using regularization.\n",
        "Regularization: Logistic Regression can be regularized using L1 (Lasso) or L2 (Ridge) regularization to avoid overfitting and improve model generalization, particularly with high-dimensional data. This flexibility with regularization makes it more adaptable to different datasets.\n",
        "Computationally Efficient: Compared to more complex models like decision trees, random forests, or neural networks, Logistic Regression is computationally efficient, especially for binary classification tasks. It requires fewer resources (in terms of memory and CPU) and is fast to train, even with large datasets.\n",
        "Works Well with Linearly-Related Features: Logistic Regression performs well when the features are linearly related to the log-odds of the target variable. It’s a good choice when this assumption holds true.\n",
        "Extensibility to Multiclass Classification: Logistic Regression can be extended to handle multiclass classification problems using techniques like One-vs-Rest (OvR) and Multinomial Logistic Regression (Softmax Regression). Disadvantages of Logistic Regression:\n",
        "Assumes Linear Relationships: Logistic Regression assumes a linear relationship between the features and the log-odds of the target. If the data is not linearly separable or there are complex interactions between features, logistic regression may perform poorly compared to more advanced algorithms like Random Forests or Neural Networks.\n",
        "Poor Performance with Complex or Non-Linear Decision Boundaries: If the data requires non-linear decision boundaries (e.g., in the case of complex interactions between features), Logistic Regression will struggle. For example, it can fail to separate classes effectively in scenarios where the data is highly non-linear.\n",
        "Sensitive to Outliers: Logistic Regression can be sensitive to outliers or extreme values in the dataset. Outliers can disproportionately influence the model's performance and lead to inaccurate predictions.\n",
        "Requires Feature Engineering: Feature scaling and preprocessing are critical in Logistic Regression, especially when using regularization. Features need to be scaled appropriately (e.g., using standardization or normalization) to ensure that the model treats them correctly. Additionally, non-linear relationships between features may need to be captured manually through feature engineering (e.g., polynomial features).\n",
        "Limited to Binary and Multiclass Classification: Logistic Regression is specifically designed for classification tasks. It is not suitable for regression tasks, and it doesn’t handle continuous output as in the case of linear regression. While extensions like multinomial logistic regression handle multiclass problems, the algorithm might not be ideal for complex multiclass scenarios with highly imbalanced data.\n",
        "Assumes Independence of Features (Multicollinearity Issues): Logistic Regression assumes that the features are independent of each other. In practice, multicollinearity (where two or more features are highly correlated) can lead to unreliable coefficient estimates and less accurate predictions. This issue can be mitigated by removing correlated features or using Regularization.\n",
        "Overfitting with High-Dimensional Data: If the number of features is much higher than the number of observations (e.g., in text classification or gene expression analysis), Logistic Regression can easily overfit unless regularization is used.\n",
        "Assumes a Binary Outcome for Each Prediction: In a standard logistic regression model, the target variable should be binary. For multiclass problems, methods like One-vs-Rest or Multinomial Logistic Regression are used, but these methods add extra complexity."
      ],
      "metadata": {
        "id": "5U_lhoB_wdJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**17 What are some use cases of Logistic Regression.**\n",
        "\n",
        "Use Cases of Logistic Regression Logistic Regression is widely used for classification tasks in various domains due to its simplicity, interpretability, and effectiveness. Here are some common use cases where Logistic Regression shines:\n",
        "\n",
        "Medical and Healthcare: Disease Prediction Use Case: Predicting the likelihood of a patient having a specific disease based on diagnostic test results or patient features. Example: Predicting Heart Disease: A Logistic Regression model can be used to predict the probability of a patient developing heart disease based on features like age, blood pressure, cholesterol levels, and smoking habits. Cancer Detection: Predicting the likelihood of a patient having cancer (e.g., breast cancer) based on medical test results (e.g., biopsy results, mammogram images). Why Logistic Regression? It’s simple, interpretable, and provides the probability of the disease, which can help doctors make decisions.\n",
        "Marketing and Sales: Customer Behavior Prediction Use Case: Predicting customer behavior, such as whether a customer will purchase a product, churn, or respond to a marketing campaign. Example: Customer Churn Prediction: Predicting whether a customer will leave a service (e.g., telecom company) based on their usage patterns, demographics, and customer support interactions. Email Campaign Response: Predicting whether a customer will respond to an email marketing campaign based on their past interactions. Why Logistic Regression? It's efficient, provides clear probabilities, and can easily be interpreted to understand customer decision-making patterns.\n",
        "Finance: Credit Scoring and Fraud Detection Use Case: Determining whether a loan applicant is a credit risk or detecting fraudulent transactions. Example: Credit Scoring: Using Logistic Regression to predict the likelihood that a borrower will default on a loan based on factors like credit score, income, and loan amount. Fraud Detection: Identifying whether a transaction is fraudulent or not based on transaction history, device information, and customer profile. Why Logistic Regression? It is well-suited for binary classification tasks, and the probability output helps in making risk-based decisions, especially in financial industries.\n",
        "Social Media: Sentiment Analysis Use Case: Classifying the sentiment of social media posts, reviews, or comments as positive, negative, or neutral. Example: Tweet Sentiment Analysis: Predicting whether a tweet about a product or a company is positive or negative based on the text of the tweet. Review Classification: Predicting whether an online product review is positive or negative based on customer reviews. Why Logistic Regression? It's effective for text classification tasks when the features are transformed into a format that Logistic Regression can handle (e.g., using TF-IDF or word embeddings).\n",
        "Email Classification: Spam Detection Use Case: Classifying emails as spam or non-spam. Example: Spam Email Classification: Using Logistic Regression to classify emails into spam and non-spam categories based on features like keywords, sender, and metadata of the email. Why Logistic Regression? It works well for binary classification tasks like spam detection and is computationally efficient.\n",
        "Sports: Player Performance Prediction Use Case: Predicting player performance, such as whether a player will score or whether a team will win a game. Example: Football (Soccer) Player Scoring Prediction: Predicting whether a football player will score a goal based on factors like match statistics, player form, and opposition team data. Game Outcome Prediction: Predicting the outcome of a match (win/loss) based on historical team performance, player stats, and match conditions. Why Logistic Regression? Provides a simple, interpretable model that can give probabilities for outcomes like winning or scoring.\n",
        "Human Resources: Employee Retention and Hiring Use Case: Predicting whether an employee will stay with a company or leave (employee churn) or whether a candidate will be hired based on their qualifications. Example: Employee Retention: Using Logistic Regression to predict whether an employee will stay or leave the company based on job satisfaction, work environment, salary, and performance. Hiring Prediction: Predicting whether a job candidate will be hired based on factors like qualifications, experience, interview scores, and skills. Why Logistic Regression? It’s easy to understand and can provide probability-based insights, helping HR make data-driven decisions.\n",
        "E-commerce: Purchase Decision Prediction Use Case: Predicting whether a customer will make a purchase based on their behavior on an e-commerce site. Example: Product Recommendation: Predicting whether a user will purchase a recommended product based on their browsing history, demographic data, and previous purchases. Cart Abandonment: Predicting whether a user will complete a purchase or abandon their shopping cart. Why Logistic Regression? It’s ideal for binary classification tasks and can be used to assess the probability of a purchase, helping optimize marketing strategies.\n",
        "Government and Public Services: Voting Prediction Use Case: Predicting voting outcomes or classifying whether a voter will vote for a particular candidate. Example: Election Predictions: Predicting the likelihood of a voter supporting a specific candidate based on demographic factors like age, income, education level, and political preferences. Voter Turnout Prediction: Predicting whether a person will vote in an election based on past voting behavior and other demographic features. Why Logistic Regression? Logistic Regression is ideal for binary classification (e.g., voting for a candidate or not), and its output can provide valuable insights for campaign strategies.\n",
        "Transportation: Traffic Prediction and Route Optimization Use Case: Predicting whether traffic will be congested or determining the best route for travel. Example: Traffic Congestion Prediction: Predicting whether traffic will be heavy or light based on time of day, weather conditions, and historical traffic data. Route Optimization: Predicting the likelihood of a certain route being faster based on traffic conditions, road closures, and alternative routes. Why Logistic Regression? Logistic Regression can be used to classify binary outcomes (e.g., traffic jam/no traffic jam) based on different features."
      ],
      "metadata": {
        "id": "3PRyKlvEwnx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**18 What is the difference between Softmax Regression and Logistic Regression.**\n",
        "\n",
        "Difference Between Softmax Regression and Logistic Regression Both Logistic Regression and Softmax Regression are widely used algorithms for classification tasks, but they are suited for different types of problems, and they differ in terms of how they handle the target variable and how they model the decision boundaries.\n",
        "\n",
        "Here’s a breakdown of the differences:\n",
        "\n",
        "Type of Classification Problem: Logistic Regression:\n",
        "Binary Classification: Logistic Regression is primarily used for binary classification tasks, where the goal is to predict one of two classes (e.g., 0 or 1, true or false, positive or negative). Example: Predicting whether a customer will buy a product (1) or not (0). Softmax Regression:\n",
        "\n",
        "Multiclass Classification: Softmax Regression (also known as Multinomial Logistic Regression) is an extension of Logistic Regression used for multiclass classification problems, where there are more than two possible classes (more than just 0 or 1). Example: Predicting which category a news article belongs to (e.g., politics, sports, technology). 2. Output: Logistic Regression:\n",
        "\n",
        "Single Output: Logistic Regression outputs a single probability score, which is the probability of the instance belonging to the positive class (e.g., class 1). The output is a value between 0 and 1, representing the probability of the positive class. The decision rule is typically to classify an instance as 1 if the probability is greater than or equal to 0.5, and 0 otherwise. 𝑃 ( 𝑦\n",
        "1 ∣ 𝑋 )\n",
        "1 1 + 𝑒 − ( 𝑤 𝑇 𝑋 + 𝑏 ) P(y=1∣X)= 1+e −(w T X+b)\n",
        "\n",
        "1​\n",
        "\n",
        "Softmax Regression:\n",
        "\n",
        "Multiple Outputs: Softmax Regression outputs a vector of probabilities for each possible class. The probabilities sum to 1 across all classes, and each element in the vector represents the probability of the instance belonging to a particular class. 𝑃 ( 𝑦\n",
        "𝑘 ∣ 𝑋 )\n",
        "𝑒 𝑤 𝑘 𝑇 𝑋 + 𝑏 𝑘 ∑ 𝑗\n",
        "1 𝐶 𝑒 𝑤 𝑗 𝑇 𝑋 + 𝑏 𝑗 P(y=k∣X)= ∑ j=1 C​e w j T​X+b j​\n",
        "\n",
        "e w k T​X+b k​\n",
        "\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝐶 C is the total number of classes. 𝑤 𝑘 w k​and 𝑏 𝑘 b k​are the weights and bias for class 𝑘 k. The denominator is the sum of the exponentials of the linear scores for all classes (ensuring that the probabilities sum to 1). 3. Decision Boundary: Logistic Regression: Binary Decision Boundary: Logistic Regression creates a single decision boundary to separate the two classes. This boundary is a linear decision surface in the feature space. Softmax Regression: Multiclass Decision Boundaries: Softmax Regression creates multiple decision boundaries, one for each class, and each class is assigned the label with the highest probability. This results in C-1 decision boundaries for C classes. The decision surface for Softmax Regression is generally more complex and higher-dimensional than in Logistic Regression. 4. Model Function: Logistic Regression: Logistic Regression uses a sigmoid function (also known as the logistic function) to model the probability of the positive class. It is applied to a linear combination of the input features to give a value between 0 and 1. Softmax Regression: Softmax Regression uses the softmax function to model the probabilities for multiple classes. It generalizes the sigmoid function for multiclass problems by taking a linear combination of the features for each class and normalizing them to ensure the probabilities sum to 1. 5. Loss Function: Logistic Regression:\n",
        "\n",
        "The loss function used in Logistic Regression is the binary cross-entropy (also called log loss), which compares the predicted probabilities with the true binary labels. 𝐿\n",
        "− [ 𝑦 log ⁡ ( 𝑝 ) + ( 1 − 𝑦 ) log ⁡ ( 1 − 𝑝 ) ] L=−[ylog(p)+(1−y)log(1−p)] Where 𝑝 p is the predicted probability of class 1, and 𝑦 y is the true label (0 or 1).\n",
        "\n",
        "Softmax Regression:\n",
        "\n",
        "Softmax Regression uses the categorical cross-entropy (or multinomial log loss) as its loss function, which compares the predicted probability distribution over multiple classes with the true distribution (one-hot encoded vector for the true class). 𝐿\n",
        "− ∑ 𝑘\n",
        "1 𝐶 𝑦 𝑘 log ⁡ ( 𝑝 𝑘 ) L=− k=1 ∑ C​y k​log(p k​) Where 𝑦 𝑘 y k​is the true class indicator (1 if the instance belongs to class 𝑘 k, otherwise 0), and 𝑝 𝑘 p k​is the predicted probability for class 𝑘 k.\n",
        "\n",
        "Training Process: Logistic Regression:\n",
        "For binary classification, Logistic Regression only needs to estimate a single set of weights (one weight vector 𝑤 w and one bias 𝑏 b) for the model. It is trained by minimizing the binary cross-entropy loss. Softmax Regression:\n",
        "\n",
        "In Softmax Regression, we need to estimate a set of weights for each class. For 𝐶 C classes, there are 𝐶 C weight vectors and biases. The model is trained by minimizing the categorical cross-entropy loss function over all classes. 7. Number of Classes Handled: Logistic Regression:\n",
        "\n",
        "Binary classification: It is designed for problems where the target variable has two possible outcomes. Softmax Regression:\n",
        "\n",
        "Multiclass classification: It can handle problems with more than two possible outcomes (i.e., multiple classes). It is used when there are multiple categories to classify into. Summary of Key Differences: Feature Logistic Regression Softmax Regression Type of Classification Binary classification (2 classes) Multiclass classification (more than 2 classes) Output Single probability for one class (0 or 1) Multiple probabilities for each class (sums to 1) Decision Boundary One decision boundary Multiple decision boundaries (one for each class) Model Function Sigmoid function (for 2 classes) Softmax function (generalized sigmoid for multiple classes) Loss Function Binary cross-entropy loss Categorical cross-entropy loss Number of Classes 2 (binary classification) C classes (multiclass classification) Training Process Estimate weights for 2 classes Estimate weights for C classes Which to Use? Logistic Regression is the choice for binary classification tasks (e.g., spam vs. non-spam, healthy vs. sick). Softmax Regression (or Multinomial Logistic Regression) should be used when you have more than two classes (e.g., categorizing images into multiple categories or predicting the type of fruit). In essence, Softmax Regression is a generalization of Logistic Regression for multiclass problems, and when dealing with more than two classes, Softmax Regression is the preferred approach."
      ],
      "metadata": {
        "id": "UCW9I97rwt5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**19 How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification.**\n",
        "\n",
        "When dealing with multiclass classification, choosing between One-vs-Rest (OvR) (also called One-vs-All (OvA)) and Softmax Regression depends on factors such as model complexity, computational efficiency, performance, and the specific nature of the problem. Below is a comparison of both methods and guidelines on how to choose between them:\n",
        "\n",
        "One-vs-Rest (OvR) (One-vs-All) How it Works: In the One-vs-Rest approach, the problem is broken down into multiple binary classification problems. For each class, a binary classifier is trained to distinguish that class from all the other classes. For example, if there are 𝐶 C classes, 𝐶 C binary classifiers are trained. Each classifier predicts whether the instance belongs to its specific class or not (e.g., \"is class 1?\" vs \"is not class 1?\"). During prediction, each classifier outputs a probability, and the class with the highest probability is selected as the predicted class. Advantages of OvR: Simple: The OvR approach is simple and allows you to use any binary classification algorithm (e.g., Logistic Regression, SVM) for each class. Flexibility: It can work well with models that are naturally suited for binary classification (e.g., Logistic Regression, SVM). Efficient for Imbalanced Data: If your data is highly imbalanced across classes, OvR can sometimes perform better since each classifier only has to distinguish a specific class from the others. Disadvantages of OvR: Training Overhead: It requires 𝐶 C separate classifiers to be trained (one for each class), which can be computationally expensive for large 𝐶 C. Non-interaction Between Classes: OvR treats each class independently, so it doesn’t consider the relationships or interactions between the classes. Class Imbalance Issues: In imbalanced datasets, each classifier might be biased toward the majority class.\n",
        "Softmax Regression (Multinomial Logistic Regression) How it Works: In Softmax Regression, a single model is trained that outputs a probability distribution over all classes. The model computes a vector of scores for each class, and then the Softmax function is applied to convert these scores into probabilities. The class with the highest probability is selected as the predicted class. The model is trained using categorical cross-entropy as the loss function, which directly handles multiclass classification. Advantages of Softmax Regression: Unified Model: Only a single model is trained, which is computationally more efficient compared to OvR when dealing with a large number of classes. Probabilistic Interpretation: Softmax Regression gives a probability distribution over all classes, which provides more meaningful predictions, especially in scenarios where you want to know the likelihood of each class. Captures Inter-Class Relationships: Softmax Regression directly models the relationships between classes, making it more appropriate when classes are mutually exclusive and there are interdependencies between them. Scalability: Since it trains just one model, it is more scalable and easier to maintain when there are many classes. Disadvantages of Softmax Regression: Complexity: The Softmax function requires the computation of the exponential of the scores for each class, which can become computationally expensive as the number of classes increases. Potential for Poor Performance with Imbalanced Data: If your data is highly imbalanced, the Softmax model might be biased toward the majority class because the loss function treats all classes equally, without regard for class distribution. Less Flexible: Unlike OvR, Softmax requires a direct multiclass formulation, which might not work well with certain models that are specifically designed for binary classification. When to Use OvR vs. Softmax: Use OvR When: Binary classifiers are preferred: If you already have a binary classifier (like Logistic Regression, SVM) that works well, OvR allows you to leverage that model for multiclass classification. You have imbalanced data: OvR might perform better in situations where there is a significant class imbalance, as each classifier focuses on distinguishing a particular class from the others, which could be less affected by imbalanced data. You want flexibility: If you want to use different types of binary classifiers for each class (e.g., using Logistic Regression for some classes and SVM for others), OvR gives you that flexibility. Computational Cost of Softmax is High: If the number of classes is very large, and the Softmax computation is too slow or resource-intensive, OvR might be more computationally feasible because you only train binary classifiers. Use Softmax Regression When: Classes are mutually exclusive and dependent: If the classes are mutually exclusive (only one class is correct per instance), Softmax is a natural fit because it models the entire classification problem in one model and handles the relationships between classes directly. You want a single unified model: If you prefer to have just one model rather than multiple separate models for each class, Softmax Regression is more efficient, both in terms of memory and training time. Interpretability of probabilities is important: Softmax gives you a probability distribution over the classes, which can be valuable for applications where you need the model to tell you the likelihood of each class, not just the final prediction. Scalability is a priority: If your problem has a large number of classes and you want to avoid training separate classifiers for each, Softmax allows you to scale more easily with a single model."
      ],
      "metadata": {
        "id": "T7Inosv_w3kQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**20. How do we interpret coefficients in Logistic Regression?**\n",
        "\n",
        "Interpreting Coefficients in Logistic Regression In Logistic Regression, the coefficients ( 𝛽 β) represent the relationship between the predictor variables (features) and the log-odds of the dependent variable (target). The interpretation of these coefficients differs from that in Linear Regression because Logistic Regression models the probability of a binary outcome, not a continuous one.\n",
        "\n",
        "Here’s how to interpret the coefficients in a Logistic Regression model:\n",
        "\n",
        "Logistic Regression Model: The model predicts the probability of a binary outcome 𝑦\n",
        "1 y=1 given a set of features 𝑋\n",
        "( 𝑥 1 , 𝑥 2 , … , 𝑥 𝑛 ) X=(x 1​,x 2​,…,x n​). The model is usually written as:\n",
        "log ⁡ ( 𝑃 ( 𝑦\n",
        "1 ∣ 𝑋 ) 1 − 𝑃 ( 𝑦\n",
        "1 ∣ 𝑋 ) )\n",
        "𝛽 0 + 𝛽 1 𝑥 1 + 𝛽 2 𝑥 2 + ⋯ + 𝛽 𝑛 𝑥 𝑛 log( 1−P(y=1∣X) P(y=1∣X)​)=β 0​+β 1​x 1​+β 2​x 2​+⋯+β n​x n​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑃 ( 𝑦\n",
        "1 ∣ 𝑋 ) P(y=1∣X) is the probability of the positive class (class 1). 𝑃 ( 𝑦\n",
        "1 ∣ 𝑋 ) 1 − 𝑃 ( 𝑦\n",
        "1 ∣ 𝑋 ) 1−P(y=1∣X) P(y=1∣X)​is the odds of the positive class. The left-hand side is the log-odds (logarithm of the odds). 𝛽 0 β 0​is the intercept (bias). 𝛽 1 , 𝛽 2 , … , 𝛽 𝑛 β 1​,β 2​,…,β n​are the coefficients associated with the features 𝑥 1 , 𝑥 2 , … , 𝑥 𝑛 x 1​,x 2​,…,x n​. 2. Interpreting the Coefficients: The key to interpreting the coefficients in Logistic Regression is to remember that they represent the log-odds of the target variable, meaning how the logarithm of the odds of the outcome changes with a unit change in the corresponding predictor variable.\n",
        "\n",
        "Log-Odds Interpretation: Log-Odds: A one-unit increase in 𝑥 𝑖 x i​(the 𝑖 i-th feature) will change the log-odds of the target variable by 𝛽 𝑖 β i​. Example: If 𝛽 1\n",
        "0.5 β 1​=0.5, it means that for every one-unit increase in 𝑥 1 x 1​, the log-odds of 𝑦\n",
        "1 y=1 will increase by 0.5. Odds Ratio (Exponential of Coefficients): To make the interpretation more intuitive, we often take the exponent of the coefficient to convert it into an odds ratio. The odds ratio tells us how the odds of the event occurring change with a one-unit increase in the predictor variable.\n",
        "\n",
        "Odds Ratio: 𝑒 𝛽 𝑖 e β i​\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "If 𝑒 𝛽 𝑖\n",
        "1 e β i​\n",
        "\n",
        "=1, the feature 𝑥 𝑖 x i​has no effect on the odds of the outcome. If 𝑒 𝛽 𝑖\n",
        "\n",
        "1 e β i​\n",
        "\n",
        "1, a one-unit increase in 𝑥 𝑖 x i​increases the odds of the outcome 𝑦\n",
        "1 y=1. If 𝑒 𝛽 𝑖 < 1 e β i​\n",
        "\n",
        "<1, a one-unit increase in 𝑥 𝑖 x i​decreases the odds of the outcome 𝑦\n",
        "1 y=1. Example Interpretation of Coefficients: Assume you have a logistic regression model with the following output:\n",
        "\n",
        "log ⁡ ( 𝑃 ( 𝑦\n",
        "1 ∣ 𝑋 ) 1 − 𝑃 ( 𝑦\n",
        "1 ∣ 𝑋 ) )\n",
        "− 2 + 0.7 𝑥 1 − 1.5 𝑥 2 log( 1−P(y=1∣X) P(y=1∣X)​)=−2+0.7x 1​−1.5x 2​\n",
        "\n",
        "Intercept: 𝛽 0\n",
        "− 2 β 0​=−2\n",
        "\n",
        "The intercept represents the log-odds of the outcome when all features are zero. If 𝑥 1\n",
        "0 x 1​=0 and 𝑥 2\n",
        "0 x 2​=0, the log-odds of the outcome 𝑦\n",
        "1 y=1 are -2. You can convert this log-odds to a probability using the sigmoid function. Coefficient 𝛽 1\n",
        "0.7 β 1​=0.7 (associated with 𝑥 1 x 1​):\n",
        "\n",
        "For every one-unit increase in 𝑥 1 x 1​, the log-odds of 𝑦\n",
        "1 y=1 increase by 0.7. The odds ratio is 𝑒 0.7 ≈ 2.01 e 0.7 ≈2.01. This means that for every one-unit increase in 𝑥 1 x 1​, the odds of 𝑦\n",
        "1 y=1 double (increase by a factor of 2.01). Coefficient 𝛽 2\n",
        "− 1.5 β 2​=−1.5 (associated with 𝑥 2 x 2​):\n",
        "\n",
        "For every one-unit increase in 𝑥 2 x 2​, the log-odds of 𝑦\n",
        "1 y=1 decrease by 1.5. The odds ratio is 𝑒 − 1.5 ≈ 0.22 e −1.5 ≈0.22. This means that for every one-unit increase in 𝑥 2 x 2​, the odds of 𝑦\n",
        "1 y=1 decrease by a factor of approximately 0.22. 3. Interpreting the Model’s Predictions: In Logistic Regression, the model predicts the probability 𝑃 ( 𝑦\n",
        "1 ∣ 𝑋 ) P(y=1∣X), which is the probability of the target being 1 (the positive class). The equation for the probability is:\n",
        "\n",
        "𝑃 ( 𝑦\n",
        "1 ∣ 𝑋 )\n",
        "1 1 + 𝑒 − ( 𝛽 0 + 𝛽 1 𝑥 1 + ⋯ + 𝛽 𝑛 𝑥 𝑛 ) P(y=1∣X)= 1+e −(β 0​+β 1​x 1​+⋯+β n​x n​)\n",
        "\n",
        "1​\n",
        "\n",
        "The predicted probability represents the likelihood that the event 𝑦\n",
        "1 y=1 will occur, given the feature values. The predicted class is typically decided using a threshold of 0.5: if 𝑃 ( 𝑦\n",
        "1 ∣ 𝑋 ) ≥ 0.5 P(y=1∣X)≥0.5, the predicted class is 1; otherwise, it is 0. Example: Given the model equation:\n",
        "\n",
        "𝑃 ( 𝑦\n",
        "1 ∣ 𝑋 )\n",
        "1 1 + 𝑒 − ( − 2 + 0.7 𝑥 1 − 1.5 𝑥 2 ) P(y=1∣X)= 1+e −(−2+0.7x 1​−1.5x 2​)\n",
        "\n",
        "1​\n",
        "\n",
        "For 𝑥 1\n",
        "2 x 1​=2 and 𝑥 2\n",
        "1 x 2​=1, the predicted probability 𝑃 ( 𝑦\n",
        "1 ∣ 𝑋 ) P(y=1∣X) is calculated as:\n",
        "\n",
        "𝑃 ( 𝑦\n",
        "1 ∣ 𝑋 )\n",
        "1 1 + 𝑒 − ( − 2 + 0.7 × 2 − 1.5 × 1 ) P(y=1∣X)= 1+e −(−2+0.7×2−1.5×1)\n",
        "\n",
        "1​\n",
        "\n",
        "Now calculate this to get the probability."
      ],
      "metadata": {
        "id": "SR0smvEGxBTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "# Regression, and prints the model accuracyC\n",
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "\n",
        "# Convert the dataset into a DataFrame for easier handling\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Features (X) and target (y)\n",
        "X = df[data.feature_names]  # Input features\n",
        "y = df['target']  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Model Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "id": "eUGk40FFXhNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tI1rivdmWUA-"
      },
      "outputs": [],
      "source": [
        "#  2 Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "# and print the model accuracyC\n",
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "\n",
        "# Convert the dataset into a DataFrame for easier handling\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Features (X) and target (y)\n",
        "X = df[data.feature_names]  # Input features\n",
        "y = df['target']  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model with L1 regularization (Lasso)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Model Accuracy with L1 Regularization (Lasso): {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "# LogisticRegression(penalty='l2'). Print model accuracy and coefficientsC\n",
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "\n",
        "# Convert the dataset into a DataFrame for easier handling\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Features (X) and target (y)\n",
        "X = df[data.feature_names]  # Input features\n",
        "y = df['target']  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model with L1 regularization (Lasso)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Model Accuracy with L1 Regularization (Lasso): {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "id": "uooWpOnoxUd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "\n",
        "# Convert the dataset into a DataFrame for easier handling\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Features (X) and target (y)\n",
        "X = df[data.feature_names]  # Input features\n",
        "y = df['target']  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model with Elastic Net regularization\n",
        "# The l1_ratio parameter controls the mix between L1 (Lasso) and L2 (Ridge) regularization.\n",
        "# l1_ratio = 0.5 means equal mixture of L1 and L2.\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Model Accuracy with Elastic Net Regularization: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print the model coefficients\n",
        "print(\"\\nModel Coefficients:\")\n",
        "print(model.coef_)"
      ],
      "metadata": {
        "id": "rW8-WIR2xXfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "# multi_class='ovr'\n",
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "\n",
        "# Convert the dataset into a DataFrame for easier handling\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Features (X) and target (y)\n",
        "X = df[data.feature_names]  # Input features\n",
        "y = df['target']  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model with One-vs-Rest approach for multiclass classification\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Model Accuracy with One-vs-Rest Multiclass Classification: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print the model coefficients\n",
        "print(\"\\nModel Coefficients:\")\n",
        "print(model.coef_)"
      ],
      "metadata": {
        "id": "IKZ-_1iqxal0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "# Regression. Print the best parameters and accuracyC\n",
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "\n",
        "# Convert the dataset into a DataFrame for easier handling\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Features (X) and target (y)\n",
        "X = df[data.feature_names]  # Input features\n",
        "y = df['target']  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Define the parameter grid to search over\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],       # Regularization strength\n",
        "    'penalty': ['l1', 'l2']             # Types of regularization\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "\n",
        "# Train the model with GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters from GridSearchCV\n",
        "print(f'Best Parameters: {grid_search.best_params_}')\n",
        "\n",
        "# Get the best model from GridSearchCV\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the best model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Best Model Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "id": "AxSIwPDvxdzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7 Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "# average accuracyC\n",
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "\n",
        "# Convert the dataset into a DataFrame for easier handling\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Features (X) and target (y)\n",
        "X = df[data.feature_names]  # Input features\n",
        "y = df['target']  # Target labels\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Define the StratifiedKFold cross-validator\n",
        "# We use 5 splits in this example, but you can change it based on your needs\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform Stratified K-Fold Cross-Validation and calculate accuracy\n",
        "accuracies = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Print the accuracy for each fold\n",
        "print(f'Accuracies for each fold: {accuracies}')\n",
        "\n",
        "# Print the average accuracy\n",
        "average_accuracy = np.mean(accuracies)\n",
        "print(f'Average Accuracy: {average_accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "id": "Y10GijGhxhUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8 Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "# accuracy.\n",
        "# Importing necessary libraries\n",
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset from a CSV file\n",
        "# Replace 'your_dataset.csv' with the path to your dataset\n",
        "df = pd.read_csv('titanic .csv')\n",
        "\n",
        "# Display the first few rows of the dataset to understand its structure\n",
        "print(df.head())\n",
        "\n",
        "# Assuming the last column is the target variable and the rest are features\n",
        "X = df.iloc[:, :-1]  # Features (all columns except the last one)\n",
        "y = df.iloc[:, -1]   # Target variable (the last column)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Model Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "id": "ZSkoJLK8xkRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9 Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "# Logistic Regression. Print the best parameters and accuracyM\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "\n",
        "# Convert the dataset into a DataFrame for easier handling\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Features (X) and target (y)\n",
        "X = df[data.feature_names]  # Input features\n",
        "y = df['target']  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Define the parameter distribution for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'C': np.logspace(-4, 4, 20),           # Regularization strength\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'], # Types of regularization\n",
        "    'solver': ['lbfgs', 'liblinear', 'saga']  # Solvers for optimization\n",
        "}\n",
        "\n",
        "# Set up the RandomizedSearchCV with 5-fold cross-validation\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist,\n",
        "                                   n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Perform the RandomizedSearchCV to find the best hyperparameters\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by RandomizedSearchCV\n",
        "print(f'Best Parameters: {random_search.best_params_}')\n",
        "\n",
        "# Get the best model from the random search\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the best model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Best Model Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "id": "sDABRg4GxnAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracyM\n",
        "# Import necessary libraries\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset (which has 3 classes)\n",
        "data = load_iris()\n",
        "\n",
        "# Convert the dataset into a DataFrame for easier handling\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Features (X) and target (y)\n",
        "X = df[data.feature_names]  # Input features\n",
        "y = df['target']  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Apply One-vs-One classifier using Logistic Regression\n",
        "ovo_model = OneVsOneClassifier(log_reg)\n",
        "\n",
        "# Train the model on the training set\n",
        "ovo_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Model Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "id": "aW460c-pxqYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12 Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "# Recall, and F1-Score\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Load the Iris dataset (which has 3 classes)\n",
        "data = load_iris()\n",
        "\n",
        "# Convert the dataset into a DataFrame for easier handling\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Features (X) and target (y)\n",
        "X = df[data.feature_names]  # Input features\n",
        "y = df['target']  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score for each class\n",
        "precision = precision_score(y_test, y_pred, average='macro')  # For multiclass, 'macro' calculates metrics for each class\n",
        "recall = recall_score(y_test, y_pred, average='macro')  # For multiclass, 'macro' calculates metrics for each class\n",
        "f1 = f1_score(y_test, y_pred, average='macro')  # For multiclass, 'macro' calculates metrics for each class\n",
        "\n",
        "# Print the results\n",
        "print(f'Precision: {precision * 100:.2f}%')\n",
        "print(f'Recall: {recall * 100:.2f}%')\n",
        "print(f'F1-Score: {f1 * 100:.2f}%')\n",
        "\n",
        "# Optionally, print the accuracy as well\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "id": "3-whabL0x4q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 13 Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "# improve model performanceM\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Step 1: Create an imbalanced dataset using make_classification\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,\n",
        "                           weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
        "\n",
        "# Check the distribution of the classes\n",
        "class_counts = np.bincount(y)\n",
        "print(f\"Class distribution: {class_counts[0]} samples for class 0, {class_counts[1]} samples for class 1\")\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Create and train the Logistic Regression model with class weights\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=200, random_state=42)\n",
        "\n",
        "# Train the model on the imbalanced dataset\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(f\"Recall: {recall * 100:.2f}%\")\n",
        "print(f\"F1-Score: {f1 * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "4Hwe6oqNx9XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 14 Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "# evaluate performanceM\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"First few rows of the Titanic dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Step 1: Handle missing values\n",
        "# For simplicity, let's use SimpleImputer to fill missing values\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # Fill missing values with the most frequent value\n",
        "\n",
        "# Impute missing values in 'Age' column\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])  # This will correctly transform the column\n",
        "\n",
        "# Impute missing values in 'Embarked' column (now assigning correctly as a single column)\n",
        "df['Embarked'] = imputer.fit_transform(df[['Embarked']]).flatten()  # Fixing the error here with flatten\n",
        "\n",
        "# Drop the 'Cabin' column because it has too many missing values\n",
        "df.drop(columns=['Cabin'], inplace=True)\n",
        "\n",
        "# Step 2: Convert categorical features into numeric values\n",
        "# Convert 'Sex' column using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df['Sex'] = label_encoder.fit_transform(df['Sex'])\n",
        "\n",
        "# Convert 'Embarked' column to numeric using one-hot encoding\n",
        "df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)\n",
        "\n",
        "# Step 3: Prepare the features and target variable\n",
        "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']]  # Features\n",
        "y = df['Survived']  # Target variable\n",
        "\n",
        "# Step 4: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 7: Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(f\"Recall: {recall * 100:.2f}%\")\n",
        "print(f\"F1-Score: {f1 * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "NGzwkwALyAJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15 Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "# model. Evaluate its accuracy and compare results with and without scalingM\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"First few rows of the Titanic dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Step 1: Handle missing values\n",
        "# For simplicity, let's use SimpleImputer to fill missing values\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # Fill missing values with the most frequent value\n",
        "\n",
        "# Impute missing values in 'Age' column\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])  # This will correctly transform the column\n",
        "\n",
        "# Impute missing values in 'Embarked' column (now assigning correctly as a single column)\n",
        "df['Embarked'] = imputer.fit_transform(df[['Embarked']]).flatten()  # Fixing the error here with flatten\n",
        "\n",
        "# Drop the 'Cabin' column because it has too many missing values\n",
        "df.drop(columns=['Cabin'], inplace=True)\n",
        "\n",
        "# Step 2: Convert categorical features into numeric values\n",
        "# Convert 'Sex' column using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df['Sex'] = label_encoder.fit_transform(df['Sex'])\n",
        "\n",
        "# Convert 'Embarked' column to numeric using one-hot encoding\n",
        "df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)\n",
        "\n",
        "# Step 3: Prepare the features and target variable\n",
        "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']]  # Features\n",
        "y = df['Survived']  # Target variable\n",
        "\n",
        "# Step 4: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ---- Without Scaling ----\n",
        "# Step 5: Train the Logistic Regression model without scaling\n",
        "model_no_scaling = LogisticRegression(max_iter=200, random_state=42)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions on the test set\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "\n",
        "# Step 7: Evaluate the model's performance without scaling\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scaling * 100:.2f}%\")\n",
        "\n",
        "# ---- With Scaling (Standardization) ----\n",
        "# Step 8: Apply standardization to the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 9: Train the Logistic Regression model with scaling\n",
        "model_with_scaling = LogisticRegression(max_iter=200, random_state=42)\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 10: Make predictions on the scaled test set\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "\n",
        "# Step 11: Evaluate the model's performance with scaling\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "print(f\"Accuracy with scaling: {accuracy_with_scaling * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "7zHXPq2DyDh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 16 Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC scoreM\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"First few rows of the Titanic dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Step 1: Handle missing values\n",
        "# For simplicity, let's use SimpleImputer to fill missing values\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # Fill missing values with the most frequent value\n",
        "\n",
        "# Impute missing values in 'Age' column\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])  # This will correctly transform the column\n",
        "\n",
        "# Impute missing values in 'Embarked' column (now assigning correctly as a single column)\n",
        "df['Embarked'] = imputer.fit_transform(df[['Embarked']]).flatten()  # Fixing the error here with flatten\n",
        "\n",
        "# Drop the 'Cabin' column because it has too many missing values\n",
        "df.drop(columns=['Cabin'], inplace=True)\n",
        "\n",
        "# Step 2: Convert categorical features into numeric values\n",
        "# Convert 'Sex' column using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df['Sex'] = label_encoder.fit_transform(df['Sex'])\n",
        "\n",
        "# Convert 'Embarked' column to numeric using one-hot encoding\n",
        "df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)\n",
        "\n",
        "# Step 3: Prepare the features and target variable\n",
        "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']]  # Features\n",
        "y = df['Survived']  # Target variable\n",
        "\n",
        "# Step 4: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions on the test set\n",
        "y_pred_prob = model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
        "\n",
        "# Step 7: Calculate the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "print(f\"ROC-AUC score: {roc_auc:.4f}\")\n",
        "\n",
        "# Step 8: Plot the ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='red', linestyle='--')  # Diagonal line for random classifier\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cpFyH3V1yH7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 17 Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "# accuracyM\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"First few rows of the Titanic dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Step 1: Handle missing values\n",
        "# For simplicity, let's use SimpleImputer to fill missing values\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # Fill missing values with the most frequent value\n",
        "\n",
        "# Impute missing values in 'Age' column\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])  # This will correctly transform the column\n",
        "\n",
        "# Impute missing values in 'Embarked' column\n",
        "df['Embarked'] = imputer.fit_transform(df[['Embarked']]).flatten()  # Fixing the error here with flatten\n",
        "\n",
        "# Drop the 'Cabin' column because it has too many missing values\n",
        "df.drop(columns=['Cabin'], inplace=True)\n",
        "\n",
        "# Step 2: Convert categorical features into numeric values\n",
        "# Convert 'Sex' column using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df['Sex'] = label_encoder.fit_transform(df['Sex'])\n",
        "\n",
        "# Convert 'Embarked' column to numeric using one-hot encoding\n",
        "df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)\n",
        "\n",
        "# Step 3: Prepare the features and target variable\n",
        "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']]  # Features\n",
        "y = df['Survived']  # Target variable\n",
        "\n",
        "# Step 4: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Train the Logistic Regression model with custom learning rate (C=0.5)\n",
        "model = LogisticRegression(C=0.5, max_iter=200, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 7: Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with C=0.5: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "woZrs04XyLQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 18 Write a Python program to train Logistic Regression and identify important features based on model\n",
        "# coefficients\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"First few rows of the Titanic dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Step 1: Handle missing values\n",
        "# For simplicity, let's use SimpleImputer to fill missing values\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # Fill missing values with the most frequent value\n",
        "\n",
        "# Impute missing values in 'Age' column\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])  # This will correctly transform the column\n",
        "\n",
        "# Impute missing values in 'Embarked' column\n",
        "df['Embarked'] = imputer.fit_transform(df[['Embarked']]).flatten()  # Fixing the error here with flatten\n",
        "\n",
        "# Drop the 'Cabin' column because it has too many missing values\n",
        "df.drop(columns=['Cabin'], inplace=True)\n",
        "\n",
        "# Step 2: Convert categorical features into numeric values\n",
        "# Convert 'Sex' column using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df['Sex'] = label_encoder.fit_transform(df['Sex'])\n",
        "\n",
        "# Convert 'Embarked' column to numeric using one-hot encoding\n",
        "df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)\n",
        "\n",
        "# Step 3: Prepare the features and target variable\n",
        "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']]  # Features\n",
        "y = df['Survived']  # Target variable\n",
        "\n",
        "# Step 4: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Get the coefficients and features\n",
        "coefficients = model.coef_[0]  # The model coefficients for each feature\n",
        "feature_names = X.columns  # The names of the features\n",
        "\n",
        "# Step 7: Create a DataFrame to show feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': coefficients\n",
        "})\n",
        "\n",
        "# Sort the features based on the absolute value of the coefficients\n",
        "feature_importance['Abs_Coefficient'] = feature_importance['Coefficient'].abs()\n",
        "feature_importance = feature_importance.sort_values(by='Abs_Coefficient', ascending=False)\n",
        "\n",
        "# Step 8: Display the important features based on coefficients\n",
        "print(\"\\nImportant Features Based on Model Coefficients:\")\n",
        "print(feature_importance[['Feature', 'Coefficient']])\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "hT1TVvG3yO3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 19 Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa\n",
        "# ScoreM\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"First few rows of the Titanic dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Step 1: Handle missing values\n",
        "# For simplicity, let's use SimpleImputer to fill missing values\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # Fill missing values with the most frequent value\n",
        "\n",
        "# Impute missing values in 'Age' column\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])  # This will correctly transform the column\n",
        "\n",
        "# Impute missing values in 'Embarked' column\n",
        "df['Embarked'] = imputer.fit_transform(df[['Embarked']]).flatten()  # Fixing the error here with flatten\n",
        "\n",
        "# Drop the 'Cabin' column because it has too many missing values\n",
        "df.drop(columns=['Cabin'], inplace=True)\n",
        "\n",
        "# Step 2: Convert categorical features into numeric values\n",
        "# Convert 'Sex' column using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df['Sex'] = label_encoder.fit_transform(df['Sex'])\n",
        "\n",
        "# Convert 'Embarked' column to numeric using one-hot encoding\n",
        "df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)\n",
        "\n",
        "# Step 3: Prepare the features and target variable\n",
        "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']]  # Features\n",
        "y = df['Survived']  # Target variable\n",
        "\n",
        "# Step 4: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 7: Calculate Cohen's Kappa score\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "# Step 8: Display the Cohen's Kappa score\n",
        "print(f\"Cohen's Kappa Score: {kappa:.4f}\")"
      ],
      "metadata": {
        "id": "e7uJahl2ySYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 20 Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "# classificatio:\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # Fill missing values with the most frequent value\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])  # Impute missing values in 'Age' column\n",
        "df['Embarked'] = imputer.fit_transform(df[['Embarked']]).flatten()  # Impute missing values in 'Embarked' column\n",
        "df.drop(columns=['Cabin'], inplace=True)  # Drop 'Cabin' column as it has too many missing values\n",
        "\n",
        "# Convert categorical columns into numeric\n",
        "label_encoder = LabelEncoder()\n",
        "df['Sex'] = label_encoder.fit_transform(df['Sex'])  # Convert 'Sex' to numeric values\n",
        "df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)  # One-hot encode 'Embarked'\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']]  # Features\n",
        "y = df['Survived']  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "y_probs = model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class (Survived=1)\n",
        "\n",
        "# Calculate Precision and Recall\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
        "\n",
        "# Plot the Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='blue', label='Precision-Recall curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for Logistic Regression')\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Optionally, calculate the area under the Precision-Recall curve\n",
        "pr_auc = auc(recall, precision)\n",
        "print(f'Area under Precision-Recall Curve (AUC-PR): {pr_auc:.4f}')"
      ],
      "metadata": {
        "id": "ENkriCoPyVh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 21 Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "# their accuracyM\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # Fill missing values with the most frequent value\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])  # Impute missing values in 'Age' column\n",
        "df['Embarked'] = imputer.fit_transform(df[['Embarked']]).flatten()  # Impute missing values in 'Embarked' column\n",
        "df.drop(columns=['Cabin'], inplace=True)  # Drop 'Cabin' column as it has too many missing values\n",
        "\n",
        "# Convert categorical columns into numeric\n",
        "label_encoder = LabelEncoder()\n",
        "df['Sex'] = label_encoder.fit_transform(df['Sex'])  # Convert 'Sex' to numeric values\n",
        "df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)  # One-hot encode 'Embarked'\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']]  # Features\n",
        "y = df['Survived']  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# List of solvers to test\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "# Dictionary to store accuracies for each solver\n",
        "solver_accuracies = {}\n",
        "\n",
        "# Train Logistic Regression with each solver and evaluate accuracy\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=200, random_state=42)\n",
        "    model.fit(X_train, y_train)  # Train the model\n",
        "    y_pred = model.predict(X_test)  # Predict on the test set\n",
        "    accuracy = accuracy_score(y_test, y_pred)  # Calculate accuracy\n",
        "    solver_accuracies[solver] = accuracy  # Store the accuracy\n",
        "\n",
        "# Print accuracies for each solver\n",
        "for solver, accuracy in solver_accuracies.items():\n",
        "    print(f'Accuracy using solver {solver}: {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "3WKFGv_OybQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 22 Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "# Correlation Coefficient (MCC)M\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # Fill missing values with the most frequent value\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])  # Impute missing values in 'Age' column\n",
        "df['Embarked'] = imputer.fit_transform(df[['Embarked']]).flatten()  # Impute missing values in 'Embarked' column\n",
        "df.drop(columns=['Cabin'], inplace=True)  # Drop 'Cabin' column as it has too many missing values\n",
        "\n",
        "# Convert categorical columns into numeric\n",
        "label_encoder = LabelEncoder()\n",
        "df['Sex'] = label_encoder.fit_transform(df['Sex'])  # Convert 'Sex' to numeric values\n",
        "df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)  # One-hot encode 'Embarked'\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']]  # Features\n",
        "y = df['Survived']  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear', max_iter=200, random_state=42)\n",
        "model.fit(X_train, y_train)  # Train the model\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "# Print the MCC score\n",
        "print(f'Matthews Correlation Coefficient (MCC): {mcc:.4f}')"
      ],
      "metadata": {
        "id": "0IV-OvZayb92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 23 Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "# accuracy to see the impact of feature scalingM\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # Fill missing values with the most frequent value\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])  # Impute missing values in 'Age' column\n",
        "df['Embarked'] = imputer.fit_transform(df[['Embarked']]).flatten()  # Impute missing values in 'Embarked' column\n",
        "df.drop(columns=['Cabin'], inplace=True)  # Drop 'Cabin' column as it has too many missing values\n",
        "\n",
        "# Convert categorical columns into numeric\n",
        "label_encoder = LabelEncoder()\n",
        "df['Sex'] = label_encoder.fit_transform(df['Sex'])  # Convert 'Sex' to numeric values\n",
        "df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)  # One-hot encode 'Embarked'\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']]  # Features\n",
        "y = df['Survived']  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear', max_iter=200, random_state=42)\n",
        "\n",
        "# Train on raw data (no scaling)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_raw = model.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Standardize the data (feature scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train on standardized data\n",
        "model.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print the accuracy comparison\n",
        "print(f'Accuracy on raw data: {accuracy_raw:.4f}')\n",
        "print(f'Accuracy on standardized data: {accuracy_scaled:.4f}')"
      ],
      "metadata": {
        "id": "ktw2lU6pyfNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 24 Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "# cross-validationM\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # Fill missing values with the most frequent value\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])  # Impute missing values in 'Age' column\n",
        "df['Embarked'] = imputer.fit_transform(df[['Embarked']]).flatten()  # Impute missing values in 'Embarked' column\n",
        "df.drop(columns=['Cabin'], inplace=True)  # Drop 'Cabin' column as it has too many missing values\n",
        "\n",
        "# Convert categorical columns into numeric\n",
        "label_encoder = LabelEncoder()\n",
        "df['Sex'] = label_encoder.fit_transform(df['Sex'])  # Convert 'Sex' to numeric values\n",
        "df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)  # One-hot encode 'Embarked'\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']]  # Features\n",
        "y = df['Survived']  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "logreg = LogisticRegression(solver='liblinear', max_iter=200, random_state=42)\n",
        "\n",
        "# Set the range of C values to test in cross-validation\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# Use GridSearchCV for cross-validation to find the optimal C\n",
        "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameter (optimal C)\n",
        "best_C = grid_search.best_params_['C']\n",
        "print(f'Optimal C: {best_C}')\n",
        "\n",
        "# Evaluate the model with the best C on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy with optimal C: {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "zYYYZbO-ylyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 25 Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "# make predictions.\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import joblib  # For saving and loading the model\n",
        "\n",
        "# Load Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy='most_frequent')  # Fill missing values with the most frequent value\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])  # Impute missing values in 'Age' column\n",
        "df['Embarked'] = imputer.fit_transform(df[['Embarked']]).flatten()  # Impute missing values in 'Embarked' column\n",
        "df.drop(columns=['Cabin'], inplace=True)  # Drop 'Cabin' column as it has too many missing values\n",
        "\n",
        "# Convert categorical columns into numeric\n",
        "label_encoder = LabelEncoder()\n",
        "df['Sex'] = label_encoder.fit_transform(df['Sex'])  # Convert 'Sex' to numeric values\n",
        "df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)  # One-hot encode 'Embarked'\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']]  # Features\n",
        "y = df['Survived']  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "logreg = LogisticRegression(solver='liblinear', max_iter=200, random_state=42)\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Save the trained model using joblib\n",
        "joblib.dump(logreg, 'logreg_model.joblib')\n",
        "print(\"Model saved successfully!\")\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = joblib.load('logreg_model.joblib')\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Evaluate the loaded model on the test set\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy of the loaded model: {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "8VRFfG6MymoA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}